{
  "version": 4,
  "terraform_version": "1.1.3",
  "serial": 686,
  "lineage": "045f9330-027d-54b4-3f93-621df58ea50d",
  "outputs": {
    "ecr_registry_id": {
      "value": "687227541429",
      "type": "string"
    },
    "ecr_repository_url": {
      "value": "687227541429.dkr.ecr.ap-northeast-2.amazonaws.com/bar",
      "type": "string"
    },
    "id": {
      "value": "Z096848533MX8215J1WYJ",
      "type": "string"
    },
    "name_server": {
      "value": [
        "ns-1167.awsdns-17.org",
        "ns-1831.awsdns-36.co.uk",
        "ns-429.awsdns-53.com",
        "ns-788.awsdns-34.net"
      ],
      "type": [
        "list",
        "string"
      ]
    }
  },
  "resources": [
    {
      "mode": "data",
      "type": "aws_ami",
      "name": "amazonLinux",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "architecture": "x86_64",
            "arn": "arn:aws:ec2:ap-northeast-2::image/ami-0eb7a369386789460",
            "block_device_mappings": [
              {
                "device_name": "/dev/xvda",
                "ebs": {
                  "delete_on_termination": "true",
                  "encrypted": "false",
                  "iops": "0",
                  "snapshot_id": "snap-016326c3576cb710d",
                  "throughput": "0",
                  "volume_size": "8",
                  "volume_type": "gp2"
                },
                "no_device": "",
                "virtual_name": ""
              }
            ],
            "creation_date": "2022-02-08T00:25:15.000Z",
            "description": "Amazon Linux 2 AMI 2.0.20220207.1 x86_64 HVM gp2",
            "ena_support": true,
            "executable_users": null,
            "filter": [
              {
                "name": "architecture",
                "values": [
                  "x86_64"
                ]
              },
              {
                "name": "name",
                "values": [
                  "amzn2-ami-hvm-2.0.*"
                ]
              },
              {
                "name": "virtualization-type",
                "values": [
                  "hvm"
                ]
              }
            ],
            "hypervisor": "xen",
            "id": "ami-0eb7a369386789460",
            "image_id": "ami-0eb7a369386789460",
            "image_location": "amazon/amzn2-ami-hvm-2.0.20220207.1-x86_64-gp2",
            "image_owner_alias": "amazon",
            "image_type": "machine",
            "kernel_id": null,
            "most_recent": true,
            "name": "amzn2-ami-hvm-2.0.20220207.1-x86_64-gp2",
            "name_regex": null,
            "owner_id": "137112412989",
            "owners": [
              "amazon"
            ],
            "platform": null,
            "platform_details": "Linux/UNIX",
            "product_codes": [],
            "public": true,
            "ramdisk_id": null,
            "root_device_name": "/dev/xvda",
            "root_device_type": "ebs",
            "root_snapshot_id": "snap-016326c3576cb710d",
            "sriov_net_support": "simple",
            "state": "available",
            "state_reason": {
              "code": "UNSET",
              "message": "UNSET"
            },
            "tags": {},
            "usage_operation": "RunInstances",
            "virtualization_type": "hvm"
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "data",
      "type": "aws_eks_cluster",
      "name": "cluster",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:eks:ap-northeast-2:687227541429:cluster/terraformEKScluster",
            "certificate_authority": [
              {
                "data": "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1ESXhNekF4TXprME5sb1hEVE15TURJeE1UQXhNemswTmxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTi81ClhaZXJMeFNodzlQK0VZdTNDNjJLMk9xOXFuTnJyMTNkRG0yM3JJcGR5YzVwSENYMTBHTWZmZWExVnljZGdBMmUKWjRHUTdJdlQyOXZ2UnZ1akdNcm9mczdKazdXVXBVWVczYkZSV0VKTkxXUHZvMVZxRGJ6KzZCQzFEamYzaHFPbApvTXphVnlOVFFMWG1JM3BGbmxMUU9wTmIwNUF6ajc2TmtHZ0MxQlJkbXFtV3A3SFoxWjVmV2IxenE2QkJxQXBBCnZ6N3lRRVR0VDl0ZGIzZkZsZWhqR3hIQjNzZlFpNkFYUk5ETkRWYW5ZUjVldzA3dy9FMitnQjE2QVlzUW14aDkKZUpNVGNCbXFGbGF6c0o4MmNKTFFteTU0RUJ4c3ZINFVKcDlMZEl4d0o2ZGsxUnBzM2RVYlMrdk11MXBlWHcxTwpiVVhlcERJUkU0YUJyeXpXRXo4Q0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZMTk1vcHUxcjNoN0lONkJ6MTRkVnVmRG55YVJNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFCOFl0cllJMEIrUzB0V20wWFV0MDZLQm90SDd3bFcxYlZoWVQzKzVHemh2aWRhdm9QdApxTlVuMk52bHByeVBhTGNiTllaeVIxYXFwTGFwcWdSZEdFcFI3MDRQaVFVeDZrTXJjc1hwNktXWW5VUUcvSVNwCmNJSFVtUForM2JDU2dpU2dNL2M4bHd4dTlRWWhiRWx5UXMyTDdXNU1OY0p6RW1kQlVvSkhzeGV1bXMwc1A1aEYKQ0Y3ZVJFMUNNRDJqeDhMci9pLzc1VHc5ZDBvYzNTWFh4RkVOSXg1djJiUEN4U05abEdOdU9MZDE3NmNHUC9iMwozQWd2enc0ditxWXphUGVVblF2R0I4TUMyMjVJR0dZdHg2RDNMUGhRQ0NveEtPRnVOSFMyZUl1UWwrWFRpV1Z2CnhueGswdDRZZHc3UlBzRVFKbUdVOGY4UkY4K2FTMklXK1kvUwotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg=="
              }
            ],
            "created_at": "2022-02-13 01:33:18.685 +0000 UTC",
            "enabled_cluster_log_types": [],
            "endpoint": "https://B7ED8B7607FD4F380A7B07E9F6E13427.gr7.ap-northeast-2.eks.amazonaws.com",
            "id": "terraformEKScluster",
            "identity": [
              {
                "oidc": [
                  {
                    "issuer": "https://oidc.eks.ap-northeast-2.amazonaws.com/id/B7ED8B7607FD4F380A7B07E9F6E13427"
                  }
                ]
              }
            ],
            "kubernetes_network_config": [
              {
                "ip_family": "ipv4",
                "service_ipv4_cidr": "10.100.0.0/16"
              }
            ],
            "name": "terraformEKScluster",
            "platform_version": "eks.3",
            "role_arn": "arn:aws:iam::687227541429:role/terraformekscluster",
            "status": "ACTIVE",
            "tags": {},
            "version": "1.20",
            "vpc_config": [
              {
                "cluster_security_group_id": "sg-0cf82a413d219690b",
                "endpoint_private_access": false,
                "endpoint_public_access": true,
                "public_access_cidrs": [
                  "0.0.0.0/0"
                ],
                "security_group_ids": [
                  "sg-0c562377ad26baf00"
                ],
                "subnet_ids": [
                  "subnet-07f100a568c2c9fd0",
                  "subnet-083bd41dff266b7f9"
                ],
                "vpc_id": "vpc-03307991857eec5c3"
              }
            ]
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "data",
      "type": "aws_eks_cluster_auth",
      "name": "cluster",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "terraformEKScluster",
            "name": "terraformEKScluster",
            "token": "k8s-aws-v1.aHR0cHM6Ly9zdHMuYXAtbm9ydGhlYXN0LTIuYW1hem9uYXdzLmNvbS8_QWN0aW9uPUdldENhbGxlcklkZW50aXR5JlZlcnNpb249MjAxMS0wNi0xNSZYLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUEyQUFQVUM2MjVZTTc3TTdRJTJGMjAyMjAyMTMlMkZhcC1ub3J0aGVhc3QtMiUyRnN0cyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjIwMjEzVDEwMzkwNFomWC1BbXotRXhwaXJlcz0wJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCUzQngtazhzLWF3cy1pZCZYLUFtei1TaWduYXR1cmU9OTFkMTIyYjQwZjliYTk2ZTgwZWQ0ZTgyNDdmMTE3NTkxNzgyMmJjMzdjMjI2MjNjNTZjMzQ2MDg2MDczNGFmZg"
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "data",
      "type": "tls_certificate",
      "name": "cluster",
      "provider": "provider[\"registry.terraform.io/hashicorp/tls\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "certificates": [
              {
                "is_ca": true,
                "issuer": "OU=Starfield Class 2 Certification Authority,O=Starfield Technologies\\, Inc.,C=US",
                "not_after": "2034-06-28T17:39:16Z",
                "not_before": "2009-09-02T00:00:00Z",
                "public_key_algorithm": "RSA",
                "serial_number": "12037640545166866303",
                "sha1_fingerprint": "9e99a48a9960b14926bb7f3b02e22da2b0ab7280",
                "signature_algorithm": "SHA256-RSA",
                "subject": "CN=Starfield Services Root Certificate Authority - G2,O=Starfield Technologies\\, Inc.,L=Scottsdale,ST=Arizona,C=US",
                "version": 3
              },
              {
                "is_ca": true,
                "issuer": "CN=Starfield Services Root Certificate Authority - G2,O=Starfield Technologies\\, Inc.,L=Scottsdale,ST=Arizona,C=US",
                "not_after": "2037-12-31T01:00:00Z",
                "not_before": "2015-05-25T12:00:00Z",
                "public_key_algorithm": "RSA",
                "serial_number": "144918191876577076464031512351042010504348870",
                "sha1_fingerprint": "06b25927c42a721631c1efd9431e648fa62e1e39",
                "signature_algorithm": "SHA256-RSA",
                "subject": "CN=Amazon Root CA 1,O=Amazon,C=US",
                "version": 3
              },
              {
                "is_ca": true,
                "issuer": "CN=Amazon Root CA 1,O=Amazon,C=US",
                "not_after": "2025-10-19T00:00:00Z",
                "not_before": "2015-10-22T00:00:00Z",
                "public_key_algorithm": "RSA",
                "serial_number": "144918209630989264145272943054026349679957517",
                "sha1_fingerprint": "917e732d330f9a12404f73d8bea36948b929dffc",
                "signature_algorithm": "SHA256-RSA",
                "subject": "CN=Amazon,OU=Server CA 1B,O=Amazon,C=US",
                "version": 3
              },
              {
                "is_ca": false,
                "issuer": "CN=Amazon,OU=Server CA 1B,O=Amazon,C=US",
                "not_after": "2022-07-11T23:59:59Z",
                "not_before": "2021-06-12T00:00:00Z",
                "public_key_algorithm": "RSA",
                "serial_number": "2321944145033399005498492451938446828",
                "sha1_fingerprint": "7ad06d3b76c81d98b1f6dc27574fe4cfc7e3cce9",
                "signature_algorithm": "SHA256-RSA",
                "subject": "CN=oidc.eks.ap-northeast-2.amazonaws.com",
                "version": 3
              }
            ],
            "id": "2022-02-13 10:39:04.928608147 +0000 UTC",
            "url": "https://oidc.eks.ap-northeast-2.amazonaws.com/id/B7ED8B7607FD4F380A7B07E9F6E13427",
            "verify_chain": true
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_acm_certificate",
      "name": "kubedns",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:acm:ap-northeast-2:687227541429:certificate/abcaf752-73c3-4a73-ae47-c1705ca534e1",
            "certificate_authority_arn": "",
            "certificate_body": null,
            "certificate_chain": null,
            "domain_name": "kubedns.click",
            "domain_validation_options": [
              {
                "domain_name": "kubedns.click",
                "resource_record_name": "_d639929d59e4909daf7f06659708fe50.kubedns.click.",
                "resource_record_type": "CNAME",
                "resource_record_value": "_ed0671414b4f0d83bac8eece608cee88.bgpjyrktby.acm-validations.aws."
              }
            ],
            "id": "arn:aws:acm:ap-northeast-2:687227541429:certificate/abcaf752-73c3-4a73-ae47-c1705ca534e1",
            "options": [
              {
                "certificate_transparency_logging_preference": "ENABLED"
              }
            ],
            "private_key": null,
            "status": "ISSUED",
            "subject_alternative_names": [],
            "tags": {},
            "tags_all": {},
            "validation_emails": [],
            "validation_method": "DNS"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_acm_certificate_validation",
      "name": "kubedns",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "certificate_arn": "arn:aws:acm:ap-northeast-2:687227541429:certificate/abcaf752-73c3-4a73-ae47-c1705ca534e1",
            "id": "2022-02-13 01:33:26.953 +0000 UTC",
            "timeouts": {
              "create": "20m"
            },
            "validation_record_fqdns": [
              "_d639929d59e4909daf7f06659708fe50.kubedns.click"
            ]
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoxMjAwMDAwMDAwMDAwfX0=",
          "dependencies": [
            "aws_acm_certificate.kubedns",
            "aws_route53_record.kubedns",
            "aws_route53_zone.kubedns"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_ecr_repository",
      "name": "foo",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:ecr:ap-northeast-2:687227541429:repository/bar",
            "encryption_configuration": [
              {
                "encryption_type": "AES256",
                "kms_key": ""
              }
            ],
            "id": "bar",
            "image_scanning_configuration": [
              {
                "scan_on_push": true
              }
            ],
            "image_tag_mutability": "IMMUTABLE",
            "name": "bar",
            "registry_id": "687227541429",
            "repository_url": "687227541429.dkr.ecr.ap-northeast-2.amazonaws.com/bar",
            "tags": {},
            "tags_all": {},
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjoxMjAwMDAwMDAwMDAwfX0="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_ecr_repository_policy",
      "name": "bar-policy",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "bar",
            "policy": "{\"Statement\":[{\"Action\":[\"ecr:GetDownloadUrlForLayer\",\"ecr:BatchGetImage\",\"ecr:BatchCheckLayerAvailability\",\"ecr:PutImage\",\"ecr:InitiateLayerUpload\",\"ecr:UploadLayerPart\",\"ecr:CompleteLayerUpload\",\"ecr:DescribeRepositories\",\"ecr:GetRepositoryPolicy\",\"ecr:ListImages\",\"ecr:DeleteRepository\",\"ecr:BatchDeleteImage\",\"ecr:SetRepositoryPolicy\",\"ecr:DeleteRepositoryPolicy\"],\"Effect\":\"Allow\",\"Principal\":\"*\",\"Sid\":\"adds full ecr access to the demo repository\"}],\"Version\":\"2008-10-17\"}",
            "registry_id": "687227541429",
            "repository": "bar"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_ecr_repository.foo"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_eip",
      "name": "nateip",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"].vpc1",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "address": null,
            "allocation_id": "eipalloc-054cbdbb9c5632faa",
            "associate_with_private_ip": null,
            "association_id": "eipassoc-0537003f5506a17a9",
            "carrier_ip": "",
            "customer_owned_ip": "",
            "customer_owned_ipv4_pool": "",
            "domain": "vpc",
            "id": "eipalloc-054cbdbb9c5632faa",
            "instance": "",
            "network_border_group": "ap-northeast-2",
            "network_interface": "eni-090ff9a2a7d203b0e",
            "private_dns": "ip-192-168-2-158.ap-northeast-2.compute.internal",
            "private_ip": "192.168.2.158",
            "public_dns": "ec2-3-37-152-8.ap-northeast-2.compute.amazonaws.com",
            "public_ip": "3.37.152.8",
            "public_ipv4_pool": "amazon",
            "tags": {},
            "tags_all": {},
            "timeouts": null,
            "vpc": true
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjoxODAwMDAwMDAwMDAsInJlYWQiOjkwMDAwMDAwMDAwMCwidXBkYXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_eks_cluster",
      "name": "eks_cluster",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:eks:ap-northeast-2:687227541429:cluster/terraformEKScluster",
            "certificate_authority": [
              {
                "data": "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1ESXhNekF4TXprME5sb1hEVE15TURJeE1UQXhNemswTmxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTi81ClhaZXJMeFNodzlQK0VZdTNDNjJLMk9xOXFuTnJyMTNkRG0yM3JJcGR5YzVwSENYMTBHTWZmZWExVnljZGdBMmUKWjRHUTdJdlQyOXZ2UnZ1akdNcm9mczdKazdXVXBVWVczYkZSV0VKTkxXUHZvMVZxRGJ6KzZCQzFEamYzaHFPbApvTXphVnlOVFFMWG1JM3BGbmxMUU9wTmIwNUF6ajc2TmtHZ0MxQlJkbXFtV3A3SFoxWjVmV2IxenE2QkJxQXBBCnZ6N3lRRVR0VDl0ZGIzZkZsZWhqR3hIQjNzZlFpNkFYUk5ETkRWYW5ZUjVldzA3dy9FMitnQjE2QVlzUW14aDkKZUpNVGNCbXFGbGF6c0o4MmNKTFFteTU0RUJ4c3ZINFVKcDlMZEl4d0o2ZGsxUnBzM2RVYlMrdk11MXBlWHcxTwpiVVhlcERJUkU0YUJyeXpXRXo4Q0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZMTk1vcHUxcjNoN0lONkJ6MTRkVnVmRG55YVJNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFCOFl0cllJMEIrUzB0V20wWFV0MDZLQm90SDd3bFcxYlZoWVQzKzVHemh2aWRhdm9QdApxTlVuMk52bHByeVBhTGNiTllaeVIxYXFwTGFwcWdSZEdFcFI3MDRQaVFVeDZrTXJjc1hwNktXWW5VUUcvSVNwCmNJSFVtUForM2JDU2dpU2dNL2M4bHd4dTlRWWhiRWx5UXMyTDdXNU1OY0p6RW1kQlVvSkhzeGV1bXMwc1A1aEYKQ0Y3ZVJFMUNNRDJqeDhMci9pLzc1VHc5ZDBvYzNTWFh4RkVOSXg1djJiUEN4U05abEdOdU9MZDE3NmNHUC9iMwozQWd2enc0ditxWXphUGVVblF2R0I4TUMyMjVJR0dZdHg2RDNMUGhRQ0NveEtPRnVOSFMyZUl1UWwrWFRpV1Z2CnhueGswdDRZZHc3UlBzRVFKbUdVOGY4UkY4K2FTMklXK1kvUwotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg=="
              }
            ],
            "created_at": "2022-02-13 01:33:18.685 +0000 UTC",
            "enabled_cluster_log_types": [],
            "encryption_config": [],
            "endpoint": "https://B7ED8B7607FD4F380A7B07E9F6E13427.gr7.ap-northeast-2.eks.amazonaws.com",
            "id": "terraformEKScluster",
            "identity": [
              {
                "oidc": [
                  {
                    "issuer": "https://oidc.eks.ap-northeast-2.amazonaws.com/id/B7ED8B7607FD4F380A7B07E9F6E13427"
                  }
                ]
              }
            ],
            "kubernetes_network_config": [
              {
                "ip_family": "ipv4",
                "service_ipv4_cidr": "10.100.0.0/16"
              }
            ],
            "name": "terraformEKScluster",
            "platform_version": "eks.3",
            "role_arn": "arn:aws:iam::687227541429:role/terraformekscluster",
            "status": "ACTIVE",
            "tags": {},
            "tags_all": {},
            "timeouts": null,
            "version": "1.20",
            "vpc_config": [
              {
                "cluster_security_group_id": "sg-0cf82a413d219690b",
                "endpoint_private_access": false,
                "endpoint_public_access": true,
                "public_access_cidrs": [
                  "0.0.0.0/0"
                ],
                "security_group_ids": [
                  "sg-0c562377ad26baf00"
                ],
                "subnet_ids": [
                  "subnet-07f100a568c2c9fd0",
                  "subnet-083bd41dff266b7f9"
                ],
                "vpc_id": "vpc-03307991857eec5c3"
              }
            ]
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoxODAwMDAwMDAwMDAwLCJkZWxldGUiOjkwMDAwMDAwMDAwMCwidXBkYXRlIjozNjAwMDAwMDAwMDAwfX0=",
          "dependencies": [
            "aws_security_group.eks-cluster",
            "aws_subnet.privateEC2Subnet1",
            "aws_subnet.privateEC2Subnet2",
            "aws_vpc.test",
            "aws_iam_role.iam-role-eks-cluster",
            "aws_iam_role_policy_attachment.eks-cluster-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-cluster-AmazonEKSServicePolicy"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_eks_node_group",
      "name": "node",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "ami_type": "AL2_x86_64",
            "arn": "arn:aws:eks:ap-northeast-2:687227541429:nodegroup/terraformEKScluster/node_group1/7ebf785f-dd36-c0a1-4bbb-6568040de9b9",
            "capacity_type": "ON_DEMAND",
            "cluster_name": "terraformEKScluster",
            "disk_size": 20,
            "force_update_version": null,
            "id": "terraformEKScluster:node_group1",
            "instance_types": [
              "t3.medium"
            ],
            "labels": {},
            "launch_template": [],
            "node_group_name": "node_group1",
            "node_group_name_prefix": "",
            "node_role_arn": "arn:aws:iam::687227541429:role/eks-node-group",
            "release_version": "1.20.11-20220123",
            "remote_access": [
              {
                "ec2_ssh_key": "mysoldesktestkey",
                "source_security_group_ids": [
                  "sg-0f1c19a8021875189"
                ]
              }
            ],
            "resources": [
              {
                "autoscaling_groups": [
                  {
                    "name": "eks-7ebf785f-dd36-c0a1-4bbb-6568040de9b9"
                  }
                ],
                "remote_access_security_group_id": "sg-0dfc3f80f6845fef8"
              }
            ],
            "scaling_config": [
              {
                "desired_size": 2,
                "max_size": 3,
                "min_size": 1
              }
            ],
            "status": "ACTIVE",
            "subnet_ids": [
              "subnet-07f100a568c2c9fd0",
              "subnet-083bd41dff266b7f9"
            ],
            "tags": {},
            "tags_all": {},
            "taint": [],
            "timeouts": null,
            "update_config": [
              {
                "max_unavailable": 1,
                "max_unavailable_percentage": 0
              }
            ],
            "version": "1.20"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozNjAwMDAwMDAwMDAwLCJkZWxldGUiOjM2MDAwMDAwMDAwMDAsInVwZGF0ZSI6MzYwMDAwMDAwMDAwMH19",
          "dependencies": [
            "aws_iam_role_policy_attachment.AmazonEKSWorkerNodePolicy",
            "aws_security_group.eks-cluster",
            "aws_subnet.privateEC2Subnet2",
            "aws_iam_role.eks_nodes",
            "aws_iam_role.iam-role-eks-cluster",
            "aws_iam_role_policy_attachment.AmazonEKS_CNI_Policy",
            "aws_iam_role_policy_attachment.eks-cluster-AmazonEKSServicePolicy",
            "aws_security_group.sec-bastion",
            "aws_vpc.test",
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role_policy_attachment.AmazonEC2ContainerRegistryReadOnly",
            "aws_iam_role_policy_attachment.eks-cluster-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.CloudWatchAgentServerPolicy",
            "aws_subnet.privateEC2Subnet1"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_openid_connect_provider",
      "name": "cluster",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::687227541429:oidc-provider/oidc.eks.ap-northeast-2.amazonaws.com/id/B7ED8B7607FD4F380A7B07E9F6E13427",
            "client_id_list": [
              "sts.amazonaws.com"
            ],
            "id": "arn:aws:iam::687227541429:oidc-provider/oidc.eks.ap-northeast-2.amazonaws.com/id/B7ED8B7607FD4F380A7B07E9F6E13427",
            "tags": {},
            "tags_all": {},
            "thumbprint_list": [
              "9e99a48a9960b14926bb7f3b02e22da2b0ab7280"
            ],
            "url": "oidc.eks.ap-northeast-2.amazonaws.com/id/B7ED8B7607FD4F380A7B07E9F6E13427"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.iam-role-eks-cluster",
            "aws_iam_role_policy_attachment.eks-cluster-AmazonEKSClusterPolicy",
            "aws_subnet.privateEC2Subnet1",
            "aws_subnet.privateEC2Subnet2",
            "data.aws_eks_cluster.cluster",
            "aws_iam_role_policy_attachment.eks-cluster-AmazonEKSServicePolicy",
            "aws_security_group.eks-cluster",
            "aws_vpc.test",
            "data.tls_certificate.cluster"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_policy",
      "name": "AllowExternalDNSUpdates",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::687227541429:policy/AllowExternalDNSUpdates",
            "description": "My EKS Route53 policy attach",
            "id": "arn:aws:iam::687227541429:policy/AllowExternalDNSUpdates",
            "name": "AllowExternalDNSUpdates",
            "name_prefix": null,
            "path": "/",
            "policy": "{\"Statement\":[{\"Action\":[\"route53:ChangeResourceRecordSets\"],\"Effect\":\"Allow\",\"Resource\":[\"arn:aws:route53:::hostedzone/*\"]},{\"Action\":[\"route53:ListHostedZones\",\"route53:ListResourceRecordSets\"],\"Effect\":\"Allow\",\"Resource\":[\"*\"]}],\"Version\":\"2012-10-17\"}",
            "policy_id": "ANPA2AAPUC6246PKWQJJH",
            "tags": {},
            "tags_all": {}
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_policy",
      "name": "aws_lb_controller",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::687227541429:policy/aws_lb_controller20220213013301068000000002",
            "description": "",
            "id": "arn:aws:iam::687227541429:policy/aws_lb_controller20220213013301068000000002",
            "name": "aws_lb_controller20220213013301068000000002",
            "name_prefix": "aws_lb_controller",
            "path": "/",
            "policy": "{\"Statement\":[{\"Action\":[\"iam:CreateServiceLinkedRole\",\"ec2:DescribeAccountAttributes\",\"ec2:DescribeAddresses\",\"ec2:DescribeAvailabilityZones\",\"ec2:DescribeInternetGateways\",\"ec2:DescribeVpcs\",\"ec2:DescribeSubnets\",\"ec2:DescribeSecurityGroups\",\"ec2:DescribeInstances\",\"ec2:DescribeNetworkInterfaces\",\"ec2:DescribeTags\",\"ec2:GetCoipPoolUsage\",\"ec2:DescribeCoipPools\",\"elasticloadbalancing:DescribeLoadBalancers\",\"elasticloadbalancing:DescribeLoadBalancerAttributes\",\"elasticloadbalancing:DescribeListeners\",\"elasticloadbalancing:DescribeListenerCertificates\",\"elasticloadbalancing:DescribeSSLPolicies\",\"elasticloadbalancing:DescribeRules\",\"elasticloadbalancing:DescribeTargetGroups\",\"elasticloadbalancing:DescribeTargetGroupAttributes\",\"elasticloadbalancing:DescribeTargetHealth\",\"elasticloadbalancing:DescribeTags\"],\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"cognito-idp:DescribeUserPoolClient\",\"acm:ListCertificates\",\"acm:DescribeCertificate\",\"iam:ListServerCertificates\",\"iam:GetServerCertificate\",\"waf-regional:GetWebACL\",\"waf-regional:GetWebACLForResource\",\"waf-regional:AssociateWebACL\",\"waf-regional:DisassociateWebACL\",\"wafv2:GetWebACL\",\"wafv2:GetWebACLForResource\",\"wafv2:AssociateWebACL\",\"wafv2:DisassociateWebACL\",\"shield:GetSubscriptionState\",\"shield:DescribeProtection\",\"shield:CreateProtection\",\"shield:DeleteProtection\"],\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"ec2:AuthorizeSecurityGroupIngress\",\"ec2:RevokeSecurityGroupIngress\"],\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"ec2:CreateSecurityGroup\"],\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"ec2:CreateTags\"],\"Condition\":{\"Null\":{\"aws:RequestTag/elbv2.k8s.aws/cluster\":\"false\"},\"StringEquals\":{\"ec2:CreateAction\":\"CreateSecurityGroup\"}},\"Effect\":\"Allow\",\"Resource\":\"arn:aws:ec2:*:*:security-group/*\"},{\"Action\":[\"ec2:CreateTags\",\"ec2:DeleteTags\"],\"Condition\":{\"Null\":{\"aws:RequestTag/elbv2.k8s.aws/cluster\":\"true\",\"aws:ResourceTag/elbv2.k8s.aws/cluster\":\"false\"}},\"Effect\":\"Allow\",\"Resource\":\"arn:aws:ec2:*:*:security-group/*\"},{\"Action\":[\"ec2:AuthorizeSecurityGroupIngress\",\"ec2:RevokeSecurityGroupIngress\",\"ec2:DeleteSecurityGroup\"],\"Condition\":{\"Null\":{\"aws:ResourceTag/elbv2.k8s.aws/cluster\":\"false\"}},\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"elasticloadbalancing:CreateLoadBalancer\",\"elasticloadbalancing:CreateTargetGroup\"],\"Condition\":{\"Null\":{\"aws:RequestTag/elbv2.k8s.aws/cluster\":\"false\"}},\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"elasticloadbalancing:CreateListener\",\"elasticloadbalancing:DeleteListener\",\"elasticloadbalancing:CreateRule\",\"elasticloadbalancing:DeleteRule\"],\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"elasticloadbalancing:AddTags\",\"elasticloadbalancing:RemoveTags\"],\"Condition\":{\"Null\":{\"aws:RequestTag/elbv2.k8s.aws/cluster\":\"true\",\"aws:ResourceTag/elbv2.k8s.aws/cluster\":\"false\"}},\"Effect\":\"Allow\",\"Resource\":[\"arn:aws:elasticloadbalancing:*:*:targetgroup/*/*\",\"arn:aws:elasticloadbalancing:*:*:loadbalancer/net/*/*\",\"arn:aws:elasticloadbalancing:*:*:loadbalancer/app/*/*\"]},{\"Action\":[\"elasticloadbalancing:AddTags\",\"elasticloadbalancing:RemoveTags\"],\"Effect\":\"Allow\",\"Resource\":[\"arn:aws:elasticloadbalancing:*:*:listener/net/*/*/*\",\"arn:aws:elasticloadbalancing:*:*:listener/app/*/*/*\",\"arn:aws:elasticloadbalancing:*:*:listener-rule/net/*/*/*\",\"arn:aws:elasticloadbalancing:*:*:listener-rule/app/*/*/*\"]},{\"Action\":[\"elasticloadbalancing:ModifyLoadBalancerAttributes\",\"elasticloadbalancing:SetIpAddressType\",\"elasticloadbalancing:SetSecurityGroups\",\"elasticloadbalancing:SetSubnets\",\"elasticloadbalancing:DeleteLoadBalancer\",\"elasticloadbalancing:ModifyTargetGroup\",\"elasticloadbalancing:ModifyTargetGroupAttributes\",\"elasticloadbalancing:DeleteTargetGroup\"],\"Condition\":{\"Null\":{\"aws:ResourceTag/elbv2.k8s.aws/cluster\":\"false\"}},\"Effect\":\"Allow\",\"Resource\":\"*\"},{\"Action\":[\"elasticloadbalancing:RegisterTargets\",\"elasticloadbalancing:DeregisterTargets\"],\"Effect\":\"Allow\",\"Resource\":\"arn:aws:elasticloadbalancing:*:*:targetgroup/*/*\"},{\"Action\":[\"elasticloadbalancing:SetWebAcl\",\"elasticloadbalancing:ModifyListener\",\"elasticloadbalancing:AddListenerCertificates\",\"elasticloadbalancing:RemoveListenerCertificates\",\"elasticloadbalancing:ModifyRule\"],\"Effect\":\"Allow\",\"Resource\":\"*\"}],\"Version\":\"2012-10-17\"}",
            "policy_id": "ANPA2AAPUC623TPZ5TOLC",
            "tags": {
              "aws_alb_policy": "1"
            },
            "tags_all": {
              "aws_alb_policy": "1"
            }
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role",
      "name": "AllowExternalDNSUpdates",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::687227541429:role/AllowExternalDNSUpdates2022021301432680130000000b",
            "assume_role_policy": "{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"Federated\":\"arn:aws:iam::687227541429:oidc-provider/oidc.eks.ap-northeast-2.amazonaws.com/id/B7ED8B7607FD4F380A7B07E9F6E13427\"},\"Action\":\"sts:AssumeRoleWithWebIdentity\",\"Condition\":{\"StringEquals\":{\"oidc.eks.ap-northeast-2.amazonaws.com/id/B7ED8B7607FD4F380A7B07E9F6E13427:sub\":\"system:serviceaccount:default:external-dns\"}}}]}",
            "create_date": "2022-02-13T01:43:27Z",
            "description": "",
            "force_detach_policies": false,
            "id": "AllowExternalDNSUpdates2022021301432680130000000b",
            "inline_policy": [
              {
                "name": "",
                "policy": ""
              }
            ],
            "managed_policy_arns": [
              "arn:aws:iam::687227541429:policy/AllowExternalDNSUpdates"
            ],
            "max_session_duration": 3600,
            "name": "AllowExternalDNSUpdates2022021301432680130000000b",
            "name_prefix": "AllowExternalDNSUpdates",
            "path": "/",
            "permissions_boundary": null,
            "tags": {
              "external_dns": "1"
            },
            "tags_all": {
              "external_dns": "1"
            },
            "unique_id": "AROA2AAPUC62URWKZ3I3A"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_subnet.privateEC2Subnet2",
            "data.aws_eks_cluster.cluster",
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.iam-role-eks-cluster",
            "aws_subnet.privateEC2Subnet1",
            "aws_security_group.eks-cluster",
            "aws_vpc.test",
            "data.tls_certificate.cluster",
            "aws_iam_openid_connect_provider.cluster",
            "aws_iam_role_policy_attachment.eks-cluster-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-cluster-AmazonEKSServicePolicy"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role",
      "name": "aws_lb_controller",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::687227541429:role/aws_lb_controller2022021301432680140000000c",
            "assume_role_policy": "{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"Federated\":\"arn:aws:iam::687227541429:oidc-provider/oidc.eks.ap-northeast-2.amazonaws.com/id/B7ED8B7607FD4F380A7B07E9F6E13427\"},\"Action\":\"sts:AssumeRoleWithWebIdentity\",\"Condition\":{\"StringEquals\":{\"oidc.eks.ap-northeast-2.amazonaws.com/id/B7ED8B7607FD4F380A7B07E9F6E13427:sub\":\"system:serviceaccount:kube-system:aws-load-balancer-controller\"}}}]}",
            "create_date": "2022-02-13T01:43:27Z",
            "description": "",
            "force_detach_policies": false,
            "id": "aws_lb_controller2022021301432680140000000c",
            "inline_policy": [
              {
                "name": "",
                "policy": ""
              }
            ],
            "managed_policy_arns": [
              "arn:aws:iam::687227541429:policy/aws_lb_controller20220213013301068000000002"
            ],
            "max_session_duration": 3600,
            "name": "aws_lb_controller2022021301432680140000000c",
            "name_prefix": "aws_lb_controller",
            "path": "/",
            "permissions_boundary": null,
            "tags": {
              "aws_alb_role": "1"
            },
            "tags_all": {
              "aws_alb_role": "1"
            },
            "unique_id": "AROA2AAPUC62Y6S5NEEP4"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_openid_connect_provider.cluster",
            "aws_iam_role_policy_attachment.eks-cluster-AmazonEKSClusterPolicy",
            "aws_subnet.privateEC2Subnet2",
            "data.tls_certificate.cluster",
            "aws_subnet.privateEC2Subnet1",
            "aws_vpc.test",
            "data.aws_eks_cluster.cluster",
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.iam-role-eks-cluster",
            "aws_iam_role_policy_attachment.eks-cluster-AmazonEKSServicePolicy",
            "aws_security_group.eks-cluster"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role",
      "name": "eks_nodes",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::687227541429:role/eks-node-group",
            "assume_role_policy": "{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"ec2.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}",
            "create_date": "2022-02-13T01:33:01Z",
            "description": "",
            "force_detach_policies": false,
            "id": "eks-node-group",
            "inline_policy": [
              {
                "name": "",
                "policy": ""
              }
            ],
            "managed_policy_arns": [
              "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly",
              "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy",
              "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy",
              "arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy"
            ],
            "max_session_duration": 3600,
            "name": "eks-node-group",
            "name_prefix": "",
            "path": "/",
            "permissions_boundary": null,
            "tags": {},
            "tags_all": {},
            "unique_id": "AROA2AAPUC62W2JL4KNTH"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role",
      "name": "iam-role-eks-cluster",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::687227541429:role/terraformekscluster",
            "assume_role_policy": "{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"eks.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}",
            "create_date": "2022-02-13T01:33:01Z",
            "description": "",
            "force_detach_policies": false,
            "id": "terraformekscluster",
            "inline_policy": [
              {
                "name": "",
                "policy": ""
              }
            ],
            "managed_policy_arns": [
              "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy",
              "arn:aws:iam::aws:policy/AmazonEKSServicePolicy"
            ],
            "max_session_duration": 3600,
            "name": "terraformekscluster",
            "name_prefix": "",
            "path": "/",
            "permissions_boundary": null,
            "tags": {},
            "tags_all": {},
            "unique_id": "AROA2AAPUC62ZZZLTAWXT"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "AllowExternalDNSUpdates",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "AllowExternalDNSUpdates2022021301432680130000000b-2022021301433164840000000d",
            "policy_arn": "arn:aws:iam::687227541429:policy/AllowExternalDNSUpdates",
            "role": "AllowExternalDNSUpdates2022021301432680130000000b"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_subnet.privateEC2Subnet2",
            "aws_vpc.test",
            "aws_eks_cluster.eks_cluster",
            "aws_iam_openid_connect_provider.cluster",
            "aws_iam_policy.AllowExternalDNSUpdates",
            "aws_iam_role.iam-role-eks-cluster",
            "aws_subnet.privateEC2Subnet1",
            "data.aws_eks_cluster.cluster",
            "data.tls_certificate.cluster",
            "aws_iam_role.AllowExternalDNSUpdates",
            "aws_iam_role_policy_attachment.eks-cluster-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-cluster-AmazonEKSServicePolicy",
            "aws_security_group.eks-cluster"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "AmazonEC2ContainerRegistryReadOnly",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "eks-node-group-20220213013306316300000004",
            "policy_arn": "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly",
            "role": "eks-node-group"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_role.eks_nodes"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "AmazonEKSWorkerNodePolicy",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "eks-node-group-20220213013306326900000006",
            "policy_arn": "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy",
            "role": "eks-node-group"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_role.eks_nodes"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "AmazonEKS_CNI_Policy",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "eks-node-group-20220213013306333000000008",
            "policy_arn": "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy",
            "role": "eks-node-group"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_role.eks_nodes"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "CloudWatchAgentServerPolicy",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "eks-node-group-20220213013306323100000005",
            "policy_arn": "arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy",
            "role": "eks-node-group"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_role.eks_nodes"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "aws_lb_controller",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "aws_lb_controller2022021301432680140000000c-2022021301433171730000000e",
            "policy_arn": "arn:aws:iam::687227541429:policy/aws_lb_controller20220213013301068000000002",
            "role": "aws_lb_controller2022021301432680140000000c"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "data.tls_certificate.cluster",
            "aws_iam_role_policy_attachment.eks-cluster-AmazonEKSClusterPolicy",
            "aws_security_group.eks-cluster",
            "aws_subnet.privateEC2Subnet2",
            "data.aws_eks_cluster.cluster",
            "aws_iam_role.iam-role-eks-cluster",
            "aws_iam_role_policy_attachment.eks-cluster-AmazonEKSServicePolicy",
            "aws_subnet.privateEC2Subnet1",
            "aws_vpc.test",
            "aws_eks_cluster.eks_cluster",
            "aws_iam_openid_connect_provider.cluster",
            "aws_iam_policy.aws_lb_controller",
            "aws_iam_role.aws_lb_controller"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "eks-cluster-AmazonEKSClusterPolicy",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "terraformekscluster-20220213013306331400000007",
            "policy_arn": "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy",
            "role": "terraformekscluster"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_role.iam-role-eks-cluster"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "eks-cluster-AmazonEKSServicePolicy",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "terraformekscluster-20220213013306333300000009",
            "policy_arn": "arn:aws:iam::aws:policy/AmazonEKSServicePolicy",
            "role": "terraformekscluster"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_role.iam-role-eks-cluster"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_instance",
      "name": "bastion",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "ami": "ami-0eb7a369386789460",
            "arn": "arn:aws:ec2:ap-northeast-2:687227541429:instance/i-0aa7e5d11557738ea",
            "associate_public_ip_address": true,
            "availability_zone": "ap-northeast-2a",
            "capacity_reservation_specification": [
              {
                "capacity_reservation_preference": "open",
                "capacity_reservation_target": []
              }
            ],
            "cpu_core_count": 1,
            "cpu_threads_per_core": 1,
            "credit_specification": [
              {
                "cpu_credits": "standard"
              }
            ],
            "disable_api_termination": false,
            "ebs_block_device": [],
            "ebs_optimized": false,
            "enclave_options": [
              {
                "enabled": false
              }
            ],
            "ephemeral_block_device": [],
            "get_password_data": false,
            "hibernation": false,
            "host_id": null,
            "iam_instance_profile": "",
            "id": "i-0aa7e5d11557738ea",
            "instance_initiated_shutdown_behavior": "stop",
            "instance_state": "running",
            "instance_type": "t2.micro",
            "ipv6_address_count": 0,
            "ipv6_addresses": [],
            "key_name": "mysoldesktestkey",
            "launch_template": [],
            "metadata_options": [
              {
                "http_endpoint": "enabled",
                "http_put_response_hop_limit": 1,
                "http_tokens": "optional",
                "instance_metadata_tags": "disabled"
              }
            ],
            "monitoring": false,
            "network_interface": [],
            "outpost_arn": "",
            "password_data": "",
            "placement_group": "",
            "placement_partition_number": null,
            "primary_network_interface_id": "eni-0b16ccfe7b43eca4a",
            "private_dns": "ip-192-168-1-184.ap-northeast-2.compute.internal",
            "private_ip": "192.168.1.184",
            "public_dns": "ec2-3-38-193-94.ap-northeast-2.compute.amazonaws.com",
            "public_ip": "3.38.193.94",
            "root_block_device": [
              {
                "delete_on_termination": true,
                "device_name": "/dev/xvda",
                "encrypted": false,
                "iops": 100,
                "kms_key_id": "",
                "tags": {
                  "Name": "bation-public-ec2-01-vloume-1"
                },
                "throughput": 0,
                "volume_id": "vol-0ad6718c17f102c69",
                "volume_size": 8,
                "volume_type": "gp2"
              }
            ],
            "secondary_private_ips": [],
            "security_groups": [],
            "source_dest_check": true,
            "subnet_id": "subnet-094a20c3b5bdecf18",
            "tags": {
              "Name": "bastion-public-ec2-01"
            },
            "tags_all": {
              "Name": "bastion-public-ec2-01"
            },
            "tenancy": "default",
            "timeouts": null,
            "user_data": null,
            "user_data_base64": null,
            "volume_tags": null,
            "vpc_security_group_ids": [
              "sg-0f1c19a8021875189"
            ]
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6MTIwMDAwMDAwMDAwMCwidXBkYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "aws_subnet.publicSubnet1",
            "aws_vpc.test",
            "data.aws_ami.amazonLinux",
            "aws_security_group.sec-bastion"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_instance",
      "name": "jenkinsserver",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "ami": "ami-0eb7a369386789460",
            "arn": "arn:aws:ec2:ap-northeast-2:687227541429:instance/i-0bfb076fcf37ff24a",
            "associate_public_ip_address": true,
            "availability_zone": "ap-northeast-2a",
            "capacity_reservation_specification": [
              {
                "capacity_reservation_preference": "open",
                "capacity_reservation_target": []
              }
            ],
            "cpu_core_count": 1,
            "cpu_threads_per_core": 1,
            "credit_specification": [
              {
                "cpu_credits": "standard"
              }
            ],
            "disable_api_termination": false,
            "ebs_block_device": [],
            "ebs_optimized": false,
            "enclave_options": [
              {
                "enabled": false
              }
            ],
            "ephemeral_block_device": [],
            "get_password_data": false,
            "hibernation": false,
            "host_id": null,
            "iam_instance_profile": "",
            "id": "i-0bfb076fcf37ff24a",
            "instance_initiated_shutdown_behavior": "stop",
            "instance_state": "running",
            "instance_type": "t2.micro",
            "ipv6_address_count": 0,
            "ipv6_addresses": [],
            "key_name": "mysoldesktestkey",
            "launch_template": [],
            "metadata_options": [
              {
                "http_endpoint": "enabled",
                "http_put_response_hop_limit": 1,
                "http_tokens": "optional",
                "instance_metadata_tags": "disabled"
              }
            ],
            "monitoring": false,
            "network_interface": [],
            "outpost_arn": "",
            "password_data": "",
            "placement_group": "",
            "placement_partition_number": null,
            "primary_network_interface_id": "eni-01994d73fe72cb315",
            "private_dns": "ip-192-168-1-164.ap-northeast-2.compute.internal",
            "private_ip": "192.168.1.164",
            "public_dns": "ec2-3-34-141-247.ap-northeast-2.compute.amazonaws.com",
            "public_ip": "3.34.141.247",
            "root_block_device": [
              {
                "delete_on_termination": true,
                "device_name": "/dev/xvda",
                "encrypted": false,
                "iops": 100,
                "kms_key_id": "",
                "tags": {
                  "Name": "jenkins-public-ec2-02-vloume-2"
                },
                "throughput": 0,
                "volume_id": "vol-00e4fe8e2215ab92b",
                "volume_size": 8,
                "volume_type": "gp2"
              }
            ],
            "secondary_private_ips": [],
            "security_groups": [],
            "source_dest_check": true,
            "subnet_id": "subnet-094a20c3b5bdecf18",
            "tags": {
              "Name": "jenkins-public-ec2-02"
            },
            "tags_all": {
              "Name": "jenkins-public-ec2-02"
            },
            "tenancy": "default",
            "timeouts": null,
            "user_data": "db5848fcc2d2cd8e5af711b534eca38de3c8bcc8",
            "user_data_base64": null,
            "volume_tags": null,
            "vpc_security_group_ids": [
              "sg-0666429dde7082169"
            ]
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6MTIwMDAwMDAwMDAwMCwidXBkYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "aws_security_group.sec-jenkins",
            "aws_subnet.publicSubnet1",
            "aws_vpc.test",
            "data.aws_ami.amazonLinux"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_internet_gateway",
      "name": "testIGW",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"].vpc1",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:ec2:ap-northeast-2:687227541429:internet-gateway/igw-021046453346ef6e7",
            "id": "igw-021046453346ef6e7",
            "owner_id": "687227541429",
            "tags": {
              "Name": "testIGW"
            },
            "tags_all": {
              "Name": "testIGW"
            },
            "vpc_id": "vpc-03307991857eec5c3"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_vpc.test"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_key_pair",
      "name": "terraform-key",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:ap-northeast-2:687227541429:key-pair/mysoldesktestkey",
            "fingerprint": "36:25:f7:88:bb:cd:13:99:7f:92:92:12:3d:6f:b5:51",
            "id": "mysoldesktestkey",
            "key_name": "mysoldesktestkey",
            "key_name_prefix": "",
            "key_pair_id": "key-04c70bdbde69ef6d0",
            "public_key": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDWlz/FAa6Llv/p82UbCBqAiBW0Ws/1VYUffEwOXxBF/5m3ykV1LHuwENx6GFOxYFwdmOXf+urYrxbq48YNJZ7+yUO+t8Z1DI9l6iNS2icdamPR5vG5bOB+122eBdjNdmbHTTFdMInuwX6EtDrGOfEdAg3COLnzgzLe1tIYTZjInss337B44Qu7GI5hFfbaj5aHPIBgyg3h7PhmGd6oJg3hgbrEIFdzUSWppvFKGsLRiOQrbnALb/CEgDcaGB6jc3evVe53uYK/xzsXGIbygekY5dfjtxOzuaYzJsE1q3+UY8yQi2ZEIk/bA7uVsZQUUeMH2ksET0B8+MRmbv0dx+PjTE3zdUbJ7sD5O0/Z1fCgiawxknat9zmrhyggK+zE02ah3qqWJjPbrCaOYmg/FY4BtwLjcEugkNFljpqD41YZEujzaKWiPHYYTX1MqMGQT3uEacm5r3NFadb/jCSwn/oEaUEcIjukJWIz5wh4O3AwREsHOBDRO7gaemN83MiTbJXWLeqbW4WVAcGdVGHHOZBHHroHGu7+E2Z8vcnC1IQwJvYO1dudsRKJxIT0Fpd0ytnsLELMN6Suh4xf+7NWBpYdf85115E2QsJz4FAEotW027LU2lCc9uxNGhdQ/vwzaUL3ny8VO9HhHxQ2EtpyJPxHEE2QPhJ6or3SXd7Ew9qTYQ== root@localhost.localdomain",
            "tags": {},
            "tags_all": {}
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_nat_gateway",
      "name": "tf-natgw",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"].vpc1",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "allocation_id": "eipalloc-054cbdbb9c5632faa",
            "connectivity_type": "public",
            "id": "nat-006c0948229e5a12a",
            "network_interface_id": "eni-090ff9a2a7d203b0e",
            "private_ip": "192.168.2.158",
            "public_ip": "3.37.152.8",
            "subnet_id": "subnet-0bc1494d344a72f7a",
            "tags": {
              "Name": "tf-natgw"
            },
            "tags_all": {
              "Name": "tf-natgw"
            }
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_eip.nateip",
            "aws_subnet.publicSubnet2",
            "aws_vpc.test"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_route53_record",
      "name": "kubedns",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "index_key": "kubedns.click",
          "schema_version": 2,
          "attributes": {
            "alias": [],
            "allow_overwrite": true,
            "failover_routing_policy": [],
            "fqdn": "_d639929d59e4909daf7f06659708fe50.kubedns.click",
            "geolocation_routing_policy": [],
            "health_check_id": "",
            "id": "Z096848533MX8215J1WYJ__d639929d59e4909daf7f06659708fe50.kubedns.click._CNAME",
            "latency_routing_policy": [],
            "multivalue_answer_routing_policy": null,
            "name": "_d639929d59e4909daf7f06659708fe50.kubedns.click",
            "records": [
              "_ed0671414b4f0d83bac8eece608cee88.bgpjyrktby.acm-validations.aws."
            ],
            "set_identifier": "",
            "ttl": 60,
            "type": "CNAME",
            "weighted_routing_policy": [],
            "zone_id": "Z096848533MX8215J1WYJ"
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjIifQ==",
          "dependencies": [
            "aws_acm_certificate.kubedns",
            "aws_route53_zone.kubedns"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_route53_zone",
      "name": "kubedns",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:route53:::hostedzone/Z096848533MX8215J1WYJ",
            "comment": "Managed by Terraform",
            "delegation_set_id": "",
            "force_destroy": true,
            "id": "Z096848533MX8215J1WYJ",
            "name": "kubedns.click",
            "name_servers": [
              "ns-1167.awsdns-17.org",
              "ns-1831.awsdns-36.co.uk",
              "ns-429.awsdns-53.com",
              "ns-788.awsdns-34.net"
            ],
            "tags": {},
            "tags_all": {},
            "vpc": [],
            "zone_id": "Z096848533MX8215J1WYJ"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_route_table",
      "name": "testPrivateRTb",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:ec2:ap-northeast-2:687227541429:route-table/rtb-07d829a917e4a2ef5",
            "id": "rtb-07d829a917e4a2ef5",
            "owner_id": "687227541429",
            "propagating_vgws": [],
            "route": [
              {
                "carrier_gateway_id": "",
                "cidr_block": "0.0.0.0/0",
                "destination_prefix_list_id": "",
                "egress_only_gateway_id": "",
                "gateway_id": "nat-006c0948229e5a12a",
                "instance_id": "",
                "ipv6_cidr_block": "",
                "local_gateway_id": "",
                "nat_gateway_id": "",
                "network_interface_id": "",
                "transit_gateway_id": "",
                "vpc_endpoint_id": "",
                "vpc_peering_connection_id": ""
              }
            ],
            "tags": {
              "Name": "test-private-rtb"
            },
            "tags_all": {
              "Name": "test-private-rtb"
            },
            "timeouts": null,
            "vpc_id": "vpc-03307991857eec5c3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozMDAwMDAwMDAwMDAsImRlbGV0ZSI6MzAwMDAwMDAwMDAwLCJ1cGRhdGUiOjEyMDAwMDAwMDAwMH19",
          "dependencies": [
            "aws_nat_gateway.tf-natgw",
            "aws_vpc.test"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_route_table",
      "name": "testPublicRTb",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"].vpc1",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:ec2:ap-northeast-2:687227541429:route-table/rtb-0f6d6e3ff11dd9a52",
            "id": "rtb-0f6d6e3ff11dd9a52",
            "owner_id": "687227541429",
            "propagating_vgws": [],
            "route": [
              {
                "carrier_gateway_id": "",
                "cidr_block": "0.0.0.0/0",
                "destination_prefix_list_id": "",
                "egress_only_gateway_id": "",
                "gateway_id": "igw-021046453346ef6e7",
                "instance_id": "",
                "ipv6_cidr_block": "",
                "local_gateway_id": "",
                "nat_gateway_id": "",
                "network_interface_id": "",
                "transit_gateway_id": "",
                "vpc_endpoint_id": "",
                "vpc_peering_connection_id": ""
              }
            ],
            "tags": {
              "Name": "test-public-rtb"
            },
            "tags_all": {
              "Name": "test-public-rtb"
            },
            "timeouts": null,
            "vpc_id": "vpc-03307991857eec5c3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozMDAwMDAwMDAwMDAsImRlbGV0ZSI6MzAwMDAwMDAwMDAwLCJ1cGRhdGUiOjEyMDAwMDAwMDAwMH19",
          "dependencies": [
            "aws_internet_gateway.testIGW",
            "aws_vpc.test"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_route_table_association",
      "name": "privateRTbAssociation01",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "gateway_id": "",
            "id": "rtbassoc-00584b663ba982d78",
            "route_table_id": "rtb-07d829a917e4a2ef5",
            "subnet_id": "subnet-083bd41dff266b7f9"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_eip.nateip",
            "aws_nat_gateway.tf-natgw",
            "aws_route_table.testPrivateRTb",
            "aws_subnet.privateEC2Subnet1",
            "aws_subnet.publicSubnet2",
            "aws_vpc.test"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_route_table_association",
      "name": "privateRTbAssociation02",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "gateway_id": "",
            "id": "rtbassoc-0a166e0d4e14a09cd",
            "route_table_id": "rtb-07d829a917e4a2ef5",
            "subnet_id": "subnet-07f100a568c2c9fd0"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_subnet.privateEC2Subnet2",
            "aws_subnet.publicSubnet2",
            "aws_vpc.test",
            "aws_eip.nateip",
            "aws_nat_gateway.tf-natgw",
            "aws_route_table.testPrivateRTb"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_route_table_association",
      "name": "privateRTbAssociation03",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "gateway_id": "",
            "id": "rtbassoc-056c24eb29d6ca5e8",
            "route_table_id": "rtb-07d829a917e4a2ef5",
            "subnet_id": "subnet-0b932828cb06470a9"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_eip.nateip",
            "aws_nat_gateway.tf-natgw",
            "aws_route_table.testPrivateRTb",
            "aws_subnet.privateRDSSubnet1",
            "aws_subnet.publicSubnet2",
            "aws_vpc.test"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_route_table_association",
      "name": "privateRTbAssociation04",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "gateway_id": "",
            "id": "rtbassoc-07c89c619b221574b",
            "route_table_id": "rtb-07d829a917e4a2ef5",
            "subnet_id": "subnet-0f2a46b9231d1b062"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_route_table.testPrivateRTb",
            "aws_subnet.privateRDSSubnet2",
            "aws_subnet.publicSubnet2",
            "aws_vpc.test",
            "aws_eip.nateip",
            "aws_nat_gateway.tf-natgw"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_route_table_association",
      "name": "publicRTbAssociation01",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "gateway_id": "",
            "id": "rtbassoc-0cbc2bee14593d74e",
            "route_table_id": "rtb-0f6d6e3ff11dd9a52",
            "subnet_id": "subnet-094a20c3b5bdecf18"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_vpc.test",
            "aws_internet_gateway.testIGW",
            "aws_route_table.testPublicRTb",
            "aws_subnet.publicSubnet1"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_route_table_association",
      "name": "publicRTbAssociation02",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "gateway_id": "",
            "id": "rtbassoc-07ce0d004a215ac33",
            "route_table_id": "rtb-0f6d6e3ff11dd9a52",
            "subnet_id": "subnet-0bc1494d344a72f7a"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_internet_gateway.testIGW",
            "aws_route_table.testPublicRTb",
            "aws_subnet.publicSubnet2",
            "aws_vpc.test"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_s3_bucket",
      "name": "testS3",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "acceleration_status": "",
            "acl": "private",
            "arn": "arn:aws:s3:::cass20220209-terraform-bucket",
            "bucket": "cass20220209-terraform-bucket",
            "bucket_domain_name": "cass20220209-terraform-bucket.s3.amazonaws.com",
            "bucket_prefix": null,
            "bucket_regional_domain_name": "cass20220209-terraform-bucket.s3.ap-northeast-2.amazonaws.com",
            "cors_rule": [],
            "force_destroy": false,
            "grant": [],
            "hosted_zone_id": "Z3W03O7B5YMIYP",
            "id": "cass20220209-terraform-bucket",
            "lifecycle_rule": [
              {
                "abort_incomplete_multipart_upload_days": 0,
                "enabled": true,
                "expiration": [],
                "id": "tf-s3-lifecycle-20220213013303693000000003",
                "noncurrent_version_expiration": [
                  {
                    "days": 180
                  }
                ],
                "noncurrent_version_transition": [],
                "prefix": "image/",
                "tags": {},
                "transition": []
              }
            ],
            "logging": [],
            "object_lock_configuration": [],
            "policy": null,
            "region": "ap-northeast-2",
            "replication_configuration": [],
            "request_payer": "BucketOwner",
            "server_side_encryption_configuration": [],
            "tags": {
              "Name": "terracass-s3.tf"
            },
            "tags_all": {
              "Name": "terracass-s3.tf"
            },
            "versioning": [
              {
                "enabled": true,
                "mfa_delete": false
              }
            ],
            "website": [],
            "website_domain": null,
            "website_endpoint": null
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_security_group",
      "name": "eks-cluster",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:ap-northeast-2:687227541429:security-group/sg-0c562377ad26baf00",
            "description": "Managed by Terraform",
            "egress": [
              {
                "cidr_blocks": [
                  "0.0.0.0/0"
                ],
                "description": "",
                "from_port": 0,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "-1",
                "security_groups": [],
                "self": false,
                "to_port": 0
              }
            ],
            "id": "sg-0c562377ad26baf00",
            "ingress": [
              {
                "cidr_blocks": [
                  "0.0.0.0/0"
                ],
                "description": "",
                "from_port": 0,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "-1",
                "security_groups": [],
                "self": false,
                "to_port": 0
              },
              {
                "cidr_blocks": [
                  "0.0.0.0/0"
                ],
                "description": "",
                "from_port": 3306,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "tcp",
                "security_groups": [],
                "self": false,
                "to_port": 3306
              }
            ],
            "name": "SG-eks-cluster",
            "name_prefix": "",
            "owner_id": "687227541429",
            "revoke_rules_on_delete": false,
            "tags": {},
            "tags_all": {},
            "timeouts": null,
            "vpc_id": "vpc-03307991857eec5c3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6OTAwMDAwMDAwMDAwfSwic2NoZW1hX3ZlcnNpb24iOiIxIn0=",
          "dependencies": [
            "aws_vpc.test"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_security_group",
      "name": "sec-bastion",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:ap-northeast-2:687227541429:security-group/sg-0f1c19a8021875189",
            "description": "Managed by Terraform",
            "egress": [
              {
                "cidr_blocks": [
                  "0.0.0.0/0"
                ],
                "description": "",
                "from_port": 0,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "-1",
                "security_groups": [],
                "self": false,
                "to_port": 0
              }
            ],
            "id": "sg-0f1c19a8021875189",
            "ingress": [
              {
                "cidr_blocks": [
                  "0.0.0.0/0"
                ],
                "description": "",
                "from_port": 22,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "tcp",
                "security_groups": [],
                "self": false,
                "to_port": 22
              }
            ],
            "name": "sec-bastion",
            "name_prefix": "",
            "owner_id": "687227541429",
            "revoke_rules_on_delete": false,
            "tags": {},
            "tags_all": {},
            "timeouts": null,
            "vpc_id": "vpc-03307991857eec5c3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6OTAwMDAwMDAwMDAwfSwic2NoZW1hX3ZlcnNpb24iOiIxIn0=",
          "dependencies": [
            "aws_vpc.test"
          ],
          "create_before_destroy": true
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_security_group",
      "name": "sec-jenkins",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:ap-northeast-2:687227541429:security-group/sg-0666429dde7082169",
            "description": "Managed by Terraform",
            "egress": [
              {
                "cidr_blocks": [
                  "0.0.0.0/0"
                ],
                "description": "",
                "from_port": 0,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "-1",
                "security_groups": [],
                "self": false,
                "to_port": 0
              }
            ],
            "id": "sg-0666429dde7082169",
            "ingress": [
              {
                "cidr_blocks": [
                  "0.0.0.0/0"
                ],
                "description": "",
                "from_port": 22,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "tcp",
                "security_groups": [],
                "self": false,
                "to_port": 22
              },
              {
                "cidr_blocks": [
                  "0.0.0.0/0"
                ],
                "description": "",
                "from_port": 8080,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "tcp",
                "security_groups": [],
                "self": false,
                "to_port": 8080
              }
            ],
            "name": "sec-jenkins",
            "name_prefix": "",
            "owner_id": "687227541429",
            "revoke_rules_on_delete": false,
            "tags": {},
            "tags_all": {},
            "timeouts": null,
            "vpc_id": "vpc-03307991857eec5c3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6OTAwMDAwMDAwMDAwfSwic2NoZW1hX3ZlcnNpb24iOiIxIn0=",
          "dependencies": [
            "aws_vpc.test"
          ],
          "create_before_destroy": true
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_security_group",
      "name": "sec-peer",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"].vpc1",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:ap-northeast-2:687227541429:security-group/sg-01d12b72920e4f0eb",
            "description": "Managed by Terraform",
            "egress": [
              {
                "cidr_blocks": [
                  "0.0.0.0/0"
                ],
                "description": "",
                "from_port": 0,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "-1",
                "security_groups": [],
                "self": false,
                "to_port": 0
              }
            ],
            "id": "sg-01d12b72920e4f0eb",
            "ingress": [
              {
                "cidr_blocks": [
                  "0.0.0.0/0"
                ],
                "description": "",
                "from_port": 0,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "-1",
                "security_groups": [],
                "self": false,
                "to_port": 0
              }
            ],
            "name": "sgpeer1",
            "name_prefix": "",
            "owner_id": "687227541429",
            "revoke_rules_on_delete": false,
            "tags": {},
            "tags_all": {},
            "timeouts": null,
            "vpc_id": "vpc-03307991857eec5c3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6OTAwMDAwMDAwMDAwfSwic2NoZW1hX3ZlcnNpb24iOiIxIn0=",
          "dependencies": [
            "aws_vpc.test"
          ],
          "create_before_destroy": true
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_security_group",
      "name": "sec_rds",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:ap-northeast-2:687227541429:security-group/sg-0143056256ae63629",
            "description": "Used for RDS",
            "egress": [
              {
                "cidr_blocks": [
                  "0.0.0.0/0"
                ],
                "description": "",
                "from_port": 0,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "-1",
                "security_groups": [],
                "self": false,
                "to_port": 0
              }
            ],
            "id": "sg-0143056256ae63629",
            "ingress": [
              {
                "cidr_blocks": [
                  "0.0.0.0/0"
                ],
                "description": "",
                "from_port": 3306,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "tcp",
                "security_groups": [],
                "self": false,
                "to_port": 3306
              }
            ],
            "name": "sec_rds",
            "name_prefix": "",
            "owner_id": "687227541429",
            "revoke_rules_on_delete": false,
            "tags": {},
            "tags_all": {},
            "timeouts": null,
            "vpc_id": "vpc-03307991857eec5c3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6OTAwMDAwMDAwMDAwfSwic2NoZW1hX3ZlcnNpb24iOiIxIn0=",
          "dependencies": [
            "aws_vpc.test"
          ],
          "create_before_destroy": true
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_subnet",
      "name": "privateEC2Subnet1",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"].vpc1",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:ap-northeast-2:687227541429:subnet/subnet-083bd41dff266b7f9",
            "assign_ipv6_address_on_creation": false,
            "availability_zone": "ap-northeast-2a",
            "availability_zone_id": "apne2-az1",
            "cidr_block": "192.168.3.0/24",
            "customer_owned_ipv4_pool": "",
            "enable_dns64": false,
            "enable_resource_name_dns_a_record_on_launch": false,
            "enable_resource_name_dns_aaaa_record_on_launch": false,
            "id": "subnet-083bd41dff266b7f9",
            "ipv6_cidr_block": "",
            "ipv6_cidr_block_association_id": "",
            "ipv6_native": false,
            "map_customer_owned_ip_on_launch": false,
            "map_public_ip_on_launch": false,
            "outpost_arn": "",
            "owner_id": "687227541429",
            "private_dns_hostname_type_on_launch": "ip-name",
            "tags": {
              "Name": "test-private-ec2-subnet-01"
            },
            "tags_all": {
              "Name": "test-private-ec2-subnet-01"
            },
            "timeouts": null,
            "vpc_id": "vpc-03307991857eec5c3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiMSJ9",
          "dependencies": [
            "aws_vpc.test"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_subnet",
      "name": "privateEC2Subnet2",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"].vpc1",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:ap-northeast-2:687227541429:subnet/subnet-07f100a568c2c9fd0",
            "assign_ipv6_address_on_creation": false,
            "availability_zone": "ap-northeast-2c",
            "availability_zone_id": "apne2-az3",
            "cidr_block": "192.168.4.0/24",
            "customer_owned_ipv4_pool": "",
            "enable_dns64": false,
            "enable_resource_name_dns_a_record_on_launch": false,
            "enable_resource_name_dns_aaaa_record_on_launch": false,
            "id": "subnet-07f100a568c2c9fd0",
            "ipv6_cidr_block": "",
            "ipv6_cidr_block_association_id": "",
            "ipv6_native": false,
            "map_customer_owned_ip_on_launch": false,
            "map_public_ip_on_launch": false,
            "outpost_arn": "",
            "owner_id": "687227541429",
            "private_dns_hostname_type_on_launch": "ip-name",
            "tags": {
              "Name": "test-private-ec2-subnet-02"
            },
            "tags_all": {
              "Name": "test-private-ec2-subnet-02"
            },
            "timeouts": null,
            "vpc_id": "vpc-03307991857eec5c3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiMSJ9",
          "dependencies": [
            "aws_vpc.test"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_subnet",
      "name": "privateRDSSubnet1",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"].vpc1",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:ap-northeast-2:687227541429:subnet/subnet-0b932828cb06470a9",
            "assign_ipv6_address_on_creation": false,
            "availability_zone": "ap-northeast-2a",
            "availability_zone_id": "apne2-az1",
            "cidr_block": "192.168.5.0/24",
            "customer_owned_ipv4_pool": "",
            "enable_dns64": false,
            "enable_resource_name_dns_a_record_on_launch": false,
            "enable_resource_name_dns_aaaa_record_on_launch": false,
            "id": "subnet-0b932828cb06470a9",
            "ipv6_cidr_block": "",
            "ipv6_cidr_block_association_id": "",
            "ipv6_native": false,
            "map_customer_owned_ip_on_launch": false,
            "map_public_ip_on_launch": false,
            "outpost_arn": "",
            "owner_id": "687227541429",
            "private_dns_hostname_type_on_launch": "ip-name",
            "tags": {
              "Name": "test-private-rds-subnet-01"
            },
            "tags_all": {
              "Name": "test-private-rds-subnet-01"
            },
            "timeouts": null,
            "vpc_id": "vpc-03307991857eec5c3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiMSJ9",
          "dependencies": [
            "aws_vpc.test"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_subnet",
      "name": "privateRDSSubnet2",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"].vpc1",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:ap-northeast-2:687227541429:subnet/subnet-0f2a46b9231d1b062",
            "assign_ipv6_address_on_creation": false,
            "availability_zone": "ap-northeast-2c",
            "availability_zone_id": "apne2-az3",
            "cidr_block": "192.168.6.0/24",
            "customer_owned_ipv4_pool": "",
            "enable_dns64": false,
            "enable_resource_name_dns_a_record_on_launch": false,
            "enable_resource_name_dns_aaaa_record_on_launch": false,
            "id": "subnet-0f2a46b9231d1b062",
            "ipv6_cidr_block": "",
            "ipv6_cidr_block_association_id": "",
            "ipv6_native": false,
            "map_customer_owned_ip_on_launch": false,
            "map_public_ip_on_launch": false,
            "outpost_arn": "",
            "owner_id": "687227541429",
            "private_dns_hostname_type_on_launch": "ip-name",
            "tags": {
              "Name": "test-private-rds-subnet-02"
            },
            "tags_all": {
              "Name": "test-private-rds-subnet-02"
            },
            "timeouts": null,
            "vpc_id": "vpc-03307991857eec5c3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiMSJ9",
          "dependencies": [
            "aws_vpc.test"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_subnet",
      "name": "publicSubnet1",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"].vpc1",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:ap-northeast-2:687227541429:subnet/subnet-094a20c3b5bdecf18",
            "assign_ipv6_address_on_creation": false,
            "availability_zone": "ap-northeast-2a",
            "availability_zone_id": "apne2-az1",
            "cidr_block": "192.168.1.0/24",
            "customer_owned_ipv4_pool": "",
            "enable_dns64": false,
            "enable_resource_name_dns_a_record_on_launch": false,
            "enable_resource_name_dns_aaaa_record_on_launch": false,
            "id": "subnet-094a20c3b5bdecf18",
            "ipv6_cidr_block": "",
            "ipv6_cidr_block_association_id": "",
            "ipv6_native": false,
            "map_customer_owned_ip_on_launch": false,
            "map_public_ip_on_launch": true,
            "outpost_arn": "",
            "owner_id": "687227541429",
            "private_dns_hostname_type_on_launch": "ip-name",
            "tags": {
              "Name": "test-public-subnet-01",
              "kubernetes.io/cluster/terraformEKScluster": "shared",
              "kubernetes.io/role/elb": "1"
            },
            "tags_all": {
              "Name": "test-public-subnet-01",
              "kubernetes.io/cluster/terraformEKScluster": "shared",
              "kubernetes.io/role/elb": "1"
            },
            "timeouts": null,
            "vpc_id": "vpc-03307991857eec5c3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiMSJ9",
          "dependencies": [
            "aws_vpc.test"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_subnet",
      "name": "publicSubnet2",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"].vpc1",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:ap-northeast-2:687227541429:subnet/subnet-0bc1494d344a72f7a",
            "assign_ipv6_address_on_creation": false,
            "availability_zone": "ap-northeast-2c",
            "availability_zone_id": "apne2-az3",
            "cidr_block": "192.168.2.0/24",
            "customer_owned_ipv4_pool": "",
            "enable_dns64": false,
            "enable_resource_name_dns_a_record_on_launch": false,
            "enable_resource_name_dns_aaaa_record_on_launch": false,
            "id": "subnet-0bc1494d344a72f7a",
            "ipv6_cidr_block": "",
            "ipv6_cidr_block_association_id": "",
            "ipv6_native": false,
            "map_customer_owned_ip_on_launch": false,
            "map_public_ip_on_launch": true,
            "outpost_arn": "",
            "owner_id": "687227541429",
            "private_dns_hostname_type_on_launch": "ip-name",
            "tags": {
              "Name": "test-public-subnet-02",
              "kubernetes.io/cluster/terraformEKScluster": "shared",
              "kubernetes.io/role/elb": "1"
            },
            "tags_all": {
              "Name": "test-public-subnet-02",
              "kubernetes.io/cluster/terraformEKScluster": "shared",
              "kubernetes.io/role/elb": "1"
            },
            "timeouts": null,
            "vpc_id": "vpc-03307991857eec5c3"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiMSJ9",
          "dependencies": [
            "aws_vpc.test"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_vpc",
      "name": "test",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"].vpc1",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:ap-northeast-2:687227541429:vpc/vpc-03307991857eec5c3",
            "assign_generated_ipv6_cidr_block": false,
            "cidr_block": "192.168.0.0/16",
            "default_network_acl_id": "acl-0d2e1a0276c4bb4ff",
            "default_route_table_id": "rtb-0624533bbaf1019ba",
            "default_security_group_id": "sg-0b83e2a8cc204f219",
            "dhcp_options_id": "dopt-0d1bd47e1db7fa646",
            "enable_classiclink": false,
            "enable_classiclink_dns_support": false,
            "enable_dns_hostnames": true,
            "enable_dns_support": true,
            "id": "vpc-03307991857eec5c3",
            "instance_tenancy": "default",
            "ipv4_ipam_pool_id": null,
            "ipv4_netmask_length": null,
            "ipv6_association_id": "",
            "ipv6_cidr_block": "",
            "ipv6_cidr_block_network_border_group": "",
            "ipv6_ipam_pool_id": "",
            "ipv6_netmask_length": 0,
            "main_route_table_id": "rtb-0624533bbaf1019ba",
            "owner_id": "687227541429",
            "tags": {
              "Name": "terraform-test-vpc"
            },
            "tags_all": {
              "Name": "terraform-test-vpc"
            }
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "create_before_destroy": true
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "argo-cd",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "argo-cd",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "argo-cd",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "v2.2.5",
                "chart": "argo-cd",
                "name": "argo-cd",
                "namespace": "argocd",
                "revision": 1,
                "values": "{\"apiVersionOverrides\":{\"certmanager\":\"\",\"ingress\":\"\"},\"configs\":{\"clusterCredentials\":[],\"credentialTemplates\":{},\"gpgKeys\":{},\"gpgKeysAnnotations\":{},\"knownHosts\":{\"data\":{\"ssh_known_hosts\":\"bitbucket.org ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAubiN81eDcafrgMeLzaFPsw2kNvEcqTKl/VqLat/MaB33pZy0y3rJZtnqwR2qOOvbwKZYKiEO1O6VqNEBxKvJJelCq0dTXWT5pbO2gDXC6h6QDXCaHo6pOHGPUy+YBaGQRGuSusMEASYiWunYN0vCAI8QaXnWMXNMdFP3jHAJH0eDsoiGnLPBlBp4TNm6rYI74nMzgz3B9IikW4WVK+dc8KZJZWYjAuORU3jc1c/NPskD2ASinf8v3xnfXeukU0sJ5N6m5E8VLjObPEO+mN2t/FZTMZLiFqPWc/ALSqnMnnhwrNi2rbfg/rd/IpL8Le3pSBne8+seeFVBoGqzHM9yXw==\\ngithub.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg=\\ngithub.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOMqqnkVzrm0SdG6UOoqKLsabgH5C9okWi0dh2l9GKJl\\ngithub.com ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAq2A7hRGmdnm9tUDbO9IDSwBK6TbQa+PXYPCPy6rbTrTtw7PHkccKrpp0yVhp5HdEIcKr6pLlVDBfOLX9QUsyCOV0wzfjIJNlGEYsdlLJizHhbn2mUjvSAHQqZETYP81eFzLQNnPHt4EVVUh7VfDESU84KezmD5QlWpXLmvU31/yMf+Se8xhHTvKSCZIFImWwoG6mbUoWf9nzpIoaSjB+weqqUUmpaaasXVal72J+UX2B+2RPW3RcT0eOzQgqlJL3RKrTJvdsjE3JEAvGq3lGHSZXy28G3skua2SmVi/w4yCE6gbODqnTWlg7+wC604ydGXA8VJiS5ap43JXiUFFAaQ==\\ngitlab.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBFSMqzJeV9rUzU4kWitGjeR4PWSa29SPqJ1fVkhtj3Hw9xjLVXVYrU9QlYWrOLXBpQ6KWjbjTDTdDkoohFzgbEY=\\ngitlab.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAfuCHKVTjquxvt6CM6tdG4SLp1Btn/nOeHHE5UOzRdf\\ngitlab.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCsj2bNKTBSpIYDEGk9KxsGh3mySTRgMtXL583qmBpzeQ+jqCMRgBqB98u3z++J1sKlXHWfM9dyhSevkMwSbhoR8XIq/U0tCNyokEi/ueaBMCvbcTHhO7FcwzY92WK4Yt0aGROY5qX2UKSeOvuP4D6TPqKF1onrSzH9bx9XUf2lEdWT/ia1NEKjunUqu1xOB/StKDHMoX4/OKyIzuS0q/T1zOATthvasJFoPrAjkohTyaDUz2LN5JoH839hViyEG82yB+MjcFV5MU3N1l1QL3cVUCh93xSaua1N85qivl+siMkPGbO5xR/En4iEY6K2XPASUEMaieWVNTRCtJ4S8H+9\\nssh.dev.azure.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Hr1oTWqNqOlzGJOfGJ4NakVyIzf1rXYd4d7wo6jBlkLvCA4odBlL0mDUyZ0/QUfTTqeu+tm22gOsv+VrVTMk6vwRU75gY/y9ut5Mb3bR5BV58dKXyq9A9UeB5Cakehn5Zgm6x1mKoVyf+FFn26iYqXJRgzIZZcZ5V6hrE0Qg39kZm4az48o0AUbf6Sp4SLdvnuMa2sVNwHBboS7EJkm57XQPVU3/QpyNLHbWDdzwtrlS+ez30S3AdYhLKEOxAG8weOnyrtLJAUen9mTkol8oII1edf7mWWbWVf0nBmly21+nZcmCTISQBtdcyPaEno7fFQMDD26/s0lfKob4Kw8H\\nvs-ssh.visualstudio.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Hr1oTWqNqOlzGJOfGJ4NakVyIzf1rXYd4d7wo6jBlkLvCA4odBlL0mDUyZ0/QUfTTqeu+tm22gOsv+VrVTMk6vwRU75gY/y9ut5Mb3bR5BV58dKXyq9A9UeB5Cakehn5Zgm6x1mKoVyf+FFn26iYqXJRgzIZZcZ5V6hrE0Qg39kZm4az48o0AUbf6Sp4SLdvnuMa2sVNwHBboS7EJkm57XQPVU3/QpyNLHbWDdzwtrlS+ez30S3AdYhLKEOxAG8weOnyrtLJAUen9mTkol8oII1edf7mWWbWVf0nBmly21+nZcmCTISQBtdcyPaEno7fFQMDD26/s0lfKob4Kw8H\\n\"}},\"knownHostsAnnotations\":{},\"repositories\":{},\"repositoryCredentials\":{},\"secret\":{\"annotations\":{},\"argocdServerAdminPassword\":\"\",\"argocdServerAdminPasswordMtime\":\"\",\"argocdServerTlsConfig\":{},\"bitbucketServerSecret\":\"\",\"bitbucketUUID\":\"\",\"createSecret\":true,\"extra\":{},\"githubSecret\":\"\",\"gitlabSecret\":\"\",\"gogsSecret\":\"\"},\"styles\":\"\",\"tlsCerts\":{},\"tlsCertsAnnotations\":{}},\"controller\":{\"affinity\":{},\"args\":{\"appResyncPeriod\":\"180\",\"operationProcessors\":\"10\",\"repoServerTimeoutSeconds\":\"60\",\"selfHealTimeout\":\"5\",\"statusProcessors\":\"20\"},\"clusterAdminAccess\":{\"enabled\":true},\"clusterRoleRules\":{\"enabled\":false,\"rules\":[]},\"containerPort\":8082,\"containerSecurityContext\":{},\"enableStatefulSet\":false,\"env\":[],\"envFrom\":[],\"extraArgs\":[],\"extraContainers\":[],\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"\",\"tag\":\"\"},\"initContainers\":[],\"livenessProbe\":{\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"logFormat\":\"text\",\"logLevel\":\"info\",\"metrics\":{\"applicationLabels\":{\"enabled\":false,\"labels\":{}},\"enabled\":false,\"rules\":{\"enabled\":false,\"spec\":[]},\"service\":{\"annotations\":{},\"labels\":{},\"servicePort\":8082},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":false,\"interval\":\"30s\",\"metricRelabelings\":[],\"namespace\":\"\",\"relabelings\":[],\"selector\":{}}},\"name\":\"application-controller\",\"nodeSelector\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{}},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"readinessProbe\":{\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"replicas\":1,\"resources\":{},\"service\":{\"annotations\":{},\"labels\":{},\"port\":8082,\"portName\":\"https-controller\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"argocd-application-controller\"},\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[],\"volumes\":[]},\"createAggregateRoles\":false,\"dex\":{\"affinity\":{},\"containerPortGrpc\":5557,\"containerPortHttp\":5556,\"containerPortMetrics\":5558,\"containerSecurityContext\":{},\"enabled\":true,\"env\":[],\"envFrom\":[],\"extraContainers\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"imagePullPolicy\":\"IfNotPresent\",\"repository\":\"ghcr.io/dexidp/dex\",\"tag\":\"v2.30.0\"},\"initContainers\":[],\"initImage\":{\"imagePullPolicy\":\"\",\"repository\":\"\",\"tag\":\"\"},\"livenessProbe\":{\"enabled\":false,\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"metrics\":{\"enabled\":false,\"service\":{\"annotations\":{},\"labels\":{}},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":false,\"interval\":\"30s\",\"metricRelabelings\":[],\"namespace\":\"\",\"relabelings\":[],\"selector\":{}}},\"name\":\"dex-server\",\"nodeSelector\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{}},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"readinessProbe\":{\"enabled\":false,\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"resources\":{},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"argocd-dex-server\"},\"servicePortGrpc\":5557,\"servicePortGrpcName\":\"grpc\",\"servicePortHttp\":5556,\"servicePortHttpName\":\"http\",\"servicePortMetrics\":5558,\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[{\"mountPath\":\"/shared\",\"name\":\"static-files\"}],\"volumes\":[{\"emptyDir\":{},\"name\":\"static-files\"}]},\"extraObjects\":[],\"fullnameOverride\":\"\",\"global\":{\"additionalLabels\":{},\"hostAliases\":[],\"image\":{\"imagePullPolicy\":\"IfNotPresent\",\"repository\":\"quay.io/argoproj/argocd\",\"tag\":\"\"},\"imagePullSecrets\":[],\"networkPolicy\":{\"create\":false,\"defaultDenyIngress\":false},\"podAnnotations\":{},\"podLabels\":{},\"securityContext\":{}},\"kubeVersionOverride\":\"\",\"nameOverride\":\"argocd\",\"openshift\":{\"enabled\":false},\"redis\":{\"affinity\":{},\"containerPort\":6379,\"containerSecurityContext\":{},\"enabled\":true,\"env\":[],\"envFrom\":[],\"extraArgs\":[],\"extraContainers\":[],\"image\":{\"imagePullPolicy\":\"IfNotPresent\",\"repository\":\"redis\",\"tag\":\"6.2.6-alpine\"},\"initContainers\":[],\"metrics\":{\"containerPort\":9121,\"enabled\":false,\"image\":{\"imagePullPolicy\":\"IfNotPresent\",\"repository\":\"quay.io/bitnami/redis-exporter\",\"tag\":\"1.26.0-debian-10-r2\"},\"resources\":{},\"service\":{\"annotations\":{},\"clusterIP\":\"None\",\"labels\":{},\"portName\":\"http-metrics\",\"servicePort\":9121,\"type\":\"ClusterIP\"},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":false,\"interval\":\"30s\",\"metricRelabelings\":[],\"namespace\":\"\",\"relabelings\":[],\"selector\":{}}},\"name\":\"redis\",\"nodeSelector\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{}},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"resources\":{},\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":999},\"service\":{\"annotations\":{},\"labels\":{}},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":false,\"create\":false,\"name\":\"\"},\"servicePort\":6379,\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[],\"volumes\":[]},\"redis-ha\":{\"enabled\":false,\"exporter\":{\"enabled\":true},\"haproxy\":{\"enabled\":true,\"metrics\":{\"enabled\":true}},\"image\":{\"tag\":\"6.2.6-alpine\"},\"persistentVolume\":{\"enabled\":false},\"redis\":{\"config\":{\"save\":\"\\\"\\\"\"},\"masterGroupName\":\"argocd\"}},\"repoServer\":{\"affinity\":{},\"autoscaling\":{\"enabled\":false,\"maxReplicas\":5,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50,\"targetMemoryUtilizationPercentage\":50},\"clusterAdminAccess\":{\"enabled\":false},\"clusterRoleRules\":{\"enabled\":false,\"rules\":[]},\"containerPort\":8081,\"containerSecurityContext\":{},\"copyutil\":{\"resources\":{}},\"env\":[],\"envFrom\":[],\"extraArgs\":[],\"extraContainers\":[],\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"\",\"tag\":\"\"},\"initContainers\":[],\"livenessProbe\":{\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"logFormat\":\"text\",\"logLevel\":\"info\",\"metrics\":{\"enabled\":false,\"service\":{\"annotations\":{},\"labels\":{},\"servicePort\":8084},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":false,\"interval\":\"30s\",\"metricRelabelings\":[],\"namespace\":\"\",\"relabelings\":[],\"selector\":{}}},\"name\":\"repo-server\",\"nodeSelector\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{}},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"rbac\":[],\"readinessProbe\":{\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"replicas\":1,\"resources\":{},\"service\":{\"annotations\":{},\"labels\":{},\"port\":8081,\"portName\":\"https-repo-server\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":false,\"name\":\"\"},\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[],\"volumes\":[]},\"server\":{\"GKEbackendConfig\":{\"enabled\":false,\"spec\":{}},\"GKEfrontendConfig\":{\"enabled\":false,\"spec\":{}},\"GKEmanagedCertificate\":{\"domains\":[\"argocd.example.com\"],\"enabled\":false},\"additionalApplications\":[],\"additionalProjects\":[],\"affinity\":{},\"autoscaling\":{\"enabled\":false,\"maxReplicas\":5,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50,\"targetMemoryUtilizationPercentage\":50},\"certificate\":{\"additionalHosts\":[],\"domain\":\"argocd.example.com\",\"enabled\":false,\"issuer\":{\"kind\":null,\"name\":null},\"secretName\":\"argocd-server-tls\"},\"clusterAdminAccess\":{\"enabled\":true},\"config\":{\"application.instanceLabelKey\":\"argocd.argoproj.io/instance\",\"url\":\"https://argocd.example.com\"},\"configAnnotations\":{},\"configEnabled\":true,\"containerPort\":8080,\"containerSecurityContext\":{},\"env\":[],\"envFrom\":[],\"extensions\":{\"contents\":[],\"enabled\":false,\"image\":{\"imagePullPolicy\":\"IfNotPresent\",\"repository\":\"ghcr.io/argoproj-labs/argocd-extensions\",\"tag\":\"v0.1.0\"},\"resources\":{}},\"extraArgs\":[\"--insecure\"],\"extraContainers\":[],\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"\",\"tag\":\"\"},\"ingress\":{\"annotations\":[],\"enabled\":false,\"extraPaths\":[],\"hosts\":[],\"https\":false,\"ingressClassName\":\"\",\"labels\":{},\"pathType\":\"Prefix\",\"paths\":[\"/\"],\"tls\":[]},\"ingressGrpc\":{\"annotations\":{},\"awsALB\":{\"backendProtocolVersion\":\"HTTP2\",\"serviceType\":\"NodePort\"},\"enabled\":false,\"extraPaths\":[],\"hosts\":[],\"https\":false,\"ingressClassName\":\"\",\"isAWSALB\":false,\"labels\":{},\"pathType\":\"Prefix\",\"paths\":[\"/\"],\"tls\":[]},\"initContainers\":[],\"lifecycle\":{},\"livenessProbe\":{\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"logFormat\":\"text\",\"logLevel\":\"info\",\"metrics\":{\"enabled\":false,\"service\":{\"annotations\":{},\"labels\":{},\"servicePort\":8083},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":false,\"interval\":\"30s\",\"metricRelabelings\":[],\"namespace\":\"\",\"relabelings\":[],\"selector\":{}}},\"name\":\"server\",\"nodeSelector\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{}},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"rbacConfig\":{},\"rbacConfigAnnotations\":{},\"rbacConfigCreate\":true,\"readinessProbe\":{\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"replicas\":1,\"resources\":{},\"route\":{\"annotations\":{},\"enabled\":false,\"hostname\":\"\",\"termination_policy\":\"None\",\"termination_type\":\"passthrough\"},\"service\":{\"annotations\":{},\"externalIPs\":[],\"externalTrafficPolicy\":\"\",\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"namedTargetPort\":true,\"nodePortHttp\":30080,\"nodePortHttps\":30443,\"servicePortHttp\":80,\"servicePortHttpName\":\"http\",\"servicePortHttps\":443,\"servicePortHttpsName\":\"https\",\"sessionAffinity\":\"\",\"type\":\"LoadBalancer\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"argocd-server\"},\"staticAssets\":{\"enabled\":true},\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[],\"volumes\":[]}}",
                "version": "3.33.5"
              }
            ],
            "name": "argo-cd",
            "namespace": "argocd",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://argoproj.github.io/argo-helm",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [
              {
                "name": "server.service.type",
                "type": "",
                "value": "LoadBalancer"
              }
            ],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "## ArgoCD configuration\n## Ref: https://github.com/argoproj/argo-cd\n##\n\n# -- Provide a name in place of `argocd`\nnameOverride: argocd\n# -- String to fully override `\"argo-cd.fullname\"`\nfullnameOverride: \"\"\n# -- Override the Kubernetes version, which is used to evaluate certain manifests\nkubeVersionOverride: \"\"\n\nglobal:\n  image:\n    # -- If defined, a repository applied to all ArgoCD deployments\n    repository: quay.io/argoproj/argocd\n    # -- Overrides the global ArgoCD image tag whose default is the chart appVersion\n    tag: \"\"\n    # -- If defined, a imagePullPolicy applied to all ArgoCD deployments\n    imagePullPolicy: IfNotPresent\n  # -- Annotations for the all deployed pods\n  podAnnotations: {}\n  # -- Labels for the all deployed pods\n  podLabels: {}\n  # -- Toggle and define securityContext. See [values.yaml]\n  securityContext: {}\n  #  runAsUser: 999\n  #  runAsGroup: 999\n  #  fsGroup: 999\n\n  # -- If defined, uses a Secret to pull an image from a private Docker registry or repository\n  imagePullSecrets: []\n  # -- Mapping between IP and hostnames that will be injected as entries in the pod's hosts files\n  hostAliases: []\n  # - ip: 10.20.30.40\n  #   hostnames:\n  #   - git.myhostname\n\n  # -- Additional labels to add to all resources\n  additionalLabels: {}\n    # app: argo-cd\n\n  networkPolicy:\n    # -- Create NetworkPolicy objects for all components\n    create: false\n    # -- Default deny all ingress traffic\n    defaultDenyIngress: false\n\n# Override APIVersions\n# If you want to template helm charts but cannot access k8s API server\n# you can set api versions here\napiVersionOverrides:\n  # -- String to override apiVersion of certmanager resources rendered by this helm chart\n  certmanager: \"\" # cert-manager.io/v1\n  # -- String to override apiVersion of ingresses rendered by this helm chart\n  ingress: \"\" # networking.k8s.io/v1beta1\n\n# -- Create clusterroles that extend existing clusterroles to interact with argo-cd crds\n## Ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles\ncreateAggregateRoles: false\n\n# -- Array of extra K8s manifests to deploy\nextraObjects: []\n  # - apiVersion: secrets-store.csi.x-k8s.io/v1\n  #   kind: SecretProviderClass\n  #   metadata:\n  #     name: argocd-secrets-store\n  #   spec:\n  #     provider: aws\n  #     parameters:\n  #       objects: |\n  #         - objectName: \"argocd\"\n  #           objectType: \"secretsmanager\"\n  #           jmesPath:\n  #               - path: \"client_id\"\n  #                 objectAlias: \"client_id\"\n  #               - path: \"client_secret\"\n  #                 objectAlias: \"client_secret\"\n  #     secretObjects:\n  #     - data:\n  #       - key: client_id\n  #         objectName: client_id\n  #       - key: client_secret\n  #         objectName: client_secret\n  #       secretName: argocd-secrets-store\n  #       type: Opaque\n  #       labels:\n  #         app.kubernetes.io/part-of: argocd\n\n## Controller\ncontroller:\n  # -- Application controller name string\n  name: application-controller\n\n  image:\n    # -- Repository to use for the application controller\n    # @default -- `\"\"` (defaults to global.image.repository)\n    repository: \"\"\n    # -- Tag to use for the application controller\n    # @default -- `\"\"` (defaults to global.image.tag)\n    tag: \"\"\n    # -- Image pull policy for the application controller\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\"\n\n  # -- The number of application controller pods to run.\n  # If changing the number of replicas you must pass the number as `ARGOCD_CONTROLLER_REPLICAS` as an environment variable\n  replicas: 1\n\n  # -- Deploy the application controller as a StatefulSet instead of a Deployment, this is required for HA capability.\n  # This is a feature flag that will become the default in chart version 3.x\n  enableStatefulSet: false\n\n  ## Application controller commandline flags\n  args:\n    # -- define the application controller `--status-processors`\n    statusProcessors: \"20\"\n    # -- define the application controller `--operation-processors`\n    operationProcessors: \"10\"\n    # -- define the application controller `--app-resync`\n    appResyncPeriod: \"180\"\n    # -- define the application controller `--self-heal-timeout-seconds`\n    selfHealTimeout: \"5\"\n    # -- define the application controller `--repo-server-timeout-seconds`\n    repoServerTimeoutSeconds: \"60\"\n\n  # -- Application controller log format. Either `text` or `json`\n  logFormat: text\n  # -- Application controller log level\n  logLevel: info\n\n  # -- Additional command line arguments to pass to application controller\n  extraArgs: []\n\n  # -- Environment variables to pass to application controller\n  env:\n    []\n    # - name: \"ARGOCD_CONTROLLER_REPLICAS\"\n    #   value: \"\"\n\n  # -- envFrom to pass to application controller\n  # @default -- `[]` (See [values.yaml])\n  envFrom: []\n  # - configMapRef:\n  #     name: config-map-name\n  # - secretRef:\n  #     name: secret-name\n\n  # -- Annotations to be added to application controller pods\n  podAnnotations: {}\n\n  # -- Labels to be added to application controller pods\n  podLabels: {}\n\n  # -- Application controller container-level security context\n  containerSecurityContext:\n    {}\n    # capabilities:\n    #   drop:\n    #     - all\n    # readOnlyRootFilesystem: true\n    # runAsNonRoot: true\n\n  # -- Application controller listening port\n  containerPort: 8082\n\n  ## Readiness and liveness probes for default backend\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/\n  ##\n  readinessProbe:\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n  livenessProbe:\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n\n  # -- Additional volumeMounts to the application controller main container\n  volumeMounts: []\n\n  # -- Additional volumes to the application controller pod\n  volumes: []\n\n  ## Controller service configuration\n  service:\n    # -- Application controller service annotations\n    annotations: {}\n    # -- Application controller service labels\n    labels: {}\n    # -- Application controller service port\n    port: 8082\n    # -- Application controller service port name\n    portName: https-controller\n\n  # -- [Node selector]\n  nodeSelector: {}\n  # -- [Tolerations] for use with node taints\n  tolerations: []\n  # -- Assign custom [affinity] rules to the deployment\n  affinity: {}\n\n  # -- Assign custom [TopologySpreadConstraints] rules to the application controller\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n  # - maxSkew: 1\n  #   topologyKey: topology.kubernetes.io/zone\n  #   whenUnsatisfiable: DoNotSchedule\n\n  # -- Priority class for the application controller pods\n  priorityClassName: \"\"\n\n  # -- Resource limits and requests for the application controller pods\n  resources: {}\n  #  limits:\n  #    cpu: 500m\n  #    memory: 512Mi\n  #  requests:\n  #    cpu: 250m\n  #    memory: 256Mi\n\n  serviceAccount:\n    # -- Create a service account for the application controller\n    create: true\n    # -- Service account name\n    name: argocd-application-controller\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: true\n\n  ## Application controller metrics configuration\n  metrics:\n    # -- Deploy metrics service\n    enabled: false\n    applicationLabels:\n      # -- Enables additional labels in argocd_app_labels metric\n      enabled: false\n      # -- Additional labels\n      labels: {}\n    service:\n      # -- Metrics service annotations\n      annotations: {}\n      # -- Metrics service labels\n      labels: {}\n      # -- Metrics service port\n      servicePort: 8082\n    serviceMonitor:\n      # -- Enable a prometheus ServiceMonitor\n      enabled: false\n      # -- Prometheus ServiceMonitor interval\n      interval: 30s\n      # -- Prometheus [RelabelConfigs] to apply to samples before scraping\n      relabelings: []\n      # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion\n      metricRelabelings: []\n      # -- Prometheus ServiceMonitor selector\n      selector: {}\n        # prometheus: kube-prometheus\n\n      # -- Prometheus ServiceMonitor namespace\n      namespace: \"\" # \"monitoring\"\n      # -- Prometheus ServiceMonitor labels\n      additionalLabels: {}\n    rules:\n      # -- Deploy a PrometheusRule for the application controller\n      enabled: false\n      # -- PrometheusRule.Spec for the application controller\n      spec: []\n      # - alert: ArgoAppMissing\n      #   expr: |\n      #     absent(argocd_app_info)\n      #   for: 15m\n      #   labels:\n      #     severity: critical\n      #   annotations:\n      #     summary: \"[ArgoCD] No reported applications\"\n      #     description: \u003e\n      #       ArgoCD has not reported any applications data for the past 15 minutes which\n      #       means that it must be down or not functioning properly.  This needs to be\n      #       resolved for this cloud to continue to maintain state.\n      # - alert: ArgoAppNotSynced\n      #   expr: |\n      #     argocd_app_info{sync_status!=\"Synced\"} == 1\n      #   for: 12h\n      #   labels:\n      #     severity: warning\n      #   annotations:\n      #     summary: \"[{{`{{$labels.name}}`}}] Application not synchronized\"\n      #     description: \u003e\n      #       The application [{{`{{$labels.name}}`}} has not been synchronized for over\n      #       12 hours which means that the state of this cloud has drifted away from the\n      #       state inside Git.\n    #   selector:\n    #     prometheus: kube-prometheus\n    #   namespace: monitoring\n    #   additionalLabels: {}\n\n  ## Enable if you would like to grant rights to ArgoCD to deploy to the local Kubernetes cluster.\n  clusterAdminAccess:\n    # -- Enable RBAC for local cluster deployments\n    enabled: true\n\n  ## Enable this and set the rules: to whatever custom rules you want for the Cluster Role resource.\n  ## Defaults to off\n  clusterRoleRules:\n    # -- Enable custom rules for the application controller's ClusterRole resource\n    enabled: false\n    # -- List of custom rules for the application controller's ClusterRole resource\n    rules: []\n\n  # -- Additional containers to be added to the application controller pod\n  extraContainers: []\n\n  # -- Init containers to add to the application controller pod\n  ## If your target Kubernetes cluster(s) require a custom auth provider executable\n  ## you could use this (and the same in the server pod) to bootstrap\n  ## that executable into your ArgoCD container\n  initContainers: []\n  #  - name: download-tools\n  #    image: alpine:3.8\n  #    command: [sh, -c]\n  #    args:\n  #      - wget -qO- https://get.helm.sh/helm-v2.16.1-linux-amd64.tar.gz | tar -xvzf - \u0026\u0026\n  #        mv linux-amd64/helm /custom-tools/\n  #    volumeMounts:\n  #      - mountPath: /custom-tools\n  #        name: custom-tools\n  #  volumeMounts:\n  #  - mountPath: /usr/local/bin/helm\n  #    name: custom-tools\n  #    subPath: helm\n\n  pdb:\n    # -- Labels to be added to application controller pdb\n    labels: {}\n    # -- Annotations to be added to application controller pdb\n    annotations: {}\n\n    # -- Deploy a Poddisruptionbudget for the application controller\n    enabled: false\n    # minAvailable: 1\n    # maxUnavailable: 0\n\n## Dex\ndex:\n  # -- Enable dex\n  enabled: true\n  # -- Dex name\n  name: dex-server\n\n  metrics:\n    # -- Deploy metrics service\n    enabled: false\n    service:\n      # -- Metrics service annotations\n      annotations: {}\n      # -- Metrics service labels\n      labels: {}\n    serviceMonitor:\n      # -- Enable a prometheus ServiceMonitor\n      enabled: false\n      # -- Prometheus ServiceMonitor interval\n      interval: 30s\n      # -- Prometheus [RelabelConfigs] to apply to samples before scraping\n      relabelings: []\n      # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion\n      metricRelabelings: []\n      # -- Prometheus ServiceMonitor selector\n      selector: {}\n        # prometheus: kube-prometheus\n\n      # -- Prometheus ServiceMonitor namespace\n      namespace: \"\" # \"monitoring\"\n      # -- Prometheus ServiceMonitor labels\n      additionalLabels: {}\n\n  image:\n    # -- Dex image repository\n    repository: ghcr.io/dexidp/dex\n    # -- Dex image tag\n    tag: v2.30.0\n    # -- Dex imagePullPolicy\n    imagePullPolicy: IfNotPresent\n  initImage:\n    # -- Argo CD init image repository\n    # @default -- `\"\"` (defaults to global.image.repository)\n    repository: \"\"\n    # -- Argo CD init image tag\n    # @default -- `\"\"` (defaults to global.image.tag)\n    tag: \"\"\n    # -- Argo CD init image imagePullPolicy\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\"\n\n  # -- Environment variables to pass to the Dex server\n  env: []\n\n  # -- envFrom to pass to the Dex server\n  # @default -- `[]` (See [values.yaml])\n  envFrom: []\n  # - configMapRef:\n  #     name: config-map-name\n  # - secretRef:\n  #     name: secret-name\n\n  # -- Annotations to be added to the Dex server pods\n  podAnnotations: {}\n\n  # -- Labels to be added to the Dex server pods\n  podLabels: {}\n\n  ## Probes for Dex server\n  ## Supported from Dex \u003e= 2.28.0\n  livenessProbe:\n    # -- Enable Kubernetes liveness probe for Dex \u003e= 2.28.0\n    enabled: false\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n  readinessProbe:\n    # -- Enable Kubernetes readiness probe for Dex \u003e= 2.28.0\n    enabled: false\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n\n  serviceAccount:\n    # -- Create dex service account\n    create: true\n    # -- Dex service account name\n    name: argocd-dex-server\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: true\n\n  # -- Additional volumeMounts to the dex main container\n  volumeMounts:\n    - name: static-files\n      mountPath: /shared\n\n  # -- Additional volumes to the dex pod\n  volumes:\n    - name: static-files\n      emptyDir: {}\n\n  # -- Extra volumes to the dex pod\n  extraVolumes: []\n\n  # -- Extra volumeMounts to the dex pod\n  extraVolumeMounts: []\n\n  # -- Container port for HTTP access\n  containerPortHttp: 5556\n  # -- Service port for HTTP access\n  servicePortHttp: 5556\n  # -- Service port name for HTTP access\n  servicePortHttpName: http\n  # -- Container port for gRPC access\n  containerPortGrpc: 5557\n  # -- Service port for gRPC access\n  servicePortGrpc: 5557\n  # -- Service port name for gRPC access\n  servicePortGrpcName: grpc\n  # -- Container port for metrics access\n  containerPortMetrics: 5558\n  # -- Service port for metrics access\n  servicePortMetrics: 5558\n\n  # -- [Node selector]\n  nodeSelector: {}\n  # -- [Tolerations] for use with node taints\n  tolerations: []\n  # -- Assign custom [affinity] rules to the deployment\n  affinity: {}\n\n  # -- Assign custom [TopologySpreadConstraints] rules to dex\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n  # - maxSkew: 1\n  #   topologyKey: topology.kubernetes.io/zone\n  #   whenUnsatisfiable: DoNotSchedule\n\n  # -- Priority class for dex\n  priorityClassName: \"\"\n\n  # -- Dex container-level security context\n  containerSecurityContext:\n    {}\n    # capabilities:\n    #   drop:\n    #     - all\n    # readOnlyRootFilesystem: true\n\n# -- Resource limits and requests for dex\n  resources: {}\n  #  limits:\n  #    cpu: 50m\n  #    memory: 64Mi\n  #  requests:\n  #    cpu: 10m\n  #    memory: 32Mi\n\n  # -- Additional containers to be added to the dex pod\n  extraContainers: []\n\n  # -- Init containers to add to the dex pod\n  initContainers: []\n  #  - name: download-tools\n  #    image: alpine:3.8\n  #    command: [sh, -c]\n  #    args:\n  #      - wget -qO- https://get.helm.sh/helm-v2.16.1-linux-amd64.tar.gz | tar -xvzf - \u0026\u0026\n  #        mv linux-amd64/helm /custom-tools/\n  #    volumeMounts:\n  #      - mountPath: /custom-tools\n  #        name: custom-tools\n  #  volumeMounts:\n  #  - mountPath: /usr/local/bin/helm\n  #    name: custom-tools\n  #    subPath: helm\n\n  pdb:\n    # -- Labels to be added to Dex server pdb\n    labels: {}\n    # -- Annotations to be added to Dex server pdb\n    annotations: {}\n\n    # -- Deploy a Poddisruptionbudget for the Dex server\n    enabled: false\n    # minAvailable: 1\n    # maxUnavailable: 0\n\n## Redis\nredis:\n  # -- Enable redis\n  enabled: true\n  # -- Redis name\n  name: redis\n\n  image:\n    # -- Redis repository\n    repository: redis\n    # -- Redis tag\n    tag: 6.2.6-alpine\n    # -- Redis imagePullPolicy\n    imagePullPolicy: IfNotPresent\n\n  # -- Additional command line arguments to pass to redis-server\n  extraArgs: []\n  # - --bind\n  # - \"0.0.0.0\"\n\n  # -- Redis container port\n  containerPort: 6379\n  # -- Redis service port\n  servicePort: 6379\n\n  # -- Environment variables to pass to the Redis server\n  env: []\n\n  # -- envFrom to pass to the Redis server\n  # @default -- `[]` (See [values.yaml])\n  envFrom: []\n  # - configMapRef:\n  #     name: config-map-name\n  # - secretRef:\n  #     name: secret-name\n\n  # -- Annotations to be added to the Redis server pods\n  podAnnotations: {}\n\n  # -- Labels to be added to the Redis server pods\n  podLabels: {}\n\n  # -- [Node selector]\n  nodeSelector: {}\n  # -- [Tolerations] for use with node taints\n  tolerations: []\n  # -- Assign custom [affinity] rules to the deployment\n  affinity: {}\n\n  # -- Assign custom [TopologySpreadConstraints] rules to redis\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n  # - maxSkew: 1\n  #   topologyKey: topology.kubernetes.io/zone\n  #   whenUnsatisfiable: DoNotSchedule\n\n  # -- Priority class for redis\n  priorityClassName: \"\"\n\n  # -- Redis container-level security context\n  containerSecurityContext:\n    {}\n    # capabilities:\n    #   drop:\n    #     - all\n    # readOnlyRootFilesystem: true\n\n  # -- Redis pod-level security context\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 999\n\n  serviceAccount:\n    # -- Create a service account for the redis pod\n    create: false\n    # -- Service account name for redis pod\n    name: \"\"\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: false\n\n  # -- Resource limits and requests for redis\n  resources: {}\n  #  limits:\n  #    cpu: 200m\n  #    memory: 128Mi\n  #  requests:\n  #    cpu: 100m\n  #    memory: 64Mi\n\n  # -- Additional volumeMounts to the redis container\n  volumeMounts: []\n  # -- Additional volumes to the redis pod\n  volumes: []\n\n  # -- Additional containers to be added to the redis pod\n  extraContainers: []\n\n  # -- Init containers to add to the redis pod\n  initContainers: []\n  #  - name: download-tools\n  #    image: alpine:3.8\n  #    command: [sh, -c]\n  #    args:\n  #      - wget -qO- https://get.helm.sh/helm-v2.16.1-linux-amd64.tar.gz | tar -xvzf - \u0026\u0026\n  #        mv linux-amd64/helm /custom-tools/\n  #    volumeMounts:\n  #      - mountPath: /custom-tools\n  #        name: custom-tools\n  #  volumeMounts:\n  #  - mountPath: /usr/local/bin/helm\n  #    name: custom-tools\n  #    subPath: helm\n\n  service:\n    # -- Redis service annotations\n    annotations: {}\n    # -- Additional redis service labels\n    labels: {}\n\n  metrics:\n    # -- Deploy metrics service and redis-exporter sidecar\n    enabled: false\n    image:\n      # -- redis-exporter image repository\n      repository: quay.io/bitnami/redis-exporter\n      # -- redis-exporter image tag\n      tag: 1.26.0-debian-10-r2\n      # -- redis-exporter image PullPolicy\n      imagePullPolicy: IfNotPresent\n    # -- Port to use for redis-exporter sidecar\n    containerPort: 9121\n    # -- Resource limits and requests for redis-exporter sidecar\n    resources: {}\n      # limits:\n      #   cpu: 50m\n      #   memory: 64Mi\n      # requests:\n      #   cpu: 10m\n      #   memory: 32Mi\n    service:\n      # -- Metrics service type\n      type: ClusterIP\n      # -- Metrics service clusterIP. `None` makes a \"headless service\" (no virtual IP)\n      clusterIP: None\n      # -- Metrics service annotations\n      annotations: {}\n      # -- Metrics service labels\n      labels: {}\n      # -- Metrics service port\n      servicePort: 9121\n      # -- Metrics service port name\n      portName: http-metrics\n    serviceMonitor:\n      # -- Enable a prometheus ServiceMonitor\n      enabled: false\n      # -- Interval at which metrics should be scraped\n      interval: 30s\n      # -- Prometheus [RelabelConfigs] to apply to samples before scraping\n      relabelings: []\n      # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion\n      metricRelabelings: []\n      # -- Prometheus ServiceMonitor selector\n      selector: {}\n        # prometheus: kube-prometheus\n\n      # -- Prometheus ServiceMonitor namespace\n      namespace: \"\" # \"monitoring\"\n      # -- Prometheus ServiceMonitor labels\n      additionalLabels: {}\n\n  pdb:\n    # -- Labels to be added to Redis server pdb\n    labels: {}\n    # -- Annotations to be added to Redis server pdb\n    annotations: {}\n\n    # -- Deploy a Poddisruptionbudget for the Redis server\n    enabled: false\n    # minAvailable: 1\n    # maxUnavailable: 0\n\n# This key configures Redis-HA subchart and when enabled (redis-ha.enabled=true)\n# the custom redis deployment is omitted\n# Check the redis-ha chart for more properties\nredis-ha:\n  # -- Enables the Redis HA subchart and disables the custom Redis single node deployment\n  enabled: false\n  exporter:\n    # -- If `true`, the prometheus exporter sidecar is enabled\n    enabled: true\n  persistentVolume:\n    # -- Configures persistency on Redis nodes\n    enabled: false\n  redis:\n    # -- Redis convention for naming the cluster group: must match `^[\\\\w-\\\\.]+$` and can be templated\n    masterGroupName: argocd\n    # -- Any valid redis config options in this section will be applied to each server (see `redis-ha` chart)\n    # @default -- See [values.yaml]\n    config:\n      # -- Will save the DB if both the given number of seconds and the given number of write operations against the DB occurred. `\"\"`  is disabled\n      save: '\"\"'\n  haproxy:\n    # -- Enabled HAProxy LoadBalancing/Proxy\n    enabled: true\n    metrics:\n      # -- HAProxy enable prometheus metric scraping\n      enabled: true\n  image:\n    # -- Redis tag\n    tag: 6.2.6-alpine\n\n## Server\nserver:\n  # -- Argo CD server name\n  name: server\n\n  # -- The number of server pods to run\n  replicas: 1\n\n  autoscaling:\n    # -- Enable Horizontal Pod Autoscaler ([HPA]) for the Argo CD server\n    enabled: false\n    # -- Minimum number of replicas for the Argo CD server [HPA]\n    minReplicas: 1\n    # -- Maximum number of replicas for the Argo CD server [HPA]\n    maxReplicas: 5\n    # -- Average CPU utilization percentage for the Argo CD server [HPA]\n    targetCPUUtilizationPercentage: 50\n    # -- Average memory utilization percentage for the Argo CD server [HPA]\n    targetMemoryUtilizationPercentage: 50\n\n  image:\n    # -- Repository to use for the Argo CD server\n    # @default -- `\"\"` (defaults to global.image.repository)\n    repository: \"\" # defaults to global.image.repository\n    # -- Tag to use for the Argo CD server\n    # @default -- `\"\"` (defaults to global.image.tag)\n    tag: \"\" # defaults to global.image.tag\n    # -- Image pull policy for the Argo CD server\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\" # IfNotPresent\n\n  # -- Additional command line arguments to pass to Argo CD server\n  extraArgs: \n    - --insecure\n\n  # This flag is used to either remove or pass the CLI flag --staticassets /shared/app to the Argo CD server app\n  staticAssets:\n    # -- Disable deprecated flag `--staticassets`\n    enabled: true\n\n  # -- Environment variables to pass to Argo CD server\n  env: []\n\n  # -- envFrom to pass to Argo CD server\n  # @default -- `[]` (See [values.yaml])\n  envFrom: []\n  # - configMapRef:\n  #     name: config-map-name\n  # - secretRef:\n  #     name: secret-name\n\n  # -- Specify postStart and preStop lifecycle hooks for your argo-cd-server container\n  lifecycle: {}\n\n  # -- Argo CD server log format: Either `text` or `json`\n  logFormat: text\n  # -- Argo CD server log level\n  logLevel: info\n\n  # -- Annotations to be added to server pods\n  podAnnotations: {}\n\n  # -- Labels to be added to server pods\n  podLabels: {}\n\n  # -- Configures the server port\n  containerPort: 8080\n\n  ## Readiness and liveness probes for default backend\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/\n  ##\n  readinessProbe:\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n  livenessProbe:\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n\n  # -- Additional volumeMounts to the server main container\n  volumeMounts: []\n\n  # -- Additional volumes to the server pod\n  volumes: []\n\n  # -- [Node selector]\n  nodeSelector: {}\n  # -- [Tolerations] for use with node taints\n  tolerations: []\n  # -- Assign custom [affinity] rules to the deployment\n  affinity: {}\n\n  # -- Assign custom [TopologySpreadConstraints] rules to the Argo CD server\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n  # - maxSkew: 1\n  #   topologyKey: topology.kubernetes.io/zone\n  #   whenUnsatisfiable: DoNotSchedule\n\n  # -- Priority class for the Argo CD server\n  priorityClassName: \"\"\n\n  # -- Servers container-level security context\n  containerSecurityContext:\n    {}\n    # capabilities:\n    #   drop:\n    #     - all\n    # readOnlyRootFilesystem: true\n\n  # -- Resource limits and requests for the Argo CD server\n  resources: {}\n  #  limits:\n  #    cpu: 100m\n  #    memory: 128Mi\n  #  requests:\n  #    cpu: 50m\n  #    memory: 64Mi\n\n  ## Certificate configuration\n  certificate:\n    # -- Enables a certificate manager certificate\n    enabled: false\n    # -- Certificate manager domain\n    domain: argocd.example.com\n    issuer:\n      # -- Certificate manager issuer\n      kind: # ClusterIssuer\n      # -- Certificate manager name\n      name: # letsencrypt\n    # -- Certificate manager additional hosts\n    additionalHosts: []\n    # -- Certificate manager secret name\n    secretName: argocd-server-tls\n\n  ## Server service configuration\n  service:\n    # -- Server service annotations\n    annotations: {}\n    # -- Server service labels\n    labels: {}\n    # -- Server service type\n    type: ClusterIP\n    # -- Server service http port for NodePort service type (only if `server.service.type` is set to \"NodePort\")\n    nodePortHttp: 30080\n    # -- Server service https port for NodePort service type (only if `server.service.type` is set to \"NodePort\")\n    nodePortHttps: 30443\n    # -- Server service http port\n    servicePortHttp: 80\n    # -- Server service https port\n    servicePortHttps: 443\n    # -- Server service http port name, can be used to route traffic via istio\n    servicePortHttpName: http\n    # -- Server service https port name, can be used to route traffic via istio\n    servicePortHttpsName: https\n    # -- Use named target port for argocd\n    ## Named target ports are not supported by GCE health checks, so when deploying argocd on GKE\n    ## and exposing it via GCE ingress, the health checks fail and the load balancer returns a 502.\n    namedTargetPort: true\n    # -- LoadBalancer will get created with the IP specified in this field\n    loadBalancerIP: \"\"\n    # -- Source IP ranges to allow access to service from\n    loadBalancerSourceRanges: []\n    # -- Server service external IPs\n    externalIPs: []\n    # -- Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    externalTrafficPolicy: \"\"\n    # -- Used to maintain session affinity. Supports `ClientIP` and `None`\n    sessionAffinity: \"\"\n\n  ## Server metrics service configuration\n  metrics:\n    # -- Deploy metrics service\n    enabled: false\n    service:\n      # -- Metrics service annotations\n      annotations: {}\n      # -- Metrics service labels\n      labels: {}\n      # -- Metrics service port\n      servicePort: 8083\n    serviceMonitor:\n      # -- Enable a prometheus ServiceMonitor\n      enabled: false\n      # -- Prometheus ServiceMonitor interval\n      interval: 30s\n      # -- Prometheus [RelabelConfigs] to apply to samples before scraping\n      relabelings: []\n      # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion\n      metricRelabelings: []\n      # -- Prometheus ServiceMonitor selector\n      selector: {}\n        # prometheus: kube-prometheus\n\n      # -- Prometheus ServiceMonitor namespace\n      namespace: \"\"  # monitoring\n      # -- Prometheus ServiceMonitor labels\n      additionalLabels: {}\n\n  serviceAccount:\n    # -- Create server service account\n    create: true\n    # -- Server service account name\n    name: argocd-server\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: true\n\n  ingress:\n    # -- Enable an ingress resource for the Argo CD server\n    enabled: false\n    # -- Additional ingress annotations\n    annotations: []\n      #kubernetes.io/ingress.class: alb\n      #alb.ingress.kubernetes.io/group.name: alb-01\n      #alb.ingress.kubernetes.io/target-type: instance\n      #alb.ingress.kubernetes.io/scheme: internet-facing\n      #alb.ingress.kubernetes.io/listen-ports: '[{\"HTTPS\":443}]'\n      #alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-northeast-2:687227541429:certificate/abcaf752-73c3-4a73-ae47-c1705ca534e1\n      #alb.ingress.kubernetes.io/backend-protocol: HTTP\n      #external-dns.alpha.kubernetes.io/hostname: argocd.kubedns.click\n    # -- Additional ingress labels\n    labels: {}\n    # -- Defines which ingress controller will implement the resource\n    ingressClassName: \"\"\n\n    # -- List of ingress hosts\n    ## Argo Ingress.\n    ## Hostnames must be provided if Ingress is enabled.\n    ## Secrets must be manually created in the namespace\n    hosts: []\n      \n    #  - argocd.example.com\n\n    # -- List of ingress paths\n    paths:\n      - /\n    # -- Ingress path type. One of `Exact`, `Prefix` or `ImplementationSpecific`\n    pathType: Prefix\n    # -- Additional ingress paths\n    extraPaths: []\n      \n      # - path: /*\n      #   backend:\n      #     serviceName: prot-forward\n      #     servicePort: use-annotation\n      ## for Kubernetes \u003e=1.19 (when \"networking.k8s.io/v1\" is used)\n      # - path: /*\n      #   pathType: Prefix\n      #   backend:\n      #     service:\n      #       name: port-forward\n      #       port:\n      #         name: use-annotation\n\n    # -- Ingress TLS configuration\n    tls:\n      []\n      # - secretName: argocd-tls-certificate\n      #   hosts:\n      #     - argocd.example.com\n\n    # -- Uses `server.service.servicePortHttps` instead `server.service.servicePortHttp`\n    https: false\n\n  # dedicated ingress for gRPC as documented at\n  # Ref: https://argoproj.github.io/argo-cd/operator-manual/ingress/\n  ingressGrpc:\n    # -- Enable an ingress resource for the Argo CD server for dedicated [gRPC-ingress]\n    enabled: false\n    # -- Setup up gRPC ingress to work with an AWS ALB\n    isAWSALB: false\n    # -- Additional ingress annotations for dedicated [gRPC-ingress]\n    annotations: {}\n    # -- Additional ingress labels for dedicated [gRPC-ingress]\n    labels: {}\n    # -- Defines which ingress controller will implement the resource [gRPC-ingress]\n    ingressClassName: \"\"\n\n    awsALB:\n      # -- Service type for the AWS ALB gRPC service\n      ## Service Type if isAWSALB is set to true\n      ## Can be of type NodePort or ClusterIP depending on which mode you are\n      ## are running. Instance mode needs type NodePort, IP mode needs type\n      ## ClusterIP\n      ## Ref: https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/how-it-works/#ingress-traffic\n      serviceType: NodePort\n      # -- Backend protocol version for the AWS ALB gRPC service\n      ## This tells AWS to send traffic from the ALB using HTTP2. Can use gRPC as well if you want to leverage gRPC specific features\n      backendProtocolVersion: HTTP2\n\n    # -- List of ingress hosts for dedicated [gRPC-ingress]\n    ## Argo Ingress.\n    ## Hostnames must be provided if Ingress is enabled.\n    ## Secrets must be manually created in the namespace\n    ##\n    hosts:\n      []\n      # - argocd.example.com\n\n    # -- List of ingress paths for dedicated [gRPC-ingress]\n    paths:\n      - /\n    # -- Ingress path type for dedicated [gRPC-ingress]. One of `Exact`, `Prefix` or `ImplementationSpecific`\n    pathType: Prefix\n    # -- Additional ingress paths for dedicated [gRPC-ingress]\n    extraPaths:\n      []\n      # - path: /*\n      #   backend:\n      #     serviceName: ssl-redirect\n      #     servicePort: use-annotation\n      ## for Kubernetes \u003e=1.19 (when \"networking.k8s.io/v1\" is used)\n      # - path: /*\n      #   pathType: Prefix\n      #   backend:\n      #     service:\n      #       name: ssl-redirect\n      #       port:\n      #         name: use-annotation\n\n    # -- Ingress TLS configuration for dedicated [gRPC-ingress]\n    tls:\n      []\n      # - secretName: argocd-tls-certificate\n      #   hosts:\n      #     - argocd.example.com\n\n    # -- Uses `server.service.servicePortHttps` instead `server.service.servicePortHttp`\n    https: false\n\n  # Create a OpenShift Route with SSL passthrough for UI and CLI\n  # Consider setting 'hostname' e.g. https://argocd.apps-crc.testing/ using your Default Ingress Controller Domain\n  # Find your domain with: kubectl describe --namespace=openshift-ingress-operator ingresscontroller/default | grep Domain:\n  # If 'hostname' is an empty string \"\" OpenShift will create a hostname for you.\n  route:\n    # -- Enable an OpenShift Route for the Argo CD server\n    enabled: false\n    # -- Openshift Route annotations\n    annotations: {}\n    # -- Hostname of OpenShift Route\n    hostname: \"\"\n    # -- Termination type of Openshift Route\n    termination_type: passthrough\n    # -- Termination policy of Openshift Route\n    termination_policy: None\n\n  # -- Manage ArgoCD configmap (Declarative Setup)\n  ## Ref: https://github.com/argoproj/argo-cd/blob/master/docs/operator-manual/argocd-cm.yaml\n  configEnabled: true\n  # -- [General Argo CD configuration]\n  # @default -- See [values.yaml]\n  config:\n    # Argo CD's externally facing base URL (optional). Required when configuring SSO\n    url: https://argocd.example.com\n    # Argo CD instance label key\n    application.instanceLabelKey: argocd.argoproj.io/instance\n\n    # DEPRECATED: Please instead use configs.credentialTemplates and configs.repositories\n    # repositories: |\n    #   - url: git@github.com:group/repo.git\n    #     sshPrivateKeySecret:\n    #       name: secret-name\n    #       key: sshPrivateKey\n    #   - type: helm\n    #     url: https://charts.helm.sh/stable\n    #     name: stable\n    #   - type: helm\n    #     url: https://argoproj.github.io/argo-helm\n    #     name: argo\n\n    # oidc.config: |\n    #   name: AzureAD\n    #   issuer: https://login.microsoftonline.com/TENANT_ID/v2.0\n    #   clientID: CLIENT_ID\n    #   clientSecret: $oidc.azuread.clientSecret\n    #   requestedIDTokenClaims:\n    #     groups:\n    #       essential: true\n    #   requestedScopes:\n    #     - openid\n    #     - profile\n    #     - email\n\n  # -- Annotations to be added to ArgoCD ConfigMap\n  configAnnotations: {}\n\n  # -- ArgoCD rbac config ([ArgoCD RBAC policy])\n  ## Ref: https://github.com/argoproj/argo-cd/blob/master/docs/operator-manual/rbac.md\n  rbacConfig:\n    {}\n    # policy.csv is an file containing user-defined RBAC policies and role definitions (optional).\n    # Policy rules are in the form:\n    #   p, subject, resource, action, object, effect\n    # Role definitions and bindings are in the form:\n    #   g, subject, inherited-subject\n    # See https://github.com/argoproj/argo-cd/blob/master/docs/operator-manual/rbac.md for additional information.\n    # policy.csv: |\n    #   # Grant all members of the group 'my-org:team-alpha; the ability to sync apps in 'my-project'\n    #   p, my-org:team-alpha, applications, sync, my-project/*, allow\n    #   # Grant all members of 'my-org:team-beta' admins\n    #   g, my-org:team-beta, role:admin\n    # policy.default is the name of the default role which Argo CD will falls back to, when\n    # authorizing API requests (optional). If omitted or empty, users may be still be able to login,\n    # but will see no apps, projects, etc...\n    # policy.default: role:readonly\n    # scopes controls which OIDC scopes to examine during rbac enforcement (in addition to `sub` scope).\n    # If omitted, defaults to: '[groups]'. The scope value can be a string, or a list of strings.\n    # scopes: '[cognito:groups, email]'\n\n  # -- Annotations to be added to ArgoCD rbac ConfigMap\n  rbacConfigAnnotations: {}\n\n  # -- Whether or not to create the configmap. If false, it is expected the configmap will be created\n  # by something else. ArgoCD will not work if there is no configMap created with the name above.\n  rbacConfigCreate: true\n\n  # -- Deploy ArgoCD Applications within this helm release\n  # @default -- `[]` (See [values.yaml])\n  ## Ref: https://github.com/argoproj/argo-cd/blob/master/docs/operator-manual/\n  additionalApplications: []\n  # - name: guestbook\n  #   namespace: argocd\n  #   additionalLabels: {}\n  #   additionalAnnotations: {}\n  #   finalizers:\n  #   - resources-finalizer.argocd.argoproj.io\n  #   project: guestbook\n  #   source:\n  #     repoURL: https://github.com/argoproj/argocd-example-apps.git\n  #     targetRevision: HEAD\n  #     path: guestbook\n  #     directory:\n  #       recurse: true\n  #   destination:\n  #     server: https://kubernetes.default.svc\n  #     namespace: guestbook\n  #   syncPolicy:\n  #     automated:\n  #       prune: false\n  #       selfHeal: false\n  #   ignoreDifferences:\n  #   - group: apps\n  #     kind: Deployment\n  #     jsonPointers:\n  #     - /spec/replicas\n  #   info:\n  #   - name: url\n  #     value: https://argoproj.github.io/\n\n  # -- Deploy ArgoCD Projects within this helm release\n  # @default -- `[]` (See [values.yaml])\n  ## Ref: https://github.com/argoproj/argo-cd/blob/master/docs/operator-manual/\n  additionalProjects: []\n  # - name: guestbook\n  #   namespace: argocd\n  #   additionalLabels: {}\n  #   additionalAnnotations: {}\n  #   finalizers:\n  #   - resources-finalizer.argocd.argoproj.io\n  #   description: Example Project\n  #   sourceRepos:\n  #   - '*'\n  #   destinations:\n  #   - namespace: guestbook\n  #     server: https://kubernetes.default.svc\n  #   clusterResourceWhitelist: []\n  #   namespaceResourceBlacklist:\n  #   - group: ''\n  #     kind: ResourceQuota\n  #   - group: ''\n  #     kind: LimitRange\n  #   - group: ''\n  #     kind: NetworkPolicy\n  #     orphanedResources: {}\n  #     roles: []\n  #   namespaceResourceWhitelist:\n  #   - group: 'apps'\n  #     kind: Deployment\n  #   - group: 'apps'\n  #     kind: StatefulSet\n  #   orphanedResources: {}\n  #   roles: []\n  #   syncWindows:\n  #   - kind: allow\n  #     schedule: '10 1 * * *'\n  #     duration: 1h\n  #     applications:\n  #     - '*-prod'\n  #     manualSync: true\n  #   signatureKeys:\n  #   - keyID: ABCDEF1234567890\n\n  ## Enable Admin ClusterRole resources.\n  ## Enable if you would like to grant rights to ArgoCD to deploy to the local Kubernetes cluster.\n  clusterAdminAccess:\n    # -- Enable RBAC for local cluster deployments\n    enabled: true\n\n  GKEbackendConfig:\n    # -- Enable BackendConfig custom resource for Google Kubernetes Engine\n    enabled: false\n    # -- [BackendConfigSpec]\n    spec: {}\n  #  spec:\n  #    iap:\n  #      enabled: true\n  #      oauthclientCredentials:\n  #        secretName: argocd-secret\n\n  ## Create a Google Managed Certificate for use with the GKE Ingress Controller\n  ## https://cloud.google.com/kubernetes-engine/docs/how-to/managed-certs\n  GKEmanagedCertificate:\n    # -- Enable ManagedCertificate custom resource for Google Kubernetes Engine.\n    enabled: false\n    # -- Domains for the Google Managed Certificate\n    domains:\n    - argocd.example.com\n\n  ## Create a Google FrontendConfig Custom Resource, for use with the GKE Ingress Controller\n  ## https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-features#configuring_ingress_features_through_frontendconfig_parameters\n  GKEfrontendConfig:\n    # -- Enable FrontConfig custom resource for Google Kubernetes Engine\n    enabled: false\n    # -- [FrontendConfigSpec]\n    spec: {}\n  # spec:\n  #   redirectToHttps:\n  #     enabled: true\n  #     responseCodeName: RESPONSE_CODE\n\n  # -- Additional containers to be added to the server pod\n  ## See https://github.com/lemonldap-ng-controller/lemonldap-ng-controller as example.\n  extraContainers: []\n  # - name: my-sidecar\n  #   image: nginx:latest\n  # - name: lemonldap-ng-controller\n  #   image: lemonldapng/lemonldap-ng-controller:0.2.0\n  #   args:\n  #     - /lemonldap-ng-controller\n  #     - --alsologtostderr\n  #     - --configmap=$(POD_NAMESPACE)/lemonldap-ng-configuration\n  #   env:\n  #     - name: POD_NAME\n  #       valueFrom:\n  #         fieldRef:\n  #           fieldPath: metadata.name\n  #     - name: POD_NAMESPACE\n  #       valueFrom:\n  #         fieldRef:\n  #           fieldPath: metadata.namespace\n  #   volumeMounts:\n  #   - name: copy-portal-skins\n  #     mountPath: /srv/var/lib/lemonldap-ng/portal/skins\n\n  # -- Init containers to add to the server pod\n  ## If your target Kubernetes cluster(s) require a custom auth provider executable\n  ## you could use this (and the same in the application controller pod) to bootstrap\n  ## that executable into your ArgoCD container\n  initContainers: []\n  #  - name: download-tools\n  #    image: alpine:3.8\n  #    command: [sh, -c]\n  #    args:\n  #      - wget -qO- https://get.helm.sh/helm-v2.16.1-linux-amd64.tar.gz | tar -xvzf - \u0026\u0026\n  #        mv linux-amd64/helm /custom-tools/\n  #    volumeMounts:\n  #      - mountPath: /custom-tools\n  #        name: custom-tools\n  #  volumeMounts:\n  #  - mountPath: /usr/local/bin/helm\n  #    name: custom-tools\n  #    subPath: helm\n\n  extensions:\n    # -- Enable support for extensions\n    ## This function in tech preview stage, do expect unstability or breaking changes in newer versions. Bump image.tag if necessary.\n    enabled: false\n\n    image:\n      # -- Repository to use for extensions image\n      repository: \"ghcr.io/argoproj-labs/argocd-extensions\"\n      # -- Tag to use for extensions image\n      tag: \"v0.1.0\"\n      # -- Image pull policy for extensions\n      imagePullPolicy: IfNotPresent\n\n    # -- Resource limits and requests for the argocd-extensions container\n    resources: {}\n    #  limits:\n    #    cpu: 50m\n    #    memory: 128Mi\n    #  requests:\n    #    cpu: 10m\n    #    memory: 64Mi\n\n    # -- Extensions to be loaded into the server\n    contents: []\n    # - name: argo-rollouts\n    #   url: https://github.com/argoproj-labs/rollout-extension/releases/download/v0.1.0/extension.tar\n\n  pdb:\n    # -- Labels to be added to server pdb\n    labels: {}\n    # -- Annotations to be added to server pdb\n    annotations: {}\n\n    # -- Deploy a Poddisruptionbudget for the server\n    enabled: false\n    # minAvailable: 1\n    # maxUnavailable: 0\n\n## Repo Server\nrepoServer:\n  # -- Repo server name\n  name: repo-server\n\n  # -- The number of repo server pods to run\n  replicas: 1\n\n  autoscaling:\n    # -- Enable Horizontal Pod Autoscaler ([HPA]) for the repo server\n    enabled: false\n    # -- Minimum number of replicas for the repo server [HPA]\n    minReplicas: 1\n    # -- Maximum number of replicas for the repo server [HPA]\n    maxReplicas: 5\n    # -- Average CPU utilization percentage for the repo server [HPA]\n    targetCPUUtilizationPercentage: 50\n    # -- Average memory utilization percentage for the repo server [HPA]\n    targetMemoryUtilizationPercentage: 50\n\n  image:\n    # -- Repository to use for the repo server\n    # @default -- `\"\"` (defaults to global.image.repository)\n    repository: \"\" # defaults to global.image.repository\n    # -- Tag to use for the repo server\n    # @default -- `\"\"` (defaults to global.image.tag)\n    tag: \"\" # defaults to global.image.tag\n    # -- Image pull policy for the repo server\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\" # IfNotPresent\n\n  # -- Additional command line arguments to pass to repo server\n  extraArgs: []\n\n  # -- Environment variables to pass to repo server\n  env: []\n\n  # -- envFrom to pass to repo server\n  # @default -- `[]` (See [values.yaml])\n  envFrom: []\n  # - configMapRef:\n  #     name: config-map-name\n  # - secretRef:\n  #     name: secret-name\n\n  # -- Repo server log format: Either `text` or `json`\n  logFormat: text\n  # -- Repo server log level\n  logLevel: info\n\n  # -- Annotations to be added to repo server pods\n  podAnnotations: {}\n\n  # -- Labels to be added to repo server pods\n  podLabels: {}\n\n  # -- Configures the repo server port\n  containerPort: 8081\n\n  ## Readiness and liveness probes for default backend\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/\n  ##\n  readinessProbe:\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n  livenessProbe:\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n\n  # -- Additional volumeMounts to the repo server main container\n  volumeMounts: []\n\n  # -- Additional volumes to the repo server pod\n  volumes: []\n  ## Use init containers to configure custom tooling\n  ## https://argoproj.github.io/argo-cd/operator-manual/custom_tools/\n  ## When using the volumes \u0026 volumeMounts section bellow, please comment out those above.\n  #  - name: custom-tools\n  #    emptyDir: {}\n\n  # -- [Node selector]\n  nodeSelector: {}\n  # -- [Tolerations] for use with node taints\n  tolerations: []\n  # -- Assign custom [affinity] rules to the deployment\n  affinity: {}\n\n  # -- Assign custom [TopologySpreadConstraints] rules to the repo server\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n  # - maxSkew: 1\n  #   topologyKey: topology.kubernetes.io/zone\n  #   whenUnsatisfiable: DoNotSchedule\n\n  # -- Priority class for the repo server\n  priorityClassName: \"\"\n\n  # -- Repo server container-level security context\n  containerSecurityContext:\n    {}\n    # capabilities:\n    #   drop:\n    #     - all\n    # readOnlyRootFilesystem: true\n\n  # -- Resource limits and requests for the repo server pods\n  resources: {}\n  #  limits:\n  #    cpu: 50m\n  #    memory: 128Mi\n  #  requests:\n  #    cpu: 10m\n  #    memory: 64Mi\n\n  ## Repo server service configuration\n  service:\n    # -- Repo server service annotations\n    annotations: {}\n    # -- Repo server service labels\n    labels: {}\n    # -- Repo server service port\n    port: 8081\n    # -- Repo server service port name\n    portName: https-repo-server\n\n  ## Repo server metrics service configuration\n  metrics:\n    # -- Deploy metrics service\n    enabled: false\n    service:\n      # -- Metrics service annotations\n      annotations: {}\n      # -- Metrics service labels\n      labels: {}\n      # -- Metrics service port\n      servicePort: 8084\n    serviceMonitor:\n      # -- Enable a prometheus ServiceMonitor\n      enabled: false\n      # -- Prometheus ServiceMonitor interval\n      interval: 30s\n      # -- Prometheus [RelabelConfigs] to apply to samples before scraping\n      relabelings: []\n      # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion\n      metricRelabelings: []\n      # -- Prometheus ServiceMonitor selector\n      selector: {}\n        # prometheus: kube-prometheus\n\n      # -- Prometheus ServiceMonitor namespace\n      namespace: \"\" # \"monitoring\"\n      # -- Prometheus ServiceMonitor labels\n      additionalLabels: {}\n\n  ## Enable Admin ClusterRole resources.\n  ## Enable if you would like to grant cluster rights to ArgoCD repo server.\n  clusterAdminAccess:\n    # -- Enable RBAC for local cluster deployments\n    enabled: false\n  ## Enable Custom Rules for the Repo server's Cluster Role resource\n  ## Enable this and set the rules: to whatever custom rules you want for the Cluster Role resource.\n  ## Defaults to off\n  clusterRoleRules:\n    # -- Enable custom rules for the Repo server's Cluster Role resource\n    enabled: false\n    # -- List of custom rules for the Repo server's Cluster Role resource\n    rules: []\n\n  ## Repo server service account\n  ## If create is set to true, make sure to uncomment the name and update the rbac section below\n  serviceAccount:\n    # -- Create repo server service account\n    create: false\n    # -- Repo server service account name\n    name: \"\" # \"argocd-repo-server\"\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: true\n\n  # -- Additional containers to be added to the repo server pod\n  extraContainers: []\n\n  # -- Repo server rbac rules\n  rbac: []\n  #   - apiGroups:\n  #     - argoproj.io\n  #     resources:\n  #     - applications\n  #     verbs:\n  #     - get\n  #     - list\n  #     - watch\n\n  # Init container to copy argocd binary\n  copyutil:\n    # -- Resource limits and requests for the copyutil initContainer\n    resources: {}\n    #  limits:\n    #    cpu: 50m\n    #    memory: 64Mi\n    #  requests:\n    #    cpu: 10m\n    #    memory: 32Mi\n\n  # -- Init containers to add to the repo server pods\n  initContainers: []\n  #  - name: download-tools\n  #    image: alpine:3.8\n  #    command: [sh, -c]\n  #    args:\n  #      - wget -qO- https://get.helm.sh/helm-v2.16.1-linux-amd64.tar.gz | tar -xvzf - \u0026\u0026\n  #        mv linux-amd64/helm /custom-tools/\n  #    volumeMounts:\n  #      - mountPath: /custom-tools\n  #        name: custom-tools\n  #  volumeMounts:\n  #  - mountPath: /usr/local/bin/helm\n  #    name: custom-tools\n  #    subPath: helm\n\n  pdb:\n    # -- Labels to be added to Repo server pdb\n    labels: {}\n    # -- Annotations to be added to Repo server pdb\n    annotations: {}\n\n    # -- Deploy a Poddisruptionbudget for the Repo server\n    enabled: false\n    # minAvailable: 1\n    # maxUnavailable: 0\n\n## Argo Configs\nconfigs:\n  # -- Provide one or multiple [external cluster credentials]\n  # @default -- `[]` (See [values.yaml])\n  ## Ref:\n  ## - https://argoproj.github.io/argo-cd/operator-manual/declarative-setup/#clusters\n  ## - https://argoproj.github.io/argo-cd/operator-manual/security/#external-cluster-credentials\n  clusterCredentials: []\n    # - name: mycluster\n    #   server: https://mycluster.com\n    #   labels: {}\n    #   annotations: {}\n    #   config:\n    #     bearerToken: \"\u003cauthentication token\u003e\"\n    #     tlsClientConfig:\n    #       insecure: false\n    #       caData: \"\u003cbase64 encoded certificate\u003e\"\n    # - name: mycluster2\n    #   server: https://mycluster2.com\n    #   labels: {}\n    #   annotations: {}\n    #   namespaces: namespace1,namespace2\n    #   config:\n    #     bearerToken: \"\u003cauthentication token\u003e\"\n    #     tlsClientConfig:\n    #       insecure: false\n    #       caData: \"\u003cbase64 encoded certificate\u003e\"\n\n  # -- GnuPG key ring annotations\n  gpgKeysAnnotations: {}\n  # -- [GnuPG](https://argoproj.github.io/argo-cd/user-guide/gpg-verification/) keys to add to the key ring\n  # @default -- `{}` (See [values.yaml])\n  gpgKeys: {}\n    # 4AEE18F83AFDEB23: |\n    #     -----BEGIN PGP PUBLIC KEY BLOCK-----\n    #\n    #     mQENBFmUaEEBCACzXTDt6ZnyaVtueZASBzgnAmK13q9Urgch+sKYeIhdymjuMQta\n    #     x15OklctmrZtqre5kwPUosG3/B2/ikuPYElcHgGPL4uL5Em6S5C/oozfkYzhwRrT\n    #     SQzvYjsE4I34To4UdE9KA97wrQjGoz2Bx72WDLyWwctD3DKQtYeHXswXXtXwKfjQ\n    #     7Fy4+Bf5IPh76dA8NJ6UtjjLIDlKqdxLW4atHe6xWFaJ+XdLUtsAroZcXBeWDCPa\n    #     buXCDscJcLJRKZVc62gOZXXtPfoHqvUPp3nuLA4YjH9bphbrMWMf810Wxz9JTd3v\n    #     yWgGqNY0zbBqeZoGv+TuExlRHT8ASGFS9SVDABEBAAG0NUdpdEh1YiAod2ViLWZs\n    #     b3cgY29tbWl0IHNpZ25pbmcpIDxub3JlcGx5QGdpdGh1Yi5jb20+iQEiBBMBCAAW\n    #     BQJZlGhBCRBK7hj4Ov3rIwIbAwIZAQAAmQEH/iATWFmi2oxlBh3wAsySNCNV4IPf\n    #     DDMeh6j80WT7cgoX7V7xqJOxrfrqPEthQ3hgHIm7b5MPQlUr2q+UPL22t/I+ESF6\n    #     9b0QWLFSMJbMSk+BXkvSjH9q8jAO0986/pShPV5DU2sMxnx4LfLfHNhTzjXKokws\n    #     +8ptJ8uhMNIDXfXuzkZHIxoXk3rNcjDN5c5X+sK8UBRH092BIJWCOfaQt7v7wig5\n    #     4Ra28pM9GbHKXVNxmdLpCFyzvyMuCmINYYADsC848QQFFwnd4EQnupo6QvhEVx1O\n    #     j7wDwvuH5dCrLuLwtwXaQh0onG4583p0LGms2Mf5F+Ick6o/4peOlBoZz48=\n    #     =Bvzs\n    #     -----END PGP PUBLIC KEY BLOCK-----\n\n  # -- Known Hosts configmap annotations\n  knownHostsAnnotations: {}\n  knownHosts:\n    data:\n      # -- Known Hosts\n      # @default -- See [values.yaml]\n      ssh_known_hosts: |\n        bitbucket.org ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAubiN81eDcafrgMeLzaFPsw2kNvEcqTKl/VqLat/MaB33pZy0y3rJZtnqwR2qOOvbwKZYKiEO1O6VqNEBxKvJJelCq0dTXWT5pbO2gDXC6h6QDXCaHo6pOHGPUy+YBaGQRGuSusMEASYiWunYN0vCAI8QaXnWMXNMdFP3jHAJH0eDsoiGnLPBlBp4TNm6rYI74nMzgz3B9IikW4WVK+dc8KZJZWYjAuORU3jc1c/NPskD2ASinf8v3xnfXeukU0sJ5N6m5E8VLjObPEO+mN2t/FZTMZLiFqPWc/ALSqnMnnhwrNi2rbfg/rd/IpL8Le3pSBne8+seeFVBoGqzHM9yXw==\n        github.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg=\n        github.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOMqqnkVzrm0SdG6UOoqKLsabgH5C9okWi0dh2l9GKJl\n        github.com ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAq2A7hRGmdnm9tUDbO9IDSwBK6TbQa+PXYPCPy6rbTrTtw7PHkccKrpp0yVhp5HdEIcKr6pLlVDBfOLX9QUsyCOV0wzfjIJNlGEYsdlLJizHhbn2mUjvSAHQqZETYP81eFzLQNnPHt4EVVUh7VfDESU84KezmD5QlWpXLmvU31/yMf+Se8xhHTvKSCZIFImWwoG6mbUoWf9nzpIoaSjB+weqqUUmpaaasXVal72J+UX2B+2RPW3RcT0eOzQgqlJL3RKrTJvdsjE3JEAvGq3lGHSZXy28G3skua2SmVi/w4yCE6gbODqnTWlg7+wC604ydGXA8VJiS5ap43JXiUFFAaQ==\n        gitlab.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBFSMqzJeV9rUzU4kWitGjeR4PWSa29SPqJ1fVkhtj3Hw9xjLVXVYrU9QlYWrOLXBpQ6KWjbjTDTdDkoohFzgbEY=\n        gitlab.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAfuCHKVTjquxvt6CM6tdG4SLp1Btn/nOeHHE5UOzRdf\n        gitlab.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCsj2bNKTBSpIYDEGk9KxsGh3mySTRgMtXL583qmBpzeQ+jqCMRgBqB98u3z++J1sKlXHWfM9dyhSevkMwSbhoR8XIq/U0tCNyokEi/ueaBMCvbcTHhO7FcwzY92WK4Yt0aGROY5qX2UKSeOvuP4D6TPqKF1onrSzH9bx9XUf2lEdWT/ia1NEKjunUqu1xOB/StKDHMoX4/OKyIzuS0q/T1zOATthvasJFoPrAjkohTyaDUz2LN5JoH839hViyEG82yB+MjcFV5MU3N1l1QL3cVUCh93xSaua1N85qivl+siMkPGbO5xR/En4iEY6K2XPASUEMaieWVNTRCtJ4S8H+9\n        ssh.dev.azure.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Hr1oTWqNqOlzGJOfGJ4NakVyIzf1rXYd4d7wo6jBlkLvCA4odBlL0mDUyZ0/QUfTTqeu+tm22gOsv+VrVTMk6vwRU75gY/y9ut5Mb3bR5BV58dKXyq9A9UeB5Cakehn5Zgm6x1mKoVyf+FFn26iYqXJRgzIZZcZ5V6hrE0Qg39kZm4az48o0AUbf6Sp4SLdvnuMa2sVNwHBboS7EJkm57XQPVU3/QpyNLHbWDdzwtrlS+ez30S3AdYhLKEOxAG8weOnyrtLJAUen9mTkol8oII1edf7mWWbWVf0nBmly21+nZcmCTISQBtdcyPaEno7fFQMDD26/s0lfKob4Kw8H\n        vs-ssh.visualstudio.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Hr1oTWqNqOlzGJOfGJ4NakVyIzf1rXYd4d7wo6jBlkLvCA4odBlL0mDUyZ0/QUfTTqeu+tm22gOsv+VrVTMk6vwRU75gY/y9ut5Mb3bR5BV58dKXyq9A9UeB5Cakehn5Zgm6x1mKoVyf+FFn26iYqXJRgzIZZcZ5V6hrE0Qg39kZm4az48o0AUbf6Sp4SLdvnuMa2sVNwHBboS7EJkm57XQPVU3/QpyNLHbWDdzwtrlS+ez30S3AdYhLKEOxAG8weOnyrtLJAUen9mTkol8oII1edf7mWWbWVf0nBmly21+nZcmCTISQBtdcyPaEno7fFQMDD26/s0lfKob4Kw8H\n  # -- TLS certificate configmap annotations\n  tlsCertsAnnotations: {}\n  # -- TLS certificate\n  # @default -- See [values.yaml]\n  tlsCerts:\n    {}\n    # data:\n    #   argocd.example.com: |\n    #     -----BEGIN CERTIFICATE-----\n    #     MIIF1zCCA7+gAwIBAgIUQdTcSHY2Sxd3Tq/v1eIEZPCNbOowDQYJKoZIhvcNAQEL\n    #     BQAwezELMAkGA1UEBhMCREUxFTATBgNVBAgMDExvd2VyIFNheG9ueTEQMA4GA1UE\n    #     BwwHSGFub3ZlcjEVMBMGA1UECgwMVGVzdGluZyBDb3JwMRIwEAYDVQQLDAlUZXN0\n    #     c3VpdGUxGDAWBgNVBAMMD2Jhci5leGFtcGxlLmNvbTAeFw0xOTA3MDgxMzU2MTda\n    #     Fw0yMDA3MDcxMzU2MTdaMHsxCzAJBgNVBAYTAkRFMRUwEwYDVQQIDAxMb3dlciBT\n    #     YXhvbnkxEDAOBgNVBAcMB0hhbm92ZXIxFTATBgNVBAoMDFRlc3RpbmcgQ29ycDES\n    #     MBAGA1UECwwJVGVzdHN1aXRlMRgwFgYDVQQDDA9iYXIuZXhhbXBsZS5jb20wggIi\n    #     MA0GCSqGSIb3DQEBAQUAA4ICDwAwggIKAoICAQCv4mHMdVUcafmaSHVpUM0zZWp5\n    #     NFXfboxA4inuOkE8kZlbGSe7wiG9WqLirdr39Ts+WSAFA6oANvbzlu3JrEQ2CHPc\n    #     CNQm6diPREFwcDPFCe/eMawbwkQAPVSHPts0UoRxnpZox5pn69ghncBR+jtvx+/u\n    #     P6HdwW0qqTvfJnfAF1hBJ4oIk2AXiip5kkIznsAh9W6WRy6nTVCeetmIepDOGe0G\n    #     ZJIRn/OfSz7NzKylfDCat2z3EAutyeT/5oXZoWOmGg/8T7pn/pR588GoYYKRQnp+\n    #     YilqCPFX+az09EqqK/iHXnkdZ/Z2fCuU+9M/Zhrnlwlygl3RuVBI6xhm/ZsXtL2E\n    #     Gxa61lNy6pyx5+hSxHEFEJshXLtioRd702VdLKxEOuYSXKeJDs1x9o6cJ75S6hko\n    #     Ml1L4zCU+xEsMcvb1iQ2n7PZdacqhkFRUVVVmJ56th8aYyX7KNX6M9CD+kMpNm6J\n    #     kKC1li/Iy+RI138bAvaFplajMF551kt44dSvIoJIbTr1LigudzWPqk31QaZXV/4u\n    #     kD1n4p/XMc9HYU/was/CmQBFqmIZedTLTtK7clkuFN6wbwzdo1wmUNgnySQuMacO\n    #     gxhHxxzRWxd24uLyk9Px+9U3BfVPaRLiOPaPoC58lyVOykjSgfpgbus7JS69fCq7\n    #     bEH4Jatp/10zkco+UQIDAQABo1MwUTAdBgNVHQ4EFgQUjXH6PHi92y4C4hQpey86\n    #     r6+x1ewwHwYDVR0jBBgwFoAUjXH6PHi92y4C4hQpey86r6+x1ewwDwYDVR0TAQH/\n    #     BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAgEAFE4SdKsX9UsLy+Z0xuHSxhTd0jfn\n    #     Iih5mtzb8CDNO5oTw4z0aMeAvpsUvjJ/XjgxnkiRACXh7K9hsG2r+ageRWGevyvx\n    #     CaRXFbherV1kTnZw4Y9/pgZTYVWs9jlqFOppz5sStkfjsDQ5lmPJGDii/StENAz2\n    #     XmtiPOgfG9Upb0GAJBCuKnrU9bIcT4L20gd2F4Y14ccyjlf8UiUi192IX6yM9OjT\n    #     +TuXwZgqnTOq6piVgr+FTSa24qSvaXb5z/mJDLlk23npecTouLg83TNSn3R6fYQr\n    #     d/Y9eXuUJ8U7/qTh2Ulz071AO9KzPOmleYPTx4Xty4xAtWi1QE5NHW9/Ajlv5OtO\n    #     OnMNWIs7ssDJBsB7VFC8hcwf79jz7kC0xmQqDfw51Xhhk04kla+v+HZcFW2AO9so\n    #     6ZdVHHQnIbJa7yQJKZ+hK49IOoBR6JgdB5kymoplLLiuqZSYTcwSBZ72FYTm3iAr\n    #     jzvt1hxpxVDmXvRnkhRrIRhK4QgJL0jRmirBjDY+PYYd7bdRIjN7WNZLFsgplnS8\n    #     9w6CwG32pRlm0c8kkiQ7FXA6BYCqOsDI8f1VGQv331OpR2Ck+FTv+L7DAmg6l37W\n    #     +LB9LGh4OAp68ImTjqf6ioGKG0RBSznwME+r4nXtT1S/qLR6ASWUS4ViWRhbRlNK\n    #     XWyb96wrUlv+E8I=\n    #     -----END CERTIFICATE-----\n\n  # -- *DEPRECATED:* Instead, use `configs.credentialTemplates` and/or `configs.repositories`\n  repositoryCredentials: {}\n\n  # -- Repository credentials to be used as Templates for other repos\n  ## Creates a secret for each key/value specified below to create repository credentials\n  credentialTemplates: {}\n    # github-enterprise-creds-1:\n    #   url: https://github.com/argoproj\n    #   githubAppID: 1\n    #   githubAppInstallationID: 2\n    #   githubAppEnterpriseBaseUrl: https://ghe.example.com/api/v3\n    #   githubAppPrivateKey: |\n    #     -----BEGIN OPENSSH PRIVATE KEY-----\n    #     ...\n    #     -----END OPENSSH PRIVATE KEY-----\n    # https-creds:\n    #   url: https://github.com/argoproj\n    #   password: my-password\n    #   username: my-username\n    # ssh-creds:\n    #  url: git@github.com:argoproj-labs\n    #  sshPrivateKey: |\n    #    -----BEGIN OPENSSH PRIVATE KEY-----\n    #    ...\n    #    -----END OPENSSH PRIVATE KEY-----\n\n  # -- Repositories list to be used by applications\n  ## Creates a secret for each key/value specified below to create repositories\n  ## Note: the last example in the list would use a repository credential template, configured under \"configs.repositoryCredentials\".\n  repositories: {}\n    # istio-helm-repo:\n    #   url: https://storage.googleapis.com/istio-prerelease/daily-build/master-latest-daily/charts\n    #   name: istio.io\n    #   type: helm\n    # private-helm-repo:\n    #   url: https://my-private-chart-repo.internal\n    #   name: private-repo\n    #   type: helm\n    #   password: my-password\n    #   username: my-username\n    # private-repo:\n    #   url: https://github.com/argoproj/private-repo\n\n  secret:\n    # -- Create the argocd-secret\n    createSecret: true\n    # -- Annotations to be added to argocd-secret\n    annotations: {}\n\n    # -- Shared secret for authenticating GitHub webhook events\n    githubSecret: \"\"\n    # -- Shared secret for authenticating GitLab webhook events\n    gitlabSecret: \"\"\n    # -- Shared secret for authenticating BitbucketServer webhook events\n    bitbucketServerSecret: \"\"\n    # -- UUID for authenticating Bitbucket webhook events\n    bitbucketUUID: \"\"\n    # -- Shared secret for authenticating Gogs webhook events\n    gogsSecret: \"\"\n\n    # -- add additional secrets to be added to argocd-secret\n    ## Custom secrets. Useful for injecting SSO secrets into environment variables.\n    ## Ref: https://argoproj.github.io/argo-cd/operator-manual/sso/\n    ## Note that all values must be non-empty.\n    extra:\n      {}\n      # LDAP_PASSWORD: \"mypassword\"\n\n    # -- Argo TLS Data\n    argocdServerTlsConfig:\n      {}\n      # key:\n      # crt: |\n      #   -----BEGIN CERTIFICATE-----\n      #   \u003ccert data\u003e\n      #   -----END CERTIFICATE-----\n      #   -----BEGIN CERTIFICATE-----\n      #   \u003cca cert data\u003e\n      #   -----END CERTIFICATE-----\n\n    # -- Bcrypt hashed admin password\n    ## Argo expects the password in the secret to be bcrypt hashed. You can create this hash with\n    ## `htpasswd -nbBC 10 \"\" $ARGO_PWD | tr -d ':\\n' | sed 's/$2y/$2a/'`\n    argocdServerAdminPassword: \"\"\n    # -- Admin password modification time. Eg. `\"2006-01-02T15:04:05Z\"`\n    # @default -- `\"\"` (defaults to current time)\n    argocdServerAdminPasswordMtime: \"\"\n\n  # -- Define custom [CSS styles] for your argo instance.\n  # This setting will automatically mount the provided CSS and reference it in the argo configuration.\n  # @default -- `\"\"` (See [values.yaml])\n  ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/custom-styles/\n  styles: \"\"\n  # styles: |\n  #  .nav-bar {\n  #    background: linear-gradient(to bottom, #999, #777, #333, #222, #111);\n  #  }\n\nopenshift:\n  # -- enables using arbitrary uid for argo repo server\n  enabled: false\n"
            ],
            "verify": false,
            "version": "3.33.5",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_eks_cluster.eks_cluster",
            "data.aws_eks_cluster.cluster",
            "helm_release.aws-load-balancer-controller",
            "helm_release.external_dns"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "aws-load-balancer-controller",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "aws-load-balancer-controller",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "aws-load-balancer-controller",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "v2.3.1",
                "chart": "aws-load-balancer-controller",
                "name": "aws-load-balancer-controller",
                "namespace": "kube-system",
                "revision": 2,
                "values": "{\"clusterName\":\"terraformEKScluster\",\"serviceAccount\":{\"annotations\":{\"eks.amazonaws.com/role-arn\":\"arn:aws:iam::687227541429:role/aws_lb_controller2022021301432680140000000c\"}}}",
                "version": "1.3.3"
              }
            ],
            "name": "aws-load-balancer-controller",
            "namespace": "kube-system",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://aws.github.io/eks-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [
              {
                "name": "clusterName",
                "type": "",
                "value": "terraformEKScluster"
              },
              {
                "name": "serviceAccount.annotations.eks\\.amazonaws\\.com/role-arn",
                "type": "",
                "value": "arn:aws:iam::687227541429:role/aws_lb_controller2022021301432680140000000c"
              }
            ],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": null,
            "verify": false,
            "version": "1.3.3",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_policy.aws_lb_controller",
            "aws_iam_role_policy_attachment.eks-cluster-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.eks-cluster-AmazonEKSServicePolicy",
            "aws_subnet.privateEC2Subnet1",
            "data.tls_certificate.cluster",
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.aws_lb_controller",
            "aws_security_group.eks-cluster",
            "aws_subnet.privateEC2Subnet2",
            "aws_iam_openid_connect_provider.cluster",
            "aws_iam_role_policy_attachment.aws_lb_controller",
            "aws_vpc.test",
            "data.aws_eks_cluster.cluster",
            "aws_iam_role.iam-role-eks-cluster"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "external_dns",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "external-dns",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "external-dns",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "0.10.2",
                "chart": "external-dns",
                "name": "external-dns",
                "namespace": "default",
                "revision": 1,
                "values": "{\"affinity\":{},\"aws-zone-type\":\"public\",\"deploymentAnnotations\":{},\"domainFilters\":[\"kubedns.click\"],\"env\":[],\"extraArgs\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"fullnameOverride\":\"\",\"image\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"k8s.gcr.io/external-dns/external-dns\",\"tag\":\"\"},\"imagePullSecrets\":[],\"interval\":\"1m\",\"livenessProbe\":{\"failureThreshold\":2,\"httpGet\":{\"path\":\"/healthz\",\"port\":\"http\"},\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5},\"logFormat\":\"text\",\"logLevel\":\"info\",\"nameOverride\":\"\",\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"podSecurityContext\":{\"fsGroup\":65534},\"policy\":\"upsert-only\",\"priorityClassName\":\"\",\"provider\":\"aws\",\"rbac\":{\"additionalPermissions\":{},\"create\":true},\"readinessProbe\":{\"failureThreshold\":6,\"httpGet\":{\"path\":\"/healthz\",\"port\":\"http\"},\"initialDelaySeconds\":5,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":5},\"registry\":\"txt\",\"resources\":{},\"securityContext\":{\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"runAsUser\":65534},\"service\":{\"annotations\":{},\"port\":7979},\"serviceAccount\":{\"annotations\":{\"eks.amazonaws.com/role-arn\":\"arn:aws:iam::687227541429:role/AllowExternalDNSUpdates2022021301432680130000000b\"},\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":false,\"interval\":\"1m\",\"scrapeTimeout\":\"10s\"},\"sources\":[\"service\",\"ingress\"],\"terminationGracePeriodSeconds\":null,\"tolerations\":[],\"topologySpreadConstraints\":[],\"triggerLoopOnEvent\":false,\"txtOwnerId\":\"Z096848533MX8215J1WYJ\",\"txtPrefix\":\"\",\"txtSuffix\":\"\"}",
                "version": "1.7.1"
              }
            ],
            "name": "external-dns",
            "namespace": "default",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://kubernetes-sigs.github.io/external-dns/",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [
              {
                "name": "serviceAccount.annotations.eks\\.amazonaws\\.com/role-arn",
                "type": "",
                "value": "arn:aws:iam::687227541429:role/AllowExternalDNSUpdates2022021301432680130000000b"
              },
              {
                "name": "txtOwnerId",
                "type": "",
                "value": "Z096848533MX8215J1WYJ"
              }
            ],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "# Default values for external-dns.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\nimage:\n  repository: k8s.gcr.io/external-dns/external-dns\n  # Overrides the image tag whose default is v{{ .Chart.AppVersion }}\n  tag: \"\"\n  pullPolicy: IfNotPresent\n\nimagePullSecrets: []\nnameOverride: \"\"\nfullnameOverride: \"\"\n\nserviceAccount:\n  # Specifies whether a service account should be created\n  create: true\n  # Annotations to add to the service account\n  annotations: {}\n  # The name of the service account to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: \"\"\n\nrbac:\n  # Specifies whether RBAC resources should be created\n  create: true\n  additionalPermissions: {}\n\n# Annotations to add to the Deployment\ndeploymentAnnotations: {}\n\npodLabels: {}\n\n# Annotations to add to the Pod\npodAnnotations: {}\n\npodSecurityContext:\n  fsGroup: 65534\n\nsecurityContext:\n  runAsNonRoot: true\n  runAsUser: 65534\n  readOnlyRootFilesystem: true\n  capabilities:\n    drop: [\"ALL\"]\n\npriorityClassName: \"\"\n\nterminationGracePeriodSeconds:\n\nserviceMonitor:\n  enabled: false\n  additionalLabels: {}\n  interval: 1m\n  scrapeTimeout: 10s\n\nenv: []\n\nlivenessProbe:\n  httpGet:\n    path: /healthz\n    port: http\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  timeoutSeconds: 5\n  failureThreshold: 2\n  successThreshold: 1\n\nreadinessProbe:\n  httpGet:\n    path: /healthz\n    port: http\n  initialDelaySeconds: 5\n  periodSeconds: 10\n  timeoutSeconds: 5\n  failureThreshold: 6\n  successThreshold: 1\n\nservice:\n  port: 7979\n  annotations: {}\n\nextraVolumes: []\n\nextraVolumeMounts: []\n\nresources: {}\n\nnodeSelector: {}\n\ntolerations: []\n\naffinity: {}\n\ntopologySpreadConstraints: []\n\nlogLevel: info\nlogFormat: text\n\ninterval: 1m\ntriggerLoopOnEvent: false\n\nsources:\n  - service\n  - ingress\n\npolicy: upsert-only\n\nregistry: txt\ntxtOwnerId: \"\"\n#\"Z01349923BCGOYGEKI8ZF\"\ntxtPrefix: \"\"\ntxtSuffix: \"\"\n\ndomainFilters: \n  - kubedns.click\n\nprovider: aws\naws-zone-type: public\nextraArgs: [] \n"
            ],
            "verify": false,
            "version": "1.7.1",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_openid_connect_provider.cluster",
            "data.tls_certificate.cluster",
            "helm_release.aws-load-balancer-controller",
            "aws_eks_cluster.eks_cluster",
            "aws_iam_role.AllowExternalDNSUpdates",
            "aws_iam_role.aws_lb_controller",
            "aws_iam_role_policy_attachment.eks-cluster-AmazonEKSClusterPolicy",
            "aws_subnet.privateEC2Subnet1",
            "aws_subnet.privateEC2Subnet2",
            "data.aws_eks_cluster.cluster",
            "aws_vpc.test",
            "aws_iam_policy.aws_lb_controller",
            "aws_iam_role.iam-role-eks-cluster",
            "aws_iam_role_policy_attachment.aws_lb_controller",
            "aws_iam_role_policy_attachment.eks-cluster-AmazonEKSServicePolicy",
            "aws_route53_zone.kubedns",
            "aws_security_group.eks-cluster"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "prometheus",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "kube-prometheus-stack",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "prometheus",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "0.54.0",
                "chart": "kube-prometheus-stack",
                "name": "prometheus",
                "namespace": "monitoring",
                "revision": 1,
                "values": "{\"additionalPrometheusRulesMap\":{},\"alertmanager\":{\"alertmanagerSpec\":{\"additionalPeers\":[],\"affinity\":{},\"alertmanagerConfigNamespaceSelector\":{},\"alertmanagerConfigSelector\":{},\"clusterAdvertiseAddress\":false,\"configMaps\":[],\"containers\":[],\"externalUrl\":null,\"forceEnableClusterMode\":false,\"image\":{\"repository\":\"quay.io/prometheus/alertmanager\",\"sha\":\"\",\"tag\":\"v0.23.0\"},\"initContainers\":[],\"listenLocal\":false,\"logFormat\":\"logfmt\",\"logLevel\":\"info\",\"nodeSelector\":{},\"paused\":false,\"podAntiAffinity\":\"\",\"podAntiAffinityTopologyKey\":\"kubernetes.io/hostname\",\"podMetadata\":{},\"portName\":\"http-web\",\"priorityClassName\":\"\",\"replicas\":1,\"resources\":{},\"retention\":\"120h\",\"routePrefix\":\"/\",\"secrets\":[],\"securityContext\":{\"fsGroup\":2000,\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":1000},\"storage\":{},\"tolerations\":[],\"topologySpreadConstraints\":[],\"useExistingSecret\":false,\"volumeMounts\":[],\"volumes\":[]},\"annotations\":{},\"apiVersion\":\"v2\",\"config\":{\"global\":{\"resolve_timeout\":\"5m\"},\"receivers\":[{\"name\":\"null\"}],\"route\":{\"group_by\":[\"job\"],\"group_interval\":\"5m\",\"group_wait\":\"30s\",\"receiver\":\"null\",\"repeat_interval\":\"12h\",\"routes\":[{\"match\":{\"alertname\":\"Watchdog\"},\"receiver\":\"null\"}]},\"templates\":[\"/etc/alertmanager/config/*.tmpl\"]},\"enabled\":true,\"extraSecret\":{\"annotations\":{},\"data\":{}},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"paths\":[],\"tls\":[]},\"ingressPerReplica\":{\"annotations\":{},\"enabled\":false,\"hostDomain\":\"\",\"hostPrefix\":\"\",\"labels\":{},\"paths\":[],\"tlsSecretName\":\"\",\"tlsSecretPerReplica\":{\"enabled\":false,\"prefix\":\"alertmanager\"}},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"secret\":{\"annotations\":{}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30903,\"port\":9093,\"targetPort\":9093,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"bearerTokenFile\":null,\"interval\":\"\",\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"scheme\":\"\",\"selfMonitor\":true,\"tlsConfig\":{}},\"servicePerReplica\":{\"annotations\":{},\"enabled\":false,\"loadBalancerSourceRanges\":[],\"nodePort\":30904,\"port\":9093,\"targetPort\":9093,\"type\":\"ClusterIP\"},\"templateFiles\":{},\"tplConfig\":false},\"commonLabels\":{},\"coreDns\":{\"enabled\":true,\"service\":{\"port\":9153,\"targetPort\":9153},\"serviceMonitor\":{\"interval\":\"\",\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[]}},\"defaultRules\":{\"additionalRuleLabels\":{},\"annotations\":{},\"appNamespacesTarget\":\".*\",\"create\":true,\"disabled\":{},\"labels\":{},\"rules\":{\"alertmanager\":true,\"configReloaders\":true,\"etcd\":true,\"general\":true,\"k8s\":true,\"kubeApiserver\":true,\"kubeApiserverAvailability\":true,\"kubeApiserverSlos\":true,\"kubePrometheusGeneral\":true,\"kubePrometheusNodeRecording\":true,\"kubeProxy\":true,\"kubeScheduler\":true,\"kubeStateMetrics\":true,\"kubelet\":true,\"kubernetesApps\":true,\"kubernetesResources\":true,\"kubernetesStorage\":true,\"kubernetesSystem\":true,\"network\":true,\"node\":true,\"nodeExporterAlerting\":true,\"nodeExporterRecording\":true,\"prometheus\":true,\"prometheusOperator\":true},\"runbookUrl\":\"https://runbooks.prometheus-operator.dev/runbooks\"},\"fullnameOverride\":\"\",\"global\":{\"imagePullSecrets\":[],\"rbac\":{\"create\":true,\"pspAnnotations\":{},\"pspEnabled\":false}},\"grafana\":{\"additionalDataSources\":[],\"adminPassword\":\"prom-operator\",\"defaultDashboardsEnabled\":true,\"defaultDashboardsTimezone\":\"utc\",\"deleteDatasources\":[],\"enabled\":true,\"extraConfigmapMounts\":[],\"forceDeployDashboards\":false,\"forceDeployDatasources\":false,\"ingress\":{\"annotations\":{\"alb.ingress.kubernetes.io/group.name\":\"alb-01\",\"alb.ingress.kubernetes.io/listen-ports\":\"[{\\\"HTTP\\\": 8080}]\",\"alb.ingress.kubernetes.io/scheme\":\"internet-facing\",\"alb.ingress.kubernetes.io/target-type\":\"instance\",\"external-dns.alpha.kubernetes.io/hostname\":\"grafana.kubedns.click\",\"kubernetes.io/ingress.class\":\"alb\"},\"enabled\":true,\"labels\":{},\"path\":\"/\",\"tls\":[]},\"namespaceOverride\":\"\",\"rbac\":{\"pspEnabled\":false},\"service\":{\"portName\":\"http-web\",\"type\":\"NodePort\"},\"serviceMonitor\":{\"enabled\":false,\"interval\":\"\",\"labels\":{},\"path\":\"/metrics\",\"relabelings\":[],\"scheme\":\"http\",\"scrapeTimeout\":\"30s\",\"tlsConfig\":{}},\"sidecar\":{\"dashboards\":{\"annotations\":{},\"enabled\":true,\"label\":\"grafana_dashboard\",\"multicluster\":{\"etcd\":{\"enabled\":false},\"global\":{\"enabled\":false}},\"provider\":{\"allowUiUpdates\":false}},\"datasources\":{\"annotations\":{},\"createPrometheusReplicasDatasources\":false,\"defaultDatasourceEnabled\":true,\"enabled\":true,\"label\":\"grafana_datasource\"}}},\"kube-state-metrics\":{\"namespaceOverride\":\"\",\"prometheus\":{\"monitor\":{\"enabled\":true,\"honorLabels\":true,\"interval\":\"\",\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"scrapeTimeout\":\"\"}},\"rbac\":{\"create\":true},\"releaseLabel\":true,\"selfMonitor\":{\"enabled\":false}},\"kubeApiServer\":{\"enabled\":true,\"serviceMonitor\":{\"interval\":\"\",\"jobLabel\":\"component\",\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"selector\":{\"matchLabels\":{\"component\":\"apiserver\",\"provider\":\"kubernetes\"}}},\"tlsConfig\":{\"insecureSkipVerify\":false,\"serverName\":\"kubernetes\"}},\"kubeControllerManager\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"port\":null,\"targetPort\":null},\"serviceMonitor\":{\"enabled\":true,\"https\":null,\"insecureSkipVerify\":null,\"interval\":\"\",\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"serverName\":null}},\"kubeDns\":{\"enabled\":false,\"service\":{\"dnsmasq\":{\"port\":10054,\"targetPort\":10054},\"skydns\":{\"port\":10055,\"targetPort\":10055}},\"serviceMonitor\":{\"dnsmasqMetricRelabelings\":[],\"dnsmasqRelabelings\":[],\"interval\":\"\",\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[]}},\"kubeEtcd\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"port\":2379,\"targetPort\":2379},\"serviceMonitor\":{\"caFile\":\"\",\"certFile\":\"\",\"enabled\":true,\"insecureSkipVerify\":false,\"interval\":\"\",\"keyFile\":\"\",\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"scheme\":\"http\",\"serverName\":\"\"}},\"kubeProxy\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"port\":10249,\"targetPort\":10249},\"serviceMonitor\":{\"enabled\":true,\"https\":false,\"interval\":\"\",\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[]}},\"kubeScheduler\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"port\":null,\"targetPort\":null},\"serviceMonitor\":{\"enabled\":true,\"https\":null,\"insecureSkipVerify\":null,\"interval\":\"\",\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"serverName\":null}},\"kubeStateMetrics\":{\"enabled\":true},\"kubeTargetVersionOverride\":\"\",\"kubeVersionOverride\":\"\",\"kubelet\":{\"enabled\":true,\"namespace\":\"kube-system\",\"serviceMonitor\":{\"cAdvisor\":true,\"cAdvisorMetricRelabelings\":[],\"cAdvisorRelabelings\":[{\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"https\":true,\"interval\":\"\",\"metricRelabelings\":[],\"probes\":true,\"probesMetricRelabelings\":[],\"probesRelabelings\":[{\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"proxyUrl\":\"\",\"relabelings\":[{\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"resource\":false,\"resourcePath\":\"/metrics/resource/v1alpha1\",\"resourceRelabelings\":[{\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}]}},\"nameOverride\":\"\",\"namespaceOverride\":\"\",\"nodeExporter\":{\"enabled\":true},\"prometheus\":{\"additionalPodMonitors\":[],\"additionalRulesForClusterRole\":[],\"additionalServiceMonitors\":[],\"annotations\":{},\"enabled\":true,\"extraSecret\":{\"annotations\":{},\"data\":{}},\"ingress\":{\"annotations\":{\"alb.ingress.kubernetes.io/group.name\":\"alb-01\",\"alb.ingress.kubernetes.io/listen-ports\":\"[{\\\"HTTP\\\": 8081}]\",\"alb.ingress.kubernetes.io/scheme\":\"internet-facing\",\"alb.ingress.kubernetes.io/target-type\":\"instance\",\"external-dns.alpha.kubernetes.io/hostname\":\"prometheus.kubedns.click\",\"kubernetes.io/ingress.class\":\"alb\"},\"enabled\":true,\"labels\":{},\"paths\":[\"/*\"],\"tls\":[]},\"ingressPerReplica\":{\"annotations\":{},\"enabled\":false,\"hostDomain\":\"\",\"hostPrefix\":\"\",\"labels\":{},\"paths\":[],\"tlsSecretName\":\"\",\"tlsSecretPerReplica\":{\"enabled\":false,\"prefix\":\"prometheus\"}},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"podSecurityPolicy\":{\"allowedCapabilities\":[],\"allowedHostPaths\":[],\"volumes\":[]},\"prometheusSpec\":{\"additionalAlertManagerConfigs\":[],\"additionalAlertManagerConfigsSecret\":{},\"additionalAlertRelabelConfigs\":[],\"additionalPrometheusSecretsAnnotations\":{},\"additionalRemoteRead\":[],\"additionalRemoteWrite\":[],\"additionalScrapeConfigs\":[],\"additionalScrapeConfigsSecret\":{},\"affinity\":{},\"alertingEndpoints\":[],\"allowOverlappingBlocks\":false,\"apiserverConfig\":{},\"arbitraryFSAccessThroughSMs\":false,\"configMaps\":[],\"containers\":[],\"disableCompaction\":false,\"enableAdminAPI\":false,\"enableFeatures\":[],\"enforcedLabelLimit\":false,\"enforcedLabelNameLengthLimit\":false,\"enforcedLabelValueLengthLimit\":false,\"enforcedNamespaceLabel\":\"\",\"enforcedSampleLimit\":false,\"enforcedTargetLimit\":false,\"evaluationInterval\":\"\",\"externalLabels\":{},\"externalUrl\":\"\",\"ignoreNamespaceSelectors\":false,\"image\":{\"repository\":\"quay.io/prometheus/prometheus\",\"sha\":\"\",\"tag\":\"v2.32.1\"},\"initContainers\":[],\"listenLocal\":false,\"logFormat\":\"logfmt\",\"logLevel\":\"info\",\"nodeSelector\":{},\"overrideHonorLabels\":false,\"overrideHonorTimestamps\":false,\"paused\":false,\"podAntiAffinity\":\"\",\"podAntiAffinityTopologyKey\":\"kubernetes.io/hostname\",\"podMetadata\":{},\"podMonitorNamespaceSelector\":{},\"podMonitorSelector\":{},\"podMonitorSelectorNilUsesHelmValues\":true,\"portName\":\"http-web\",\"priorityClassName\":\"\",\"probeNamespaceSelector\":{},\"probeSelector\":{},\"probeSelectorNilUsesHelmValues\":true,\"prometheusExternalLabelName\":\"\",\"prometheusExternalLabelNameClear\":false,\"prometheusRulesExcludedFromEnforce\":[],\"query\":{},\"queryLogFile\":false,\"remoteRead\":[],\"remoteWrite\":[],\"remoteWriteDashboards\":false,\"replicaExternalLabelName\":\"\",\"replicaExternalLabelNameClear\":false,\"replicas\":1,\"resources\":{},\"retention\":\"10d\",\"retentionSize\":\"\",\"routePrefix\":\"/\",\"ruleNamespaceSelector\":{},\"ruleSelector\":{},\"ruleSelectorNilUsesHelmValues\":true,\"scrapeInterval\":\"\",\"scrapeTimeout\":\"\",\"secrets\":[],\"securityContext\":{\"fsGroup\":2000,\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":1000},\"serviceMonitorNamespaceSelector\":{},\"serviceMonitorSelector\":{},\"serviceMonitorSelectorNilUsesHelmValues\":true,\"shards\":1,\"storageSpec\":{},\"thanos\":{},\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[],\"volumes\":[],\"walCompression\":false,\"web\":{}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30090,\"port\":9090,\"publishNotReadyAddresses\":false,\"sessionAffinity\":\"\",\"targetPort\":9090,\"type\":\"NodePort\"},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"bearerTokenFile\":null,\"interval\":\"\",\"metricRelabelings\":[],\"relabelings\":[],\"scheme\":\"\",\"selfMonitor\":true,\"tlsConfig\":{}},\"servicePerReplica\":{\"annotations\":{},\"enabled\":false,\"loadBalancerSourceRanges\":[],\"nodePort\":30091,\"port\":9090,\"targetPort\":9090,\"type\":\"ClusterIP\"},\"thanosIngress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"nodePort\":30901,\"paths\":[],\"servicePort\":10901,\"tls\":[]},\"thanosService\":{\"annotations\":{},\"clusterIP\":\"None\",\"enabled\":false,\"httpNodePort\":30902,\"httpPort\":10902,\"httpPortName\":\"http\",\"labels\":{},\"nodePort\":30901,\"port\":10901,\"portName\":\"grpc\",\"targetHttpPort\":\"http\",\"targetPort\":\"grpc\",\"type\":\"ClusterIP\"},\"thanosServiceExternal\":{\"annotations\":{},\"enabled\":false,\"httpNodePort\":30902,\"httpPort\":10902,\"httpPortName\":\"http\",\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30901,\"port\":10901,\"portName\":\"grpc\",\"targetHttpPort\":\"http\",\"targetPort\":\"grpc\",\"type\":\"LoadBalancer\"},\"thanosServiceMonitor\":{\"bearerTokenFile\":null,\"enabled\":false,\"interval\":\"\",\"metricRelabelings\":[],\"relabelings\":[],\"scheme\":\"\",\"tlsConfig\":{}}},\"prometheus-node-exporter\":{\"extraArgs\":[\"--collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\",\"--collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$\"],\"namespaceOverride\":\"\",\"podLabels\":{\"jobLabel\":\"node-exporter\"},\"prometheus\":{\"monitor\":{\"enabled\":true,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"scrapeTimeout\":\"\"}},\"rbac\":{\"pspEnabled\":false},\"service\":{\"portName\":\"http-metrics\"}},\"prometheusOperator\":{\"admissionWebhooks\":{\"caBundle\":\"\",\"certManager\":{\"admissionCert\":{\"duration\":\"\"},\"enabled\":false,\"rootCert\":{\"duration\":\"\"}},\"enabled\":true,\"failurePolicy\":\"Fail\",\"patch\":{\"affinity\":{},\"enabled\":true,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen\",\"sha\":\"f3b6b39a6062328c095337b4cadcefd1612348fdd5190b1dcbcb9b9e90bd8068\",\"tag\":\"v1.0\"},\"nodeSelector\":{},\"podAnnotations\":{},\"priorityClassName\":\"\",\"resources\":{},\"securityContext\":{\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":2000},\"tolerations\":[]}},\"affinity\":{},\"alertmanagerInstanceNamespaces\":[],\"denyNamespaces\":[],\"dnsConfig\":{},\"enabled\":true,\"hostNetwork\":false,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"quay.io/prometheus-operator/prometheus-operator\",\"sha\":\"\",\"tag\":\"v0.53.1\"},\"kubeletService\":{\"enabled\":true,\"name\":\"\",\"namespace\":\"kube-system\"},\"namespaces\":{},\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"prometheusConfigReloader\":{\"image\":{\"repository\":\"quay.io/prometheus-operator/prometheus-config-reloader\",\"sha\":\"\",\"tag\":\"v0.53.1\"},\"resources\":{\"limits\":{\"cpu\":\"100m\",\"memory\":\"50Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"50Mi\"}}},\"prometheusInstanceNamespaces\":[],\"resources\":{},\"secretFieldSelector\":\"\",\"securityContext\":{\"fsGroup\":65534,\"runAsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30080,\"nodePortTls\":30443,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"interval\":\"\",\"metricRelabelings\":[],\"relabelings\":[],\"scrapeTimeout\":\"\",\"selfMonitor\":true},\"thanosImage\":{\"repository\":\"quay.io/thanos/thanos\",\"sha\":\"\",\"tag\":\"v0.24.0\"},\"thanosRulerInstanceNamespaces\":[],\"tls\":{\"enabled\":true,\"internalPort\":10250,\"tlsMinVersion\":\"VersionTLS13\"},\"tolerations\":[]}}",
                "version": "32.2.0"
              }
            ],
            "name": "prometheus",
            "namespace": "monitoring",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://prometheus-community.github.io/helm-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "# Default values for kube-prometheus-stack.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\n## Provide a name in place of kube-prometheus-stack for `app:` labels\n##\nnameOverride: \"\"\n\n## Override the deployment namespace\n##\nnamespaceOverride: \"\"\n\n## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.16.6\n##\nkubeTargetVersionOverride: \"\"\n\n## Allow kubeVersion to be overridden while creating the ingress\n##\nkubeVersionOverride: \"\"\n\n## Provide a name to substitute for the full names of resources\n##\nfullnameOverride: \"\"\n\n## Labels to apply to all resources\n##\ncommonLabels: {}\n# scmhash: abc123\n# myLabel: aakkmd\n\n## Create default rules for monitoring the cluster\n##\ndefaultRules:\n  create: true\n  rules:\n    alertmanager: true\n    etcd: true\n    configReloaders: true\n    general: true\n    k8s: true\n    kubeApiserver: true\n    kubeApiserverAvailability: true\n    kubeApiserverSlos: true\n    kubelet: true\n    kubeProxy: true\n    kubePrometheusGeneral: true\n    kubePrometheusNodeRecording: true\n    kubernetesApps: true\n    kubernetesResources: true\n    kubernetesStorage: true\n    kubernetesSystem: true\n    kubeScheduler: true\n    kubeStateMetrics: true\n    network: true\n    node: true\n    nodeExporterAlerting: true\n    nodeExporterRecording: true\n    prometheus: true\n    prometheusOperator: true\n\n  ## Reduce app namespace alert scope\n  appNamespacesTarget: \".*\"\n\n  ## Labels for default rules\n  labels: {}\n  ## Annotations for default rules\n  annotations: {}\n\n  ## Additional labels for PrometheusRule alerts\n  additionalRuleLabels: {}\n\n  ## Prefix for runbook URLs. Use this to override the first part of the runbookURLs that is common to all rules.\n  runbookUrl: \"https://runbooks.prometheus-operator.dev/runbooks\"\n\n  ## Disabled PrometheusRule alerts\n  disabled: {}\n  # KubeAPIDown: true\n  # NodeRAIDDegraded: true\n\n## Deprecated way to provide custom recording or alerting rules to be deployed into the cluster.\n##\n# additionalPrometheusRules: []\n#  - name: my-rule-file\n#    groups:\n#      - name: my_group\n#        rules:\n#        - record: my_record\n#          expr: 100 * my_record\n\n## Provide custom recording or alerting rules to be deployed into the cluster.\n##\nadditionalPrometheusRulesMap: {}\n#  rule-name:\n#    groups:\n#    - name: my_group\n#      rules:\n#      - record: my_record\n#        expr: 100 * my_record\n\n##\nglobal:\n  rbac:\n    create: true\n    pspEnabled: false\n    pspAnnotations: {}\n      ## Specify pod annotations\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\n      ##\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n\n  ## Reference to one or more secrets to be used when pulling images\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ##\n  imagePullSecrets: []\n  # - name: \"image-pull-secret\"\n\n## Configuration for alertmanager\n## ref: https://prometheus.io/docs/alerting/alertmanager/\n##\nalertmanager:\n\n  ## Deploy alertmanager\n  ##\n  enabled: true\n\n  ## Annotations for Alertmanager\n  ##\n  annotations: {}\n\n  ## Api that prometheus will use to communicate with alertmanager. Possible values are v1, v2\n  ##\n  apiVersion: v2\n\n  ## Service account for Alertmanager to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n    annotations: {}\n\n  ## Configure pod disruption budgets for Alertmanager\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\n  ## This configuration is immutable once created and will require the PDB to be deleted to be changed\n  ## https://github.com/kubernetes/kubernetes/issues/45398\n  ##\n  podDisruptionBudget:\n    enabled: false\n    minAvailable: 1\n    maxUnavailable: \"\"\n\n  ## Alertmanager configuration directives\n  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file\n  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/\n  ##\n  config:\n    global:\n      resolve_timeout: 5m\n    route:\n      group_by: ['job']\n      group_wait: 30s\n      group_interval: 5m\n      repeat_interval: 12h\n      receiver: 'null'\n      routes:\n      - match:\n          alertname: Watchdog\n        receiver: 'null'\n    receivers:\n    - name: 'null'\n    templates:\n    - '/etc/alertmanager/config/*.tmpl'\n\n  ## Pass the Alertmanager configuration directives through Helm's templating\n  ## engine. If the Alertmanager configuration contains Alertmanager templates,\n  ## they'll need to be properly escaped so that they are not interpreted by\n  ## Helm\n  ## ref: https://helm.sh/docs/developing_charts/#using-the-tpl-function\n  ##      https://prometheus.io/docs/alerting/configuration/#tmpl_string\n  ##      https://prometheus.io/docs/alerting/notifications/\n  ##      https://prometheus.io/docs/alerting/notification_examples/\n  tplConfig: false\n\n  ## Alertmanager template files to format alerts\n  ## By default, templateFiles are placed in /etc/alertmanager/config/ and if\n  ## they have a .tmpl file suffix will be loaded. See config.templates above\n  ## to change, add other suffixes. If adding other suffixes, be sure to update\n  ## config.templates above to include those suffixes.\n  ## ref: https://prometheus.io/docs/alerting/notifications/\n  ##      https://prometheus.io/docs/alerting/notification_examples/\n  ##\n  templateFiles: {}\n  #\n  ## An example template:\n  #   template_1.tmpl: |-\n  #       {{ define \"cluster\" }}{{ .ExternalURL | reReplaceAll \".*alertmanager\\\\.(.*)\" \"$1\" }}{{ end }}\n  #\n  #       {{ define \"slack.myorg.text\" }}\n  #       {{- $root := . -}}\n  #       {{ range .Alerts }}\n  #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`\n  #         *Cluster:* {{ template \"cluster\" $root }}\n  #         *Description:* {{ .Annotations.description }}\n  #         *Graph:* \u003c{{ .GeneratorURL }}|:chart_with_upwards_trend:\u003e\n  #         *Runbook:* \u003c{{ .Annotations.runbook }}|:spiral_note_pad:\u003e\n  #         *Details:*\n  #           {{ range .Labels.SortedPairs }} - *{{ .Name }}:* `{{ .Value }}`\n  #           {{ end }}\n  #       {{ end }}\n  #       {{ end }}\n\n  ingress:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n\n    labels: {}\n\n    ## Hosts must be provided if Ingress is enabled.\n    ##\n    hosts: []\n      # - alertmanager.domain.com\n\n    ## Paths to use for ingress rules - one path should match the alertmanagerSpec.routePrefix\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## TLS configuration for Alertmanager Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: alertmanager-general-tls\n    #   hosts:\n    #   - alertmanager.example.com\n\n  ## Configuration for Alertmanager secret\n  ##\n  secret:\n    annotations: {}\n\n  ## Configuration for creating an Ingress that will map to each Alertmanager replica service\n  ## alertmanager.servicePerReplica must be enabled\n  ##\n  ingressPerReplica:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n\n    ## Final form of the hostname for each per replica ingress is\n    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}\n    ##\n    ## Prefix for the per replica ingress that will have `-$replicaNumber`\n    ## appended to the end\n    hostPrefix: \"\"\n    ## Domain that will be used for the per replica ingress\n    hostDomain: \"\"\n\n    ## Paths to use for ingress rules\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## Secret name containing the TLS certificate for alertmanager per replica ingress\n    ## Secret must be manually created in the namespace\n    tlsSecretName: \"\"\n\n    ## Separated secret for each per replica Ingress. Can be used together with cert-manager\n    ##\n    tlsSecretPerReplica:\n      enabled: false\n      ## Final form of the secret for each per replica ingress is\n      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}\n      ##\n      prefix: \"alertmanager\"\n\n  ## Configuration for Alertmanager service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n    ## Port for Alertmanager Service to listen on\n    ##\n    port: 9093\n    ## To be used with a proxy extraContainer port\n    ##\n    targetPort: 9093\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30903\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n\n    ## Additional ports to open for Alertmanager service\n    additionalPorts: []\n\n    externalIPs: []\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    ## Service type\n    ##\n    type: ClusterIP\n\n  ## Configuration for creating a separate Service for each statefulset Alertmanager replica\n  ##\n  servicePerReplica:\n    enabled: false\n    annotations: {}\n\n    ## Port for Alertmanager Service per replica to listen on\n    ##\n    port: 9093\n\n    ## To be used with a proxy extraContainer port\n    targetPort: 9093\n\n    ## Port to expose on each node\n    ## Only used if servicePerReplica.type is 'NodePort'\n    ##\n    nodePort: 30904\n\n    ## Loadbalancer source IP ranges\n    ## Only used if servicePerReplica.type is \"LoadBalancer\"\n    loadBalancerSourceRanges: []\n    ## Service type\n    ##\n    type: ClusterIP\n\n  ## If true, create a serviceMonitor for alertmanager\n  ##\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    selfMonitor: true\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    bearerTokenFile:\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n  ## Settings affecting alertmanagerSpec\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerspec\n  ##\n  alertmanagerSpec:\n    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ## Metadata Labels and Annotations gets propagated to the Alertmanager pods.\n    ##\n    podMetadata: {}\n\n    ## Image of Alertmanager\n    ##\n    image:\n      repository: quay.io/prometheus/alertmanager\n      tag: v0.23.0\n      sha: \"\"\n\n    ## If true then the user will be responsible to provide a secret with alertmanager configuration\n    ## So when true the config part will be ignored (including templateFiles) and the one in the secret will be used\n    ##\n    useExistingSecret: false\n\n    ## Secrets is a list of Secrets in the same namespace as the Alertmanager object, which shall be mounted into the\n    ## Alertmanager Pods. The Secrets are mounted into /etc/alertmanager/secrets/.\n    ##\n    secrets: []\n\n    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods.\n    ## The ConfigMaps are mounted into /etc/alertmanager/configmaps/.\n    ##\n    configMaps: []\n\n    ## ConfigSecret is the name of a Kubernetes Secret in the same namespace as the Alertmanager object, which contains configuration for\n    ## this Alertmanager instance. Defaults to 'alertmanager-' The secret is mounted into /etc/alertmanager/config.\n    ##\n    # configSecret:\n\n    ## AlertmanagerConfigs to be selected to merge and configure Alertmanager with.\n    ##\n    alertmanagerConfigSelector: {}\n    ## Example which selects all alertmanagerConfig resources\n    ## with label \"alertconfig\" with values any of \"example-config\" or \"example-config-2\"\n    # alertmanagerConfigSelector:\n    #   matchExpressions:\n    #     - key: alertconfig\n    #       operator: In\n    #       values:\n    #         - example-config\n    #         - example-config-2\n    #\n    ## Example which selects all alertmanagerConfig resources with label \"role\" set to \"example-config\"\n    # alertmanagerConfigSelector:\n    #   matchLabels:\n    #     role: example-config\n\n    ## Namespaces to be selected for AlertmanagerConfig discovery. If nil, only check own namespace.\n    ##\n    alertmanagerConfigNamespaceSelector: {}\n    ## Example which selects all namespaces\n    ## with label \"alertmanagerconfig\" with values any of \"example-namespace\" or \"example-namespace-2\"\n    # alertmanagerConfigNamespaceSelector:\n    #   matchExpressions:\n    #     - key: alertmanagerconfig\n    #       operator: In\n    #       values:\n    #         - example-namespace\n    #         - example-namespace-2\n\n    ## Example which selects all namespaces with label \"alertmanagerconfig\" set to \"enabled\"\n    # alertmanagerConfigNamespaceSelector:\n    #   matchLabels:\n    #     alertmanagerconfig: enabled\n\n    ## Define Log Format\n    # Use logfmt (default) or json logging\n    logFormat: logfmt\n#\n    ## Log level for Alertmanager to be configured with.\n    ##\n    logLevel: info\n\n    ## Size is the expected size of the alertmanager cluster. The controller will eventually make the size of the\n    ## running cluster equal to the expected size.\n    replicas: 1\n\n    ## Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression\n    ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).\n    ##\n    retention: 120h\n\n    ## Storage is the definition of how storage will be used by the Alertmanager instances.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md\n    ##\n    storage: {}\n    # volumeClaimTemplate:\n    #   spec:\n    #     storageClassName: gluster\n    #     accessModes: [\"ReadWriteOnce\"]\n    #     resources:\n    #       requests:\n    #         storage: 50Gi\n    #   selector: {}\n\n\n    ## The external URL the Alertmanager instances will be available under. This is necessary to generate correct URLs. This is necessary if Alertmanager is not served from root of a DNS name. string  false\n    ##\n    externalUrl:\n\n    ## The route prefix Alertmanager registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,\n    ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.\n    ##\n    routePrefix: /\n\n    ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.\n    ##\n    paused: false\n\n    ## Define which Nodes the Pods are scheduled on.\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n\n    ## Define resources requests and limits for single Pods.\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ##\n    resources: {}\n    # requests:\n    #   memory: 400Mi\n\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\n    ##\n    podAntiAffinity: \"\"\n\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n    ##\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n\n    ## Assign custom affinity rules to the alertmanager instance\n    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n    ##\n    affinity: {}\n    # nodeAffinity:\n    #   requiredDuringSchedulingIgnoredDuringExecution:\n    #     nodeSelectorTerms:\n    #     - matchExpressions:\n    #       - key: kubernetes.io/e2e-az-name\n    #         operator: In\n    #         values:\n    #         - e2e-az1\n    #         - e2e-az2\n\n    ## If specified, the pod's tolerations.\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule\"\n\n    ## If specified, the pod's topology spread constraints.\n    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n    ##\n    topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n    #   labelSelector:\n    #     matchLabels:\n    #       app: alertmanager\n\n    ## SecurityContext holds pod-level security attributes and common container settings.\n    ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n    ##\n    securityContext:\n      runAsGroup: 2000\n      runAsNonRoot: true\n      runAsUser: 1000\n      fsGroup: 2000\n\n    ## ListenLocal makes the Alertmanager server listen on loopback, so that it does not bind against the Pod IP.\n    ## Note this is only for the Alertmanager UI, not the gossip communication.\n    ##\n    listenLocal: false\n\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an Alertmanager pod.\n    ##\n    containers: []\n\n    # Additional volumes on the output StatefulSet definition.\n    volumes: []\n\n    # Additional VolumeMounts on the output StatefulSet definition.\n    volumeMounts: []\n\n    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes\n    ## (permissions, dir tree) on mounted volumes before starting prometheus\n    initContainers: []\n\n    ## Priority class assigned to the Pods\n    ##\n    priorityClassName: \"\"\n\n    ## AdditionalPeers allows injecting a set of additional Alertmanagers to peer with to form a highly available cluster.\n    ##\n    additionalPeers: []\n\n    ## PortName to use for Alert Manager.\n    ##\n    portName: \"http-web\"\n\n    ## ClusterAdvertiseAddress is the explicit address to advertise in cluster. Needs to be provided for non RFC1918 [1] (public) addresses. [1] RFC1918: https://tools.ietf.org/html/rfc1918\n    ##\n    clusterAdvertiseAddress: false\n\n    ## ForceEnableClusterMode ensures Alertmanager does not deactivate the cluster mode when running with a single replica.\n    ## Use case is e.g. spanning an Alertmanager cluster across Kubernetes clusters with a single replica in each.\n    forceEnableClusterMode: false\n\n  ## ExtraSecret can be used to store various data in an extra secret\n  ## (use it for example to store hashed basic auth credentials)\n  extraSecret:\n    ## if not set, name will be auto generated\n    # name: \"\"\n    annotations: {}\n    data: {}\n  #   auth: |\n  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0\n  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.\n\n## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml\n##\ngrafana:\n  enabled: true\n  namespaceOverride: \"\"\n\n  ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled\n  ##\n  forceDeployDatasources: false\n\n  ## ForceDeployDashboard Create dashboard configmap even if grafana deployment has been disabled\n  ##\n  forceDeployDashboards: false\n\n  ## Deploy default dashboards\n  ##\n  defaultDashboardsEnabled: true\n\n  ## Timezone for the default dashboards\n  ## Other options are: browser or a specific timezone, i.e. Europe/Luxembourg\n  ##\n  defaultDashboardsTimezone: utc\n\n  adminPassword: prom-operator\n\n  rbac:\n    ## If true, Grafana PSPs will be created\n    ##\n    pspEnabled: false\n\n  ingress:\n    ## If true, Grafana Ingress will be created\n    ##\n    enabled: true\n\n    ## Annotations for Grafana Ingress\n    ##\n    annotations: \n      kubernetes.io/ingress.class: alb\n      alb.ingress.kubernetes.io/target-type: instance\n      alb.ingress.kubernetes.io/scheme: internet-facing\n      alb.ingress.kubernetes.io/group.name: alb-01\n      alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 8080}]'\n      external-dns.alpha.kubernetes.io/hostname: grafana.kubedns.click\n      # kubernetes.io/ingress.class: nginx\n      # kubernetes.io/tls-acme: \"true\"\n\n    ## Labels to be added to the Ingress\n    ##\n    labels: {}\n\n    ## Hostnames.\n    ## Must be provided if Ingress is enable.\n    ##\n    #hosts:\n    #  - \"grafana.kubedns.click\"\n    #hosts:  []\n\n    ## Path for grafana ingress\n    path: /\n\n    ## TLS configuration for grafana Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: grafana-general-tls\n    #   hosts:\n    #   - grafana.example.com\n\n  sidecar:\n    dashboards:\n      enabled: true\n      label: grafana_dashboard\n\n      ## Annotations for Grafana dashboard configmaps\n      ##\n      annotations: {}\n      multicluster:\n        global:\n          enabled: false\n        etcd:\n          enabled: false\n      provider:\n        allowUiUpdates: false\n    datasources:\n      enabled: true\n      defaultDatasourceEnabled: true\n\n      ## URL of prometheus datasource\n      ##\n      # url: http://prometheus-stack-prometheus:9090/\n\n      # If not defined, will use prometheus.prometheusSpec.scrapeInterval or its default\n      # defaultDatasourceScrapeInterval: 15s\n\n      ## Annotations for Grafana datasource configmaps\n      ##\n      annotations: {}\n\n      ## Create datasource for each Pod of Prometheus StatefulSet;\n      ## this uses headless service `prometheus-operated` which is\n      ## created by Prometheus Operator\n      ## ref: https://git.io/fjaBS\n      createPrometheusReplicasDatasources: false\n      label: grafana_datasource\n\n  extraConfigmapMounts: []\n  # - name: certs-configmap\n  #   mountPath: /etc/grafana/ssl/\n  #   configMap: certs-configmap\n  #   readOnly: true\n\n  deleteDatasources: []\n  # - name: example-datasource\n  #   orgId: 1\n\n  ## Configure additional grafana datasources (passed through tpl)\n  ## ref: http://docs.grafana.org/administration/provisioning/#datasources\n  additionalDataSources: []\n  # - name: prometheus-sample\n  #   access: proxy\n  #   basicAuth: true\n  #   basicAuthPassword: pass\n  #   basicAuthUser: daco\n  #   editable: false\n  #   jsonData:\n  #       tlsSkipVerify: true\n  #   orgId: 1\n  #   type: prometheus\n  #   url: https://{{ printf \"%s-prometheus.svc\" .Release.Name }}:9090\n  #   version: 1\n\n  ## Passed to grafana subchart and used by servicemonitor below\n  ##\n  service:\n    portName: http-web\n    type: NodePort\n\n  serviceMonitor:\n    # If true, a ServiceMonitor CRD is created for a prometheus operator\n    # https://github.com/coreos/prometheus-operator\n    #\n    enabled: false\n\n    # Path to use for scraping metrics. Might be different if server.root_url is set\n    # in grafana.ini\n    path: \"/metrics\"\n\n    #  namespace: monitoring  (defaults to use the namespace this chart is deployed to)\n\n    # labels for the ServiceMonitor\n    labels: {}\n\n    # Scrape interval. If not set, the Prometheus default scrape interval is used.\n    #\n    interval: \"\"\n    scheme: http\n    tlsConfig: {}\n    scrapeTimeout: 30s\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n## Component scraping the kube api server\n##\nkubeApiServer:\n  enabled: true\n  tlsConfig:\n    serverName: kubernetes\n    insecureSkipVerify: false\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    jobLabel: component\n    selector:\n      matchLabels:\n        component: apiserver\n        provider: kubernetes\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels:\n    #     - __meta_kubernetes_namespace\n    #     - __meta_kubernetes_service_name\n    #     - __meta_kubernetes_endpoint_port_name\n    #   action: keep\n    #   regex: default;kubernetes;https\n    # - targetLabel: __address__\n    #   replacement: kubernetes.default.svc:443\n\n## Component scraping the kubelet and kubelet-hosted cAdvisor\n##\nkubelet:\n  enabled: true\n  namespace: kube-system\n\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## Enable scraping the kubelet over https. For requirements to enable this see\n    ## https://github.com/prometheus-operator/prometheus-operator/issues/926\n    ##\n    https: true\n\n    ## Enable scraping /metrics/cadvisor from kubelet's service\n    ##\n    cAdvisor: true\n\n    ## Enable scraping /metrics/probes from kubelet's service\n    ##\n    probes: true\n\n    ## Enable scraping /metrics/resource from kubelet's service\n    ## This is disabled by default because container metrics are already exposed by cAdvisor\n    ##\n    resource: false\n    # From kubernetes 1.18, /metrics/resource/v1alpha1 renamed to /metrics/resource\n    resourcePath: \"/metrics/resource/v1alpha1\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    cAdvisorMetricRelabelings: []\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    probesMetricRelabelings: []\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    ## metrics_path is required to match upstream rules and charts\n    cAdvisorRelabelings:\n      - sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    probesRelabelings:\n      - sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    resourceRelabelings:\n      - sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    ## metrics_path is required to match upstream rules and charts\n    relabelings:\n      - sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n## Component scraping the kube controller manager\n##\nkubeControllerManager:\n  enabled: true\n\n  ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## If using kubeControllerManager.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change\n    ## of default port in Kubernetes 1.22.\n    ##\n    port: null\n    targetPort: null\n    # selector:\n    #   component: kube-controller-manager\n\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## Enable scraping kube-controller-manager over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version.\n    ##\n    https: null\n\n    # Skip TLS certificate validation when scraping\n    insecureSkipVerify: null\n\n    # Name of the server to use when validating TLS certificate\n    serverName: null\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n## Component scraping coreDns. Use either this or kubeDns\n##\ncoreDns:\n  enabled: true\n  service:\n    port: 9153\n    targetPort: 9153\n    # selector:\n    #   k8s-app: kube-dns\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n## Component scraping kubeDns. Use either this or coreDns\n##\nkubeDns:\n  enabled: false\n  service:\n    dnsmasq:\n      port: 10054\n      targetPort: 10054\n    skydns:\n      port: 10055\n      targetPort: 10055\n    # selector:\n    #   k8s-app: kube-dns\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    dnsmasqMetricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    dnsmasqRelabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n## Component scraping etcd\n##\nkubeEtcd:\n  enabled: true\n\n  ## If your etcd is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    port: 2379\n    targetPort: 2379\n    # selector:\n    #   component: etcd\n\n  ## Configure secure access to the etcd cluster by loading a secret into prometheus and\n  ## specifying security configuration below. For example, with a secret named etcd-client-cert\n  ##\n  ## serviceMonitor:\n  ##   scheme: https\n  ##   insecureSkipVerify: false\n  ##   serverName: localhost\n  ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca\n  ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client\n  ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key\n  ##\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n    scheme: http\n    insecureSkipVerify: false\n    serverName: \"\"\n    caFile: \"\"\n    certFile: \"\"\n    keyFile: \"\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n\n## Component scraping kube scheduler\n##\nkubeScheduler:\n  enabled: true\n\n  ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## If using kubeScheduler.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change\n    ## of default port in Kubernetes 1.23.\n    ##\n    port: null\n    targetPort: null\n    # selector:\n    #   component: kube-scheduler\n\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n    ## Enable scraping kube-scheduler over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version.\n    ##\n    https: null\n\n    ## Skip TLS certificate validation when scraping\n    insecureSkipVerify: null\n\n    ## Name of the server to use when validating TLS certificate\n    serverName: null\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n\n## Component scraping kube proxy\n##\nkubeProxy:\n  enabled: true\n\n  ## If your kube proxy is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  service:\n    enabled: true\n    port: 10249\n    targetPort: 10249\n    # selector:\n    #   k8s-app: kube-proxy\n\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## Enable scraping kube-proxy over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks\n    ##\n    https: false\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n\n## Component scraping kube state metrics\n##\nkubeStateMetrics:\n  enabled: true\n\n## Configuration for kube-state-metrics subchart\n##\nkube-state-metrics:\n  namespaceOverride: \"\"\n  rbac:\n    create: true\n  releaseLabel: true\n  prometheus:\n    monitor:\n      enabled: true\n\n      ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n      ##\n      interval: \"\"\n\n      ## Scrape Timeout. If not set, the Prometheus default scrape timeout is used.\n      ##\n      scrapeTimeout: \"\"\n\n      ## proxyUrl: URL of a proxy that should be used for scraping.\n      ##\n      proxyUrl: \"\"\n\n      # Keep labels from scraped data, overriding server-side labels\n      ##\n      honorLabels: true\n\n      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n      ##\n      metricRelabelings: []\n      # - action: keep\n      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n      #   sourceLabels: [__name__]\n\n      ## RelabelConfigs to apply to samples before scraping\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n      ##\n      relabelings: []\n      # - sourceLabels: [__meta_kubernetes_pod_node_name]\n      #   separator: ;\n      #   regex: ^(.*)$\n      #   targetLabel: nodename\n      #   replacement: $1\n      #   action: replace\n\n  selfMonitor:\n    enabled: false\n\n## Deploy node exporter as a daemonset to all nodes\n##\nnodeExporter:\n  enabled: true\n\n## Configuration for prometheus-node-exporter subchart\n##\nprometheus-node-exporter:\n  namespaceOverride: \"\"\n  podLabels:\n    ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards\n    ##\n    jobLabel: node-exporter\n  extraArgs:\n    - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n    - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$\n  service:\n    portName: http-metrics\n  prometheus:\n    monitor:\n      enabled: true\n\n      jobLabel: jobLabel\n\n      ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n      ##\n      interval: \"\"\n\n      ## How long until a scrape request times out. If not set, the Prometheus default scape timeout is used.\n      ##\n      scrapeTimeout: \"\"\n\n      ## proxyUrl: URL of a proxy that should be used for scraping.\n      ##\n      proxyUrl: \"\"\n\n      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n      ##\n      metricRelabelings: []\n      # - sourceLabels: [__name__]\n      #   separator: ;\n      #   regex: ^node_mountstats_nfs_(event|operations|transport)_.+\n      #   replacement: $1\n      #   action: drop\n\n      ## RelabelConfigs to apply to samples before scraping\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n      ##\n      relabelings: []\n      # - sourceLabels: [__meta_kubernetes_pod_node_name]\n      #   separator: ;\n      #   regex: ^(.*)$\n      #   targetLabel: nodename\n      #   replacement: $1\n      #   action: replace\n  rbac:\n    ## If true, create PSPs for node-exporter\n    ##\n    pspEnabled: false\n\n## Manages Prometheus and Alertmanager components\n##\nprometheusOperator:\n  enabled: true\n\n  ## Prometheus-Operator v0.39.0 and later support TLS natively.\n  ##\n  tls:\n    enabled: true\n    # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants\n    tlsMinVersion: VersionTLS13\n    # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.\n    internalPort: 10250\n\n  ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted\n  ## rules from making their way into prometheus and potentially preventing the container from starting\n  admissionWebhooks:\n    failurePolicy: Fail\n    enabled: true\n    ## A PEM encoded CA bundle which will be used to validate the webhook's server certificate.\n    ## If unspecified, system trust roots on the apiserver are used.\n    caBundle: \"\"\n    ## If enabled, generate a self-signed certificate, then patch the webhook configurations with the generated data.\n    ## On chart upgrades (or if the secret exists) the cert will not be re-generated. You can use this to provide your own\n    ## certs ahead of time if you wish.\n    ##\n    patch:\n      enabled: true\n      image:\n        repository: k8s.gcr.io/ingress-nginx/kube-webhook-certgen\n        tag: v1.0\n        sha: \"f3b6b39a6062328c095337b4cadcefd1612348fdd5190b1dcbcb9b9e90bd8068\"\n        pullPolicy: IfNotPresent\n      resources: {}\n      ## Provide a priority class name to the webhook patching job\n      ##\n      priorityClassName: \"\"\n      podAnnotations: {}\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n\n      ## SecurityContext holds pod-level security attributes and common container settings.\n      ## This defaults to non root user with uid 2000 and gid 2000. *v1.PodSecurityContext  false\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n      ##\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n\n    # Use certmanager to generate webhook certs\n    certManager:\n      enabled: false\n      # self-signed root certificate\n      rootCert:\n        duration: \"\"  # default to be 5y\n      admissionCert:\n        duration: \"\"  # default to be 1y\n      # issuerRef:\n      #   name: \"issuer\"\n      #   kind: \"ClusterIssuer\"\n\n  ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list).\n  ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration\n  ##\n  namespaces: {}\n    # releaseNamespace: true\n    # additional:\n    # - kube-system\n\n  ## Namespaces not to scope the interaction of the Prometheus Operator (deny list).\n  ##\n  denyNamespaces: []\n\n  ## Filter namespaces to look for prometheus-operator custom resources\n  ##\n  alertmanagerInstanceNamespaces: []\n  prometheusInstanceNamespaces: []\n  thanosRulerInstanceNamespaces: []\n\n  ## The clusterDomain value will be added to the cluster.peer option of the alertmanager.\n  ## Without this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated:9094 (default value)\n  ## With this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated.namespace.svc.cluster-domain:9094\n  ##\n  # clusterDomain: \"cluster.local\"\n\n  ## Service account for Alertmanager to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n\n  ## Configuration for Prometheus operator service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n  ## Port to expose on each node\n  ## Only used if service.type is 'NodePort'\n  ##\n    nodePort: 30080\n\n    nodePortTls: 30443\n\n  ## Additional ports to open for Prometheus service\n  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services\n  ##\n    additionalPorts: []\n\n  ## Loadbalancer IP\n  ## Only use if service.type is \"LoadBalancer\"\n  ##\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n  ## Service type\n  ## NodePort, ClusterIP, LoadBalancer\n  ##\n    type: ClusterIP\n\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n  ## Labels to add to the operator pod\n  ##\n  podLabels: {}\n\n  ## Annotations to add to the operator pod\n  ##\n  podAnnotations: {}\n\n  ## Assign a PriorityClassName to pods if set\n  # priorityClassName: \"\"\n\n  ## Define Log Format\n  # Use logfmt (default) or json logging\n  # logFormat: logfmt\n\n  ## Decrease log verbosity to errors only\n  # logLevel: error\n\n  ## If true, the operator will create and maintain a service for scraping kubelets\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/helm/prometheus-operator/README.md\n  ##\n  kubeletService:\n    enabled: true\n    namespace: kube-system\n    ## Use '{{ template \"kube-prometheus-stack.fullname\" . }}-kubelet' by default\n    name: \"\"\n\n  ## Create a servicemonitor for the operator\n  ##\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    ## Scrape timeout. If not set, the Prometheus default scrape timeout is used.\n    scrapeTimeout: \"\"\n    selfMonitor: true\n\n    ## Metric relabel configs to apply to samples before ingestion.\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    #   relabel configs to apply to samples before ingestion.\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n  ## Resource limits \u0026 requests\n  ##\n  resources: {}\n  # limits:\n  #   cpu: 200m\n  #   memory: 200Mi\n  # requests:\n  #   cpu: 100m\n  #   memory: 100Mi\n\n  # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\n  # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\n  ##\n  hostNetwork: false\n\n  ## Define which Nodes the Pods are scheduled on.\n  ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n\n  ## Tolerations for use with node taints\n  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  ##\n  tolerations: []\n  # - key: \"key\"\n  #   operator: \"Equal\"\n  #   value: \"value\"\n  #   effect: \"NoSchedule\"\n\n  ## Assign custom affinity rules to the prometheus operator\n  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  affinity: {}\n    # nodeAffinity:\n    #   requiredDuringSchedulingIgnoredDuringExecution:\n    #     nodeSelectorTerms:\n    #     - matchExpressions:\n    #       - key: kubernetes.io/e2e-az-name\n    #         operator: In\n    #         values:\n    #         - e2e-az1\n    #         - e2e-az2\n  dnsConfig: {}\n    # nameservers:\n    #   - 1.2.3.4\n    # searches:\n    #   - ns1.svc.cluster-domain.example\n    #   - my.dns.search.suffix\n    # options:\n    #   - name: ndots\n    #     value: \"2\"\n  #   - name: edns0\n  securityContext:\n    fsGroup: 65534\n    runAsGroup: 65534\n    runAsNonRoot: true\n    runAsUser: 65534\n\n  ## Prometheus-operator image\n  ##\n  image:\n    repository: quay.io/prometheus-operator/prometheus-operator\n    tag: v0.53.1\n    sha: \"\"\n    pullPolicy: IfNotPresent\n\n  ## Prometheus image to use for prometheuses managed by the operator\n  ##\n  # prometheusDefaultBaseImage: quay.io/prometheus/prometheus\n\n  ## Alertmanager image to use for alertmanagers managed by the operator\n  ##\n  # alertmanagerDefaultBaseImage: quay.io/prometheus/alertmanager\n\n  ## Prometheus-config-reloader\n  ##\n  prometheusConfigReloader:\n    # image to use for config and rule reloading\n    image:\n      repository: quay.io/prometheus-operator/prometheus-config-reloader\n      tag: v0.53.1\n      sha: \"\"\n\n    # resource config for prometheusConfigReloader\n    resources:\n      requests:\n        cpu: 100m\n        memory: 50Mi\n      limits:\n        cpu: 100m\n        memory: 50Mi\n\n  ## Thanos side-car image when configured\n  ##\n  thanosImage:\n    repository: quay.io/thanos/thanos\n    tag: v0.24.0\n    sha: \"\"\n\n  ## Set a Field Selector to filter watched secrets\n  ##\n  secretFieldSelector: \"\"\n\n## Deploy a Prometheus instance\n##\nprometheus:\n\n  enabled: true\n\n  ## Annotations for Prometheus\n  ##\n  annotations: {}\n\n  ## Service account for Prometheuses to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n    annotations: {}\n\n  # Service for thanos service discovery on sidecar\n  # Enable this can make Thanos Query can use\n  # `--store=dnssrv+_grpc._tcp.${kube-prometheus-stack.fullname}-thanos-discovery.${namespace}.svc.cluster.local` to discovery\n  # Thanos sidecar on prometheus nodes\n  # (Please remember to change ${kube-prometheus-stack.fullname} and ${namespace}. Not just copy and paste!)\n  thanosService:\n    enabled: false\n    annotations: {}\n    labels: {}\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n    ## gRPC port config\n    portName: grpc\n    port: 10901\n    targetPort: \"grpc\"\n\n    ## HTTP port config (for metrics)\n    httpPortName: http\n    httpPort: 10902\n    targetHttpPort: \"http\"\n\n    ## ClusterIP to assign\n    # Default is to make this a headless service (\"None\")\n    clusterIP: \"None\"\n\n    ## Port to expose on each node, if service type is NodePort\n    ##\n    nodePort: 30901\n    httpNodePort: 30902\n\n  # ServiceMonitor to scrape Sidecar metrics\n  # Needs thanosService to be enabled as well\n  thanosServiceMonitor:\n    enabled: false\n    interval: \"\"\n\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    bearerTokenFile:\n\n    ## Metric relabel configs to apply to samples before ingestion.\n    metricRelabelings: []\n\n    ## relabel configs to apply to samples before ingestion.\n    relabelings: []\n\n  # Service for external access to sidecar\n  # Enabling this creates a service to expose thanos-sidecar outside the cluster.\n  thanosServiceExternal:\n    enabled: false\n    annotations: {}\n    labels: {}\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## gRPC port config\n    portName: grpc\n    port: 10901\n    targetPort: \"grpc\"\n\n    ## HTTP port config (for metrics)\n    httpPortName: http\n    httpPort: 10902\n    targetHttpPort: \"http\"\n\n    ## Service type\n    ##\n    type: LoadBalancer\n\n    ## Port to expose on each node\n    ##\n    nodePort: 30901\n    httpNodePort: 30902\n\n  ## Configuration for Prometheus service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n    \n    ## Port for Prometheus Service to listen on\n    ##\n    port: 9090\n\n    ## To be used with a proxy extraContainer port\n    targetPort: 9090\n\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n    type: NodePort\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30090\n\n    ## Loadbalancer IP\n    ## Only use if service.type is \"LoadBalancer\"\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    ## Service type\n    ##\n\n    ## Additional port to define in the Service\n    additionalPorts: []\n\n    ## Consider that all endpoints are considered \"ready\" even if the Pods themselves are not\n    ## Ref: https://kubernetes.io/docs/reference/kubernetes-api/service-resources/service-v1/#ServiceSpec\n    publishNotReadyAddresses: false\n\n    sessionAffinity: \"\"\n\n  ## Configuration for creating a separate Service for each statefulset Prometheus replica\n  ##\n  servicePerReplica:\n    enabled: false\n    annotations: {}\n\n    ## Port for Prometheus Service per replica to listen on\n    ##\n    port: 9090\n\n    ## To be used with a proxy extraContainer port\n    targetPort: 9090\n\n    ## Port to expose on each node\n    ## Only used if servicePerReplica.type is 'NodePort'\n    ##\n    nodePort: 30091\n\n    ## Loadbalancer source IP ranges\n    ## Only used if servicePerReplica.type is \"LoadBalancer\"\n    loadBalancerSourceRanges: []\n    ## Service type\n    ##\n    type: ClusterIP\n\n  ## Configure pod disruption budgets for Prometheus\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\n  ## This configuration is immutable once created and will require the PDB to be deleted to be changed\n  ## https://github.com/kubernetes/kubernetes/issues/45398\n  ##\n  podDisruptionBudget:\n    enabled: false\n    minAvailable: 1\n    maxUnavailable: \"\"\n\n  # Ingress exposes thanos sidecar outside the cluster\n  thanosIngress:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n    servicePort: 10901\n\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30901\n\n    ## Hosts must be provided if Ingress is enabled.\n    ##\n    hosts: []\n      # - thanos-gateway.domain.com\n\n    ## Paths to use for ingress rules\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## TLS configuration for Thanos Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: thanos-gateway-tls\n    #   hosts:\n    #   - thanos-gateway.domain.com\n    #\n\n  ## ExtraSecret can be used to store various data in an extra secret\n  ## (use it for example to store hashed basic auth credentials)\n  extraSecret:\n    ## if not set, name will be auto generated\n    # name: \"\"\n    annotations: {}\n    data: {}\n  #   auth: |\n  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0\n  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.\n\n  ingress:\n    enabled: true\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: \n      kubernetes.io/ingress.class: alb\n      alb.ingress.kubernetes.io/target-type: instance\n      alb.ingress.kubernetes.io/scheme: internet-facing\n      alb.ingress.kubernetes.io/group.name: alb-01\n      alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 8081}]'\n      external-dns.alpha.kubernetes.io/hostname: prometheus.kubedns.click\n\n    labels: {}\n\n    ## Hostnames.\n    ## Must be provided if Ingress is enabled.\n    ##\n    #hosts:\n    #  - \"prom.kubedns.click\"\n    #hosts: []\n\n    ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix\n    ##\n    paths: \n       - /*\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n    #pathType: Prefix\n\n    ## TLS configuration for Prometheus Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n      # - secretName: prometheus-general-tls\n      #   hosts:\n      #     - prometheus.example.com\n\n  ## Configuration for creating an Ingress that will map to each Prometheus replica service\n  ## prometheus.servicePerReplica must be enabled\n  ##\n  ingressPerReplica:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n\n    ## Final form of the hostname for each per replica ingress is\n    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}\n    ##\n    ## Prefix for the per replica ingress that will have `-$replicaNumber`\n    ## appended to the end\n    hostPrefix: \"\"\n    ## Domain that will be used for the per replica ingress\n    hostDomain: \"\"\n\n    ## Paths to use for ingress rules\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## Secret name containing the TLS certificate for Prometheus per replica ingress\n    ## Secret must be manually created in the namespace\n    tlsSecretName: \"\"\n\n    ## Separated secret for each per replica Ingress. Can be used together with cert-manager\n    ##\n    tlsSecretPerReplica:\n      enabled: false\n      ## Final form of the secret for each per replica ingress is\n      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}\n      ##\n      prefix: \"prometheus\"\n\n  ## Configure additional options for default pod security policy for Prometheus\n  ## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n  podSecurityPolicy:\n    allowedCapabilities: []\n    allowedHostPaths: []\n    volumes: []\n\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    selfMonitor: true\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    bearerTokenFile:\n\n    ## Metric relabel configs to apply to samples before ingestion.\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    #   relabel configs to apply to samples before ingestion.\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n  ## Settings affecting prometheusSpec\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec\n  ##\n  prometheusSpec:\n    ## If true, pass --storage.tsdb.max-block-duration=2h to prometheus. This is already done if using Thanos\n    ##\n    disableCompaction: false\n    ## APIServerConfig\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#apiserverconfig\n    ##\n    apiserverConfig: {}\n\n    ## Interval between consecutive scrapes.\n    ## Defaults to 30s.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183\n    ##\n    scrapeInterval: \"\"\n\n    ## Number of seconds to wait for target to respond before erroring\n    ##\n    scrapeTimeout: \"\"\n\n    ## Interval between consecutive evaluations.\n    ##\n    evaluationInterval: \"\"\n\n    ## ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP.\n    ##\n    listenLocal: false\n\n    ## EnableAdminAPI enables Prometheus the administrative HTTP API which includes functionality such as deleting time series.\n    ## This is disabled by default.\n    ## ref: https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-admin-apis\n    ##\n    enableAdminAPI: false\n\n    ## WebTLSConfig defines the TLS parameters for HTTPS\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#webtlsconfig\n    web: {}\n\n    # EnableFeatures API enables access to Prometheus disabled features.\n    # ref: https://prometheus.io/docs/prometheus/latest/disabled_features/\n    enableFeatures: []\n    # - exemplar-storage\n\n    ## Image of Prometheus.\n    ##\n    image:\n      repository: quay.io/prometheus/prometheus\n      tag: v2.32.1\n      sha: \"\"\n\n    ## Tolerations for use with node taints\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    #  - key: \"key\"\n    #    operator: \"Equal\"\n    #    value: \"value\"\n    #    effect: \"NoSchedule\"\n\n    ## If specified, the pod's topology spread constraints.\n    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n    ##\n    topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n    #   labelSelector:\n    #     matchLabels:\n    #       app: prometheus\n\n    ## Alertmanagers to which alerts will be sent\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerendpoints\n    ##\n    ## Default configuration will connect to the alertmanager deployed as part of this release\n    ##\n    alertingEndpoints: []\n    # - name: \"\"\n    #   namespace: \"\"\n    #   port: http\n    #   scheme: http\n    #   pathPrefix: \"\"\n    #   tlsConfig: {}\n    #   bearerTokenFile: \"\"\n    #   apiVersion: v2\n\n    ## External labels to add to any time series or alerts when communicating with external systems\n    ##\n    externalLabels: {}\n\n    ## Name of the external label used to denote replica name\n    ##\n    replicaExternalLabelName: \"\"\n\n    ## If true, the Operator won't add the external label used to denote replica name\n    ##\n    replicaExternalLabelNameClear: false\n\n    ## Name of the external label used to denote Prometheus instance name\n    ##\n    prometheusExternalLabelName: \"\"\n\n    ## If true, the Operator won't add the external label used to denote Prometheus instance name\n    ##\n    prometheusExternalLabelNameClear: false\n\n    ## External URL at which Prometheus will be reachable.\n    ##\n    externalUrl: \"\"\n\n    ## Define which Nodes the Pods are scheduled on.\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n\n    ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.\n    ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not\n    ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated\n    ## with the new list of secrets.\n    ##\n    secrets: []\n\n    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.\n    ## The ConfigMaps are mounted into /etc/prometheus/configmaps/.\n    ##\n    configMaps: []\n\n    ## QuerySpec defines the query command line flags when starting Prometheus.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#queryspec\n    ##\n    query: {}\n\n    ## Namespaces to be selected for PrometheusRules discovery.\n    ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#namespaceselector for usage\n    ##\n    ruleNamespaceSelector: {}\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the PrometheusRule resources created\n    ##\n    ruleSelectorNilUsesHelmValues: true\n\n    ## PrometheusRules to be selected for target discovery.\n    ## If {}, select all PrometheusRules\n    ##\n    ruleSelector: {}\n    ## Example which select all PrometheusRules resources\n    ## with label \"prometheus\" with values any of \"example-rules\" or \"example-rules-2\"\n    # ruleSelector:\n    #   matchExpressions:\n    #     - key: prometheus\n    #       operator: In\n    #       values:\n    #         - example-rules\n    #         - example-rules-2\n    #\n    ## Example which select all PrometheusRules resources with label \"role\" set to \"example-rules\"\n    # ruleSelector:\n    #   matchLabels:\n    #     role: example-rules\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the servicemonitors created\n    ##\n    serviceMonitorSelectorNilUsesHelmValues: true\n\n    ## ServiceMonitors to be selected for target discovery.\n    ## If {}, select all ServiceMonitors\n    ##\n    serviceMonitorSelector: {}\n    ## Example which selects ServiceMonitors with label \"prometheus\" set to \"somelabel\"\n    # serviceMonitorSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## Namespaces to be selected for ServiceMonitor discovery.\n    ##\n    serviceMonitorNamespaceSelector: {}\n    ## Example which selects ServiceMonitors in namespaces with label \"prometheus\" set to \"somelabel\"\n    # serviceMonitorNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the podmonitors created\n    ##\n    podMonitorSelectorNilUsesHelmValues: true\n\n    ## PodMonitors to be selected for target discovery.\n    ## If {}, select all PodMonitors\n    ##\n    podMonitorSelector: {}\n    ## Example which selects PodMonitors with label \"prometheus\" set to \"somelabel\"\n    # podMonitorSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## Namespaces to be selected for PodMonitor discovery.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#namespaceselector for usage\n    ##\n    podMonitorNamespaceSelector: {}\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.probeSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the probes created\n    ##\n    probeSelectorNilUsesHelmValues: true\n\n    ## Probes to be selected for target discovery.\n    ## If {}, select all Probes\n    ##\n    probeSelector: {}\n    ## Example which selects Probes with label \"prometheus\" set to \"somelabel\"\n    # probeSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## Namespaces to be selected for Probe discovery.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#namespaceselector for usage\n    ##\n    probeNamespaceSelector: {}\n\n    ## How long to retain metrics\n    ##\n    retention: 10d\n\n    ## Maximum size of metrics\n    ##\n    retentionSize: \"\"\n\n    ## Enable compression of the write-ahead log using Snappy.\n    ##\n    walCompression: false\n\n    ## If true, the Operator won't process any Prometheus configuration changes\n    ##\n    paused: false\n\n    ## Number of replicas of each shard to deploy for a Prometheus deployment.\n    ## Number of replicas multiplied by shards is the total number of Pods created.\n    ##\n    replicas: 1\n\n    ## EXPERIMENTAL: Number of shards to distribute targets onto.\n    ## Number of replicas multiplied by shards is the total number of Pods created.\n    ## Note that scaling down shards will not reshard data onto remaining instances, it must be manually moved.\n    ## Increasing shards will not reshard data either but it will continue to be available from the same instances.\n    ## To query globally use Thanos sidecar and Thanos querier or remote write data to a central location.\n    ## Sharding is done on the content of the `__address__` target meta-label.\n    ##\n    shards: 1\n\n    ## Log level for Prometheus be configured in\n    ##\n    logLevel: info\n\n    ## Log format for Prometheus be configured in\n    ##\n    logFormat: logfmt\n\n    ## Prefix used to register routes, overriding externalUrl route.\n    ## Useful for proxies that rewrite URLs.\n    ##\n    routePrefix: /\n\n    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ## Metadata Labels and Annotations gets propagated to the prometheus pods.\n    ##\n    podMetadata: {}\n    # labels:\n    #   app: prometheus\n    #   k8s-app: prometheus\n\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\n    podAntiAffinity: \"\"\n\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n    ##\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n\n    ## Assign custom affinity rules to the prometheus instance\n    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n    ##\n    affinity: {}\n    # nodeAffinity:\n    #   requiredDuringSchedulingIgnoredDuringExecution:\n    #     nodeSelectorTerms:\n    #     - matchExpressions:\n    #       - key: kubernetes.io/e2e-az-name\n    #         operator: In\n    #         values:\n    #         - e2e-az1\n    #         - e2e-az2\n\n    ## The remote_read spec configuration for Prometheus.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotereadspec\n    remoteRead: []\n    # - url: http://remote1/read\n    ## additionalRemoteRead is appended to remoteRead\n    additionalRemoteRead: []\n\n    ## The remote_write spec configuration for Prometheus.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotewritespec\n    remoteWrite: []\n    # - url: http://remote1/push\n    ## additionalRemoteWrite is appended to remoteWrite\n    additionalRemoteWrite: []\n\n    ## Enable/Disable Grafana dashboards provisioning for prometheus remote write feature\n    remoteWriteDashboards: false\n\n    ## Resource limits \u0026 requests\n    ##\n    resources: {}\n    # requests:\n    #   memory: 400Mi\n\n    ## Prometheus StorageSpec for persistent data\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md\n    ##\n    storageSpec: {}\n    ## Using PersistentVolumeClaim\n    ##\n    #  volumeClaimTemplate:\n    #    spec:\n    #      storageClassName: gluster\n    #      accessModes: [\"ReadWriteOnce\"]\n    #      resources:\n    #        requests:\n    #          storage: 50Gi\n    #    selector: {}\n\n    ## Using tmpfs volume\n    ##\n    #  emptyDir:\n    #    medium: Memory\n\n    # Additional volumes on the output StatefulSet definition.\n    volumes: []\n\n    # Additional VolumeMounts on the output StatefulSet definition.\n    volumeMounts: []\n\n    ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations\n    ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form\n    ## as specified in the official Prometheus documentation:\n    ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are\n    ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility\n    ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible\n    ## scrape configs are going to break Prometheus after the upgrade.\n    ##\n    ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the\n    ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes\n    ##\n    additionalScrapeConfigs: []\n    # - job_name: kube-etcd\n    #   kubernetes_sd_configs:\n    #     - role: node\n    #   scheme: https\n    #   tls_config:\n    #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca\n    #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client\n    #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key\n    #   relabel_configs:\n    #   - action: labelmap\n    #     regex: __meta_kubernetes_node_label_(.+)\n    #   - source_labels: [__address__]\n    #     action: replace\n    #     targetLabel: __address__\n    #     regex: ([^:;]+):(\\d+)\n    #     replacement: ${1}:2379\n    #   - source_labels: [__meta_kubernetes_node_name]\n    #     action: keep\n    #     regex: .*mst.*\n    #   - source_labels: [__meta_kubernetes_node_name]\n    #     action: replace\n    #     targetLabel: node\n    #     regex: (.*)\n    #     replacement: ${1}\n    #   metric_relabel_configs:\n    #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)\n    #     action: labeldrop\n\n    ## If additional scrape configurations are already deployed in a single secret file you can use this section.\n    ## Expected values are the secret name and key\n    ## Cannot be used with additionalScrapeConfigs\n    additionalScrapeConfigsSecret: {}\n      # enabled: false\n      # name:\n      # key:\n\n    ## additionalPrometheusSecretsAnnotations allows to add annotations to the kubernetes secret. This can be useful\n    ## when deploying via spinnaker to disable versioning on the secret, strategy.spinnaker.io/versioned: 'false'\n    additionalPrometheusSecretsAnnotations: {}\n\n    ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified\n    ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#\u003calertmanager_config\u003e.\n    ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.\n    ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this\n    ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release\n    ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.\n    ##\n    additionalAlertManagerConfigs: []\n    # - consul_sd_configs:\n    #   - server: consul.dev.test:8500\n    #     scheme: http\n    #     datacenter: dev\n    #     tag_separator: ','\n    #     services:\n    #       - metrics-prometheus-alertmanager\n\n    ## If additional alertmanager configurations are already deployed in a single secret, or you want to manage\n    ## them separately from the helm deployment, you can use this section.\n    ## Expected values are the secret name and key\n    ## Cannot be used with additionalAlertManagerConfigs\n    additionalAlertManagerConfigsSecret: {}\n      # name:\n      # key:\n\n    ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended\n    ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the\n    ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.\n    ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the\n    ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel\n    ## configs are going to break Prometheus after the upgrade.\n    ##\n    additionalAlertRelabelConfigs: []\n    # - separator: ;\n    #   regex: prometheus_replica\n    #   replacement: $1\n    #   action: labeldrop\n\n    ## SecurityContext holds pod-level security attributes and common container settings.\n    ## This defaults to non root user with uid 1000 and gid 2000.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md\n    ##\n    securityContext:\n      runAsGroup: 2000\n      runAsNonRoot: true\n      runAsUser: 1000\n      fsGroup: 2000\n\n    ## Priority class assigned to the Pods\n    ##\n    priorityClassName: \"\"\n\n    ## Thanos configuration allows configuring various aspects of a Prometheus server in a Thanos environment.\n    ## This section is experimental, it may change significantly without deprecation notice in any release.\n    ## This is experimental and may change significantly without backward compatibility in any release.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosspec\n    ##\n    thanos: {}\n      # secretProviderClass:\n      #   provider: gcp\n      #   parameters:\n      #     secrets: |\n      #       - resourceName: \"projects/$PROJECT_ID/secrets/testsecret/versions/latest\"\n      #         fileName: \"objstore.yaml\"\n      # objectStorageConfigFile: /var/secrets/object-store.yaml\n\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to a Prometheus pod.\n    ## if using proxy extraContainer update targetPort with proxy container port\n    containers: []\n\n    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes\n    ## (permissions, dir tree) on mounted volumes before starting prometheus\n    initContainers: []\n\n    ## PortName to use for Prometheus.\n    ##\n    portName: \"http-web\"\n\n    ## ArbitraryFSAccessThroughSMs configures whether configuration based on a service monitor can access arbitrary files\n    ## on the file system of the Prometheus container e.g. bearer token files.\n    arbitraryFSAccessThroughSMs: false\n\n    ## OverrideHonorLabels if set to true overrides all user configured honor_labels. If HonorLabels is set in ServiceMonitor\n    ## or PodMonitor to true, this overrides honor_labels to false.\n    overrideHonorLabels: false\n\n    ## OverrideHonorTimestamps allows to globally enforce honoring timestamps in all scrape configs.\n    overrideHonorTimestamps: false\n\n    ## IgnoreNamespaceSelectors if set to true will ignore NamespaceSelector settings from the podmonitor and servicemonitor\n    ## configs, and they will only discover endpoints within their current namespace. Defaults to false.\n    ignoreNamespaceSelectors: false\n\n    ## EnforcedNamespaceLabel enforces adding a namespace label of origin for each alert and metric that is user created.\n    ## The label value will always be the namespace of the object that is being created.\n    ## Disabled by default\n    enforcedNamespaceLabel: \"\"\n\n    ## PrometheusRulesExcludedFromEnforce - list of prometheus rules to be excluded from enforcing of adding namespace labels.\n    ## Works only if enforcedNamespaceLabel set to true. Make sure both ruleNamespace and ruleName are set for each pair\n    prometheusRulesExcludedFromEnforce: []\n\n    ## QueryLogFile specifies the file to which PromQL queries are logged. Note that this location must be writable,\n    ## and can be persisted using an attached volume. Alternatively, the location can be set to a stdout location such\n    ## as /dev/stdout to log querie information to the default Prometheus log stream. This is only available in versions\n    ## of Prometheus \u003e= 2.16.0. For more details, see the Prometheus docs (https://prometheus.io/docs/guides/query-log/)\n    queryLogFile: false\n\n    ## EnforcedSampleLimit defines global limit on number of scraped samples that will be accepted. This overrides any SampleLimit\n    ## set per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the SampleLimit to keep overall\n    ## number of samples/series under the desired limit. Note that if SampleLimit is lower that value will be taken instead.\n    enforcedSampleLimit: false\n\n    ## EnforcedTargetLimit defines a global limit on the number of scraped targets. This overrides any TargetLimit set\n    ## per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the TargetLimit to keep the overall\n    ## number of targets under the desired limit. Note that if TargetLimit is lower, that value will be taken instead, except\n    ## if either value is zero, in which case the non-zero value will be used. If both values are zero, no limit is enforced.\n    enforcedTargetLimit: false\n\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. If more than this number of labels are present\n    ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions\n    ## 2.27.0 and newer.\n    enforcedLabelLimit: false\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. If a label name is longer than this number\n    ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions\n    ## 2.27.0 and newer.\n    enforcedLabelNameLengthLimit: false\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. If a label value is longer than this\n    ## number post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus\n    ## versions 2.27.0 and newer.\n    enforcedLabelValueLengthLimit: false\n\n    ## AllowOverlappingBlocks enables vertical compaction and vertical query merge in Prometheus. This is still experimental\n    ## in Prometheus so it may change in any upcoming release.\n    allowOverlappingBlocks: false\n\n  additionalRulesForClusterRole: []\n  #  - apiGroups: [ \"\" ]\n  #    resources:\n  #      - nodes/proxy\n  #    verbs: [ \"get\", \"list\", \"watch\" ]\n\n  additionalServiceMonitors: []\n  ## Name of the ServiceMonitor to create\n  ##\n  # - name: \"\"\n\n    ## Additional labels to set used for the ServiceMonitorSelector. Together with standard labels from\n    ## the chart\n    ##\n    # additionalLabels: {}\n\n    ## Service label for use in assembling a job name of the form \u003clabel value\u003e-\u003cport\u003e\n    ## If no label is specified, the service name is used.\n    ##\n    # jobLabel: \"\"\n\n    ## labels to transfer from the kubernetes service to the target\n    ##\n    # targetLabels: []\n\n    ## labels to transfer from the kubernetes pods to the target\n    ##\n    # podTargetLabels: []\n\n    ## Label selector for services to which this ServiceMonitor applies\n    ##\n    # selector: {}\n\n    ## Namespaces from which services are selected\n    ##\n    # namespaceSelector:\n      ## Match any namespace\n      ##\n      # any: false\n\n      ## Explicit list of namespace names to select\n      ##\n      # matchNames: []\n\n    ## Endpoints of the selected service to be monitored\n    ##\n    # endpoints: []\n      ## Name of the endpoint's service port\n      ## Mutually exclusive with targetPort\n      # - port: \"\"\n\n      ## Name or number of the endpoint's target port\n      ## Mutually exclusive with port\n      # - targetPort: \"\"\n\n      ## File containing bearer token to be used when scraping targets\n      ##\n      #   bearerTokenFile: \"\"\n\n      ## Interval at which metrics should be scraped\n      ##\n      #   interval: 30s\n\n      ## HTTP path to scrape for metrics\n      ##\n      #   path: /metrics\n\n      ## HTTP scheme to use for scraping\n      ##\n      #   scheme: http\n\n      ## TLS configuration to use when scraping the endpoint\n      ##\n      #   tlsConfig:\n\n          ## Path to the CA file\n          ##\n          # caFile: \"\"\n\n          ## Path to client certificate file\n          ##\n          # certFile: \"\"\n\n          ## Skip certificate verification\n          ##\n          # insecureSkipVerify: false\n\n          ## Path to client key file\n          ##\n          # keyFile: \"\"\n\n          ## Server name used to verify host name\n          ##\n          # serverName: \"\"\n\n  additionalPodMonitors: []\n  ## Name of the PodMonitor to create\n  ##\n  # - name: \"\"\n\n    ## Additional labels to set used for the PodMonitorSelector. Together with standard labels from\n    ## the chart\n    ##\n    # additionalLabels: {}\n\n    ## Pod label for use in assembling a job name of the form \u003clabel value\u003e-\u003cport\u003e\n    ## If no label is specified, the pod endpoint name is used.\n    ##\n    # jobLabel: \"\"\n\n    ## Label selector for pods to which this PodMonitor applies\n    ##\n    # selector: {}\n\n    ## PodTargetLabels transfers labels on the Kubernetes Pod onto the target.\n    ##\n    # podTargetLabels: {}\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    # sampleLimit: 0\n\n    ## Namespaces from which pods are selected\n    ##\n    # namespaceSelector:\n      ## Match any namespace\n      ##\n      # any: false\n\n      ## Explicit list of namespace names to select\n      ##\n      # matchNames: []\n\n    ## Endpoints of the selected pods to be monitored\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#podmetricsendpoint\n    ##\n    # podMetricsEndpoints: []\n"
            ],
            "verify": false,
            "version": "32.2.0",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_subnet.privateEC2Subnet2",
            "data.aws_eks_cluster.cluster",
            "data.tls_certificate.cluster",
            "helm_release.aws-load-balancer-controller",
            "aws_iam_role.aws_lb_controller",
            "aws_iam_role_policy_attachment.aws_lb_controller",
            "aws_iam_role_policy_attachment.eks-cluster-AmazonEKSServicePolicy",
            "aws_route53_zone.kubedns",
            "helm_release.external_dns",
            "aws_eks_cluster.eks_cluster",
            "aws_iam_openid_connect_provider.cluster",
            "aws_iam_policy.aws_lb_controller",
            "aws_iam_role.iam-role-eks-cluster",
            "aws_vpc.test",
            "aws_iam_role.AllowExternalDNSUpdates",
            "aws_iam_role_policy_attachment.eks-cluster-AmazonEKSClusterPolicy",
            "aws_security_group.eks-cluster",
            "aws_subnet.privateEC2Subnet1"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "null_resource",
      "name": "null_for_ecr_get_login_password",
      "provider": "provider[\"registry.terraform.io/hashicorp/null\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "4998262099645276164",
            "triggers": null
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "aws_ecr_repository.foo"
          ]
        }
      ]
    }
  ]
}
